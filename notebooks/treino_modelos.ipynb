{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b74fd31",
   "metadata": {},
   "source": [
    "## Testa a utilização dos modelos LSTM ou Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9acc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate, Dropout, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f38ac",
   "metadata": {},
   "source": [
    "## Função filtra_usuario_separa_x_epoca_y()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b8acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_usuario_separa_x_epoca_y(df, username=\"*\", usuarios_exclusao=[]):\n",
    "    \"\"\"\n",
    "    Prepara os dados filtrando por usuário e criando features históricas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com os dados originais\n",
    "        username: Nome do usuário para filtrar (\"*\" para todos os usuários)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (features_sequenciais, features_contextuais, target)\n",
    "    \"\"\"\n",
    "    print(f\"Processando dados para usuário: {username}\")\n",
    "    \n",
    "    # Filtrar por usuário se especificado\n",
    "\n",
    "    if username != \"*\":\n",
    "        df_filtro = df[df[\"usuario\"] == username].copy()\n",
    "        print(f\"Registros após filtro de usuário: {len(df_filtro)}\")\n",
    "    else:\n",
    "        df_filtro = df[~df[\"usuario\"].isin(usuarios_exclusao)].copy()\n",
    "        print(f\"Processando todos os usuários (excluindo {len(usuarios_exclusao)} usuários): {len(df_filtro)} registros\")\n",
    "\n",
    "        # Ordenar por data/hora para manter sequência temporal\n",
    "        df_filtro = df_filtro.sort_values([\"usuario\", \"Dia\", \"Mes\", \"Ano\", \"DataHoraCriacao\"])\n",
    "\n",
    "    # Criar features históricas (últimos 3 casos de uso)\n",
    "    print(\"Criando features históricas...\")\n",
    "    for shift in range(1, 3):\n",
    "        df_filtro[f\"casoDeUso_{shift}\"] = df_filtro.groupby([\"usuario\", \"Dia\", \"Mes\", \"Ano\"])[\"casoDeUso\"].shift(shift).fillna(\"vazio\")\n",
    "\n",
    "    # Remover registros com NaN e resetar índice\n",
    "    df_filtro = df_filtro.dropna().reset_index(drop=True)\n",
    "    print(f\"Registros após limpeza: {len(df_filtro)}\")\n",
    "\n",
    "    # Separar target\n",
    "    df_target = df_filtro[\"casoDeUso\"]\n",
    "\n",
    "    # Preparar features sequenciais (remover colunas não necessárias)\n",
    "    cols_a_remover = [\"DataHoraCriacao\", \"Dia\", \"Mes\", \"Ano\", \"casoDeUso\", \"usuario\", \"PeriodoDoMes\"]\n",
    "    df_x = df_filtro.drop(columns=cols_a_remover)\n",
    "\n",
    "    # Preparar features contextuais (período do mês)\n",
    "    df_epoca = df_filtro[[\"PeriodoDoMes\"]].copy()\n",
    "\n",
    "    return df_x, df_epoca, df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a3b8f",
   "metadata": {},
   "source": [
    "## Função aplica_OneHotEncoder_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccfd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_OneHotEncoder_x(df, cols):\n",
    "    \"\"\"\n",
    "    Aplica One-Hot Encoding nas colunas especificadas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame para processar\n",
    "        cols: Lista de colunas para aplicar encoding\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame com encoding aplicado, OneHotEncoder fitted)\n",
    "    \"\"\"\n",
    "    print(f\"Aplicando One-Hot Encoding em: {cols}\")\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    df_ohe = pd.DataFrame(\n",
    "        ohe.fit_transform(df[cols]), \n",
    "        columns=ohe.get_feature_names_out(cols), \n",
    "        index=df.index\n",
    "    )\n",
    "    return pd.concat([df.drop(columns=cols), df_ohe], axis=1), ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf53f3",
   "metadata": {},
   "source": [
    "## Função preprocessar_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632f5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_dados(data_path, username=\"*\", usuarios_exclusao=[], test_size=0.4, random_state=42):\n",
    "    \"\"\"\n",
    "    Pipeline completo de preprocessamento de dados\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para o arquivo CSV\n",
    "        username: Usuário para filtrar\n",
    "        test_size: Proporção para teste\n",
    "        random_state: Seed para reprodutibilidade\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Dados de treino e teste preprocessados\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO PREPROCESSAMENTO ===\")\n",
    "    \n",
    "    # Carregar dados\n",
    "    print(f\"Carregando dados de: {data_path}\")\n",
    "    df = pd.read_csv(\n",
    "        data_path, \n",
    "        sep=';', \n",
    "        encoding='utf-8', \n",
    "        parse_dates=['DataHoraCriacao'], \n",
    "        dayfirst=True\n",
    "    )\n",
    "    print(f\"Dataset carregado: {df.shape}\")\n",
    "    \n",
    "    # Verificar distribuição das classes\n",
    "    print(\"\\nDistribuição das classes:\")\n",
    "    print(df['casoDeUso'].value_counts().head(10))\n",
    "    \n",
    "    # Processar dados\n",
    "    df_x, df_epoca, serie_y = filtra_usuario_separa_x_epoca_y(df, username, usuarios_exclusao)\n",
    "    \n",
    "    # Aplicar One-Hot Encoding nas features históricas\n",
    "    # df_x, ohe_x = aplica_OneHotEncoder_x(df_x, [\"casoDeUso_1\", \"casoDeUso_2\", \"casoDeUso_3\"])\n",
    "    df_x, ohe_x = aplica_OneHotEncoder_x(df_x, [\"casoDeUso_1\", \"casoDeUso_2\"])\n",
    "    \n",
    "    # Mapear período do mês para valores numéricos e normalizar\n",
    "    print(\"Processando features contextuais...\")\n",
    "    df_epoca[\"PeriodoDoMes\"] = df_epoca[\"PeriodoDoMes\"].map({\n",
    "        'antes_folha': 0, \n",
    "        'dia_folha': 1, \n",
    "        'apos_folha': 2\n",
    "    })\n",
    "    \n",
    "    # Normalizar features contextuais\n",
    "    scaler_epoca = StandardScaler()\n",
    "    df_epoca_scaled = pd.DataFrame(\n",
    "        scaler_epoca.fit_transform(df_epoca), \n",
    "        columns=df_epoca.columns,\n",
    "        index=df_epoca.index\n",
    "    )\n",
    "    \n",
    "    # Converter target para One-Hot\n",
    "    y_one_hot, ohe_y = aplica_OneHotEncoder_x(serie_y.to_frame(), [\"casoDeUso\"])\n",
    "    \n",
    "    # Normalizar features sequenciais\n",
    "    print(\"Normalizando features sequenciais...\")\n",
    "    scaler_seq = StandardScaler()\n",
    "    df_x_scaled = pd.DataFrame(\n",
    "        scaler_seq.fit_transform(df_x),\n",
    "        columns=df_x.columns,\n",
    "        index=df_x.index\n",
    "    )\n",
    "    \n",
    "    # Verificar se é possível fazer divisão estratificada\n",
    "    print(\"Dividindo dados em treino e teste...\")\n",
    "    \n",
    "    # Verificar classes com poucas amostras\n",
    "    class_counts = serie_y.value_counts()\n",
    "    classes_com_poucas_amostras = class_counts[class_counts < 2]\n",
    "    \n",
    "    if len(classes_com_poucas_amostras) > 0:\n",
    "        print(f\"⚠️ Aviso: {len(classes_com_poucas_amostras)} classes com apenas 1 amostra:\")\n",
    "        print(classes_com_poucas_amostras.head())\n",
    "        print(\"Removendo classes com poucas amostras para permitir estratificação...\")\n",
    "        \n",
    "        # Filtrar classes com pelo menos 2 amostras\n",
    "        classes_validas = class_counts[class_counts >= 2].index\n",
    "        mask = serie_y.isin(classes_validas)\n",
    "        \n",
    "        df_x_scaled = df_x_scaled[mask]\n",
    "        df_epoca_scaled = df_epoca_scaled[mask]\n",
    "        y_one_hot = y_one_hot[mask]\n",
    "        serie_y = serie_y[mask]\n",
    "        \n",
    "        print(f\"Dados após filtro: {len(df_x_scaled)} amostras, {len(serie_y.unique())} classes\")\n",
    "        \n",
    "        # Recriar one-hot encoding para classes restantes\n",
    "        y_one_hot, ohe_y = aplica_OneHotEncoder_x(serie_y.to_frame(), [\"casoDeUso\"])\n",
    "    \n",
    "    # Tentar divisão estratificada, se falhar usar divisão simples\n",
    "    try:\n",
    "        X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, y_train, y_test = train_test_split(\n",
    "            df_x_scaled, df_epoca_scaled, y_one_hot, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=serie_y  # Estratificação para manter proporção das classes\n",
    "        )\n",
    "        print(\"✅ Divisão estratificada realizada com sucesso\")\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Não foi possível fazer divisão estratificada: {str(e)}\")\n",
    "        print(\"Realizando divisão simples...\")\n",
    "        X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, y_train, y_test = train_test_split(\n",
    "            df_x_scaled, df_epoca_scaled, y_one_hot, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    print(f\"Treino: {X_train_seq.shape[0]} samples\")\n",
    "    print(f\"Teste: {X_test_seq.shape[0]} samples\")\n",
    "    \n",
    "    return (X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, \n",
    "            y_train, y_test, scaler_seq, scaler_epoca, ohe_x, ohe_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99b950",
   "metadata": {},
   "source": [
    "## função criar_modelo_hibrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a2a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_modelo_hibrido(input_seq_shape, input_epoch_shape, output_shape, \n",
    "                        use_lstm=True, lstm_units=64, dense_units=[128, 64], \n",
    "                        dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Cria modelo híbrido com opção de usar LSTM ou Dense\n",
    "    \n",
    "    Args:\n",
    "        input_seq_shape: Shape das features sequenciais\n",
    "        input_epoch_shape: Shape das features contextuais\n",
    "        output_shape: Número de classes de saída\n",
    "        use_lstm: Se True, usa LSTM; se False, usa Dense\n",
    "        lstm_units: Unidades LSTM\n",
    "        dense_units: Lista com unidades das camadas Dense\n",
    "        dropout_rate: Taxa de dropout\n",
    "    \n",
    "    Returns:\n",
    "        Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    print(f\"=== CRIANDO MODELO {'LSTM' if use_lstm else 'DENSE'} HÍBRIDO ===\")\n",
    "    \n",
    "    # Input para features sequenciais\n",
    "    input_seq = Input(shape=(input_seq_shape,), name='input_sequence')\n",
    "    input_epoch = Input(shape=(input_epoch_shape,), name='input_epoch')\n",
    "    \n",
    "    if use_lstm:\n",
    "        # Para LSTM, precisamos reshapear para 3D (samples, timesteps, features)\n",
    "        # Assumindo que cada feature é um timestep\n",
    "        x = Reshape((input_seq_shape, 1))(input_seq)\n",
    "        x = LSTM(lstm_units, return_sequences=False, dropout=dropout_rate)(x)\n",
    "        print(f\"Usando LSTM com {lstm_units} unidades\")\n",
    "    else:\n",
    "        # Usar camadas Dense tradicionais\n",
    "        x = input_seq\n",
    "        for i, units in enumerate(dense_units):\n",
    "            x = Dense(units, activation='relu', name=f'dense_{i+1}')(x)\n",
    "            x = Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "        print(f\"Usando Dense layers: {dense_units}\")\n",
    "    \n",
    "    # Combinar features sequenciais com contextuais\n",
    "    combined = Concatenate(name='concatenate')([x, input_epoch])\n",
    "    \n",
    "    # Camada de saída\n",
    "    output = Dense(output_shape, activation='softmax', name='output')(combined)\n",
    "    \n",
    "    # Criar modelo\n",
    "    model = Model(inputs=[input_seq, input_epoch], outputs=output)\n",
    "    \n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo criado e compilado!\")\n",
    "    print(f\"Parâmetros totais: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a396405",
   "metadata": {},
   "source": [
    "## função treinar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b83292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo(model, X_train_seq, X_train_epoch, y_train, \n",
    "                  epochs=50, validation_split=0.2, verbose=1):\n",
    "    \"\"\"\n",
    "    Treina o modelo com callbacks para early stopping e redução de learning rate\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser treinado\n",
    "        X_train_seq, X_train_epoch: Features de treino\n",
    "        y_train: Target de treino\n",
    "        epochs: Número máximo de épocas\n",
    "        validation_split: Proporção para validação\n",
    "        verbose: Verbosidade do treinamento\n",
    "    \n",
    "    Returns:\n",
    "        History: Histórico do treinamento\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO TREINAMENTO ===\")\n",
    "    \n",
    "    # Callbacks para melhorar o treinamento\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Treinar modelo\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_epoch], \n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    print(\"Treinamento concluído!\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa75ba6",
   "metadata": {},
   "source": [
    "## Função avaliar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1538f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(model, X_test_seq, X_test_epoch, y_test, ohe_y):\n",
    "    \"\"\"\n",
    "    Avalia o modelo com múltiplas métricas\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo treinado\n",
    "        X_test_seq, X_test_epoch: Features de teste\n",
    "        y_test: Target de teste\n",
    "        ohe_y: OneHotEncoder do target para obter nomes das classes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário com métricas de avaliação\n",
    "    \"\"\"\n",
    "    print(\"=== AVALIANDO MODELO ===\")\n",
    "    \n",
    "    # Predições\n",
    "    y_pred_proba = model.predict([X_test_seq, X_test_epoch])\n",
    "    y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "    y_test_labels = np.argmax(y_test.values, axis=-1)\n",
    "    \n",
    "    # Métricas básicas\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Relatório de classificação\n",
    "    class_names = ohe_y.get_feature_names_out(['casoDeUso'])\n",
    "    class_names = [name.replace('casoDeUso_', '') for name in class_names]\n",
    "    \n",
    "    print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
    "    print(classification_report(y_test_labels, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'y_test': y_test_labels,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_names': class_names,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68b79c",
   "metadata": {},
   "source": [
    "## Função salvar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7880be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_modelo(model, scaler_seq, scaler_epoca, ohe_x, ohe_y, path_base='artefatos_modelo'):\n",
    "    \"\"\"\n",
    "    Salva o modelo Keras e os transformadores necessários para inferência futura.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras treinado.\n",
    "        scaler_seq: Scaler das features sequenciais.\n",
    "        scaler_epoca: Scaler das features contextuais.\n",
    "        ohe_x: OneHotEncoder das features históricas.\n",
    "        ohe_y: OneHotEncoder do target.\n",
    "        path_base: Pasta/caminho base para salvar os arquivos.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(path_base, exist_ok=True)\n",
    "\n",
    "    # Salva o modelo keras\n",
    "    model.save(os.path.join(path_base, 'modelo.keras'))\n",
    "\n",
    "    # Salva os transformadores com pickle\n",
    "    with open(os.path.join(path_base, 'scaler_seq.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_seq, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'scaler_epoca.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_epoca, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'ohe_x.pkl'), 'wb') as f:\n",
    "        pickle.dump(ohe_x, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'ohe_y.pkl'), 'wb') as f:\n",
    "        pickle.dump(ohe_y, f)\n",
    "\n",
    "    print(f\"\\n✅ Modelo e transformadores salvos na pasta '{path_base}'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3f9fb",
   "metadata": {},
   "source": [
    "## Função plotar_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c1196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_resultados(history, results):\n",
    "    \"\"\"\n",
    "    Plota gráficos de treinamento e matriz de confusão\n",
    "    \n",
    "    Args:\n",
    "        history: Histórico do treinamento\n",
    "        results: Resultados da avaliação\n",
    "    \"\"\"\n",
    "    print(\"Gerando visualizações...\")\n",
    "    \n",
    "    # Configurar estilo dos gráficos\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Acurácia durante treinamento\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Validação', linewidth=2)\n",
    "    plt.title('Acurácia Durante o Treinamento', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Perda durante treinamento\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validação', linewidth=2)\n",
    "    plt.title('Perda Durante o Treinamento', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning Rate (se disponível)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    if 'lr' in history.history:\n",
    "        plt.plot(history.history['lr'], linewidth=2, color='red')\n",
    "        plt.title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Learning Rate\\nnão disponível', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Matriz de confusão\n",
    "    ax4 = plt.subplot(2, 3, (4, 6))\n",
    "    sns.heatmap(\n",
    "        results['confusion_matrix'], \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=results['class_names'],\n",
    "        yticklabels=results['class_names']\n",
    "    )\n",
    "    plt.title('Matriz de Confusão', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    \n",
    "    # 5. Distribuição de confiança das predições\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    max_proba = np.max(results['y_pred_proba'], axis=1)\n",
    "    plt.hist(max_proba, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribuição de Confiança das Predições', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Confiança Máxima')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af53c5",
   "metadata": {},
   "source": [
    "## Função Principal que executa todo o pipeline\n",
    "\n",
    "Esta função:\n",
    "\n",
    "1 - Préprocessa os dados:\n",
    "\n",
    "1.1 - filtrando os usuários que serão utilizados para treinamento\n",
    "\n",
    "1.2 - Gerando linhas com dois casos de uso históricos maio o caso de uso alvo/target\n",
    "\n",
    "1.3 - Separa a massa de dados de treino e avaliação do treino\n",
    "\n",
    "2 - Cria um modelo hibrido, podendo ser:\n",
    "\n",
    "2.1 - LSTM\n",
    "\n",
    "2.2 - Dense layers\n",
    "\n",
    "3 - Treina o modelo hibrido com os dados do passo 1\n",
    "\n",
    "4 - Avaliar o modelo treinado com a base de dados separada para avaliação no passo 1\n",
    "\n",
    "5 - Plota gráficos de avaliação dos resultados\n",
    "\n",
    "6 - Salva o modelo e seus parâmetros de configuração para posterior utilização nas previsões (aqui no notebook, será salvo abaixo da pasta notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='dados.csv', \n",
    "         usuario='*', # Nome do usuário ou '*' para todos\n",
    "         use_lstm=False,  # Usar Dense layers ou LSTM\n",
    "         epochs=50,\n",
    "         plotar_resultado=True, # Se deve ou não chamar o método plotar_resultados()\n",
    "         usuarios_exclusao=[], # No caso de usar '*' em usuario, lista de exclusão da base de dados.  Não carregar os dados desses.\n",
    "         salvar_modelo=False\n",
    "\n",
    "         ):  \n",
    "    \"\"\"\n",
    "    Função principal que executa todo o pipeline\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para os dados\n",
    "        usuario: Usuário para filtrar\n",
    "        use_lstm: Se usar LSTM ou Dense layers\n",
    "        epochs: Número de épocas para treinamento\n",
    "        plotar_resultado: Se deve ou não chamar o método plotar_resultados() ao final do treino e testes\n",
    "        usuarios_exclusao: Lista de usuários a não serem considerados se o usuario='*'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocessamento\n",
    "        (X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, \n",
    "         y_train, y_test, scaler_seq, scaler_epoca, ohe_x, ohe_y) = preprocessar_dados(data_path, usuario, usuarios_exclusao)\n",
    "        \n",
    "        # Criar modelo\n",
    "        modelo = criar_modelo_hibrido(\n",
    "            input_seq_shape=X_train_seq.shape[1],\n",
    "            input_epoch_shape=X_train_epoch.shape[1],\n",
    "            output_shape=y_train.shape[1],\n",
    "            use_lstm=use_lstm\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\\n=== ARQUITETURA DO MODELO ===\")\n",
    "        modelo.summary()\n",
    "        \n",
    "        # Treinar modelo\n",
    "        historico = treinar_modelo(\n",
    "            modelo, X_train_seq, X_train_epoch, y_train, \n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        # Avaliar modelo\n",
    "        resultados = avaliar_modelo(\n",
    "            modelo, X_test_seq, X_test_epoch, y_test, ohe_y\n",
    "        )\n",
    "        \n",
    "        # Plotar resultados\n",
    "        if plotar_resultado:\n",
    "            plotar_resultados(historico, resultados)\n",
    "        \n",
    "        if salvar_modelo:\n",
    "            salvar_modelo(modelo, scaler_seq, scaler_epoca, ohe_x, ohe_y, path_base='../modelos')\n",
    "\n",
    "        return modelo, historico, resultados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante execução: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7e821",
   "metadata": {},
   "source": [
    "## Execução do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0734399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando pipeline de Machine Learning ...\n",
      "============================================================\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 9 usuários): 106118 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 106118\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 13 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0246    1\n",
      "uc2015    1\n",
      "uc0154    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 106105 amostras, 280 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 63663 samples\n",
      "Teste: 42442 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748219167.455892  112020 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 101,872\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">586</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">75,136</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,480</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m586\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m75,136\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m280\u001b[0m)       │     \u001b[38;5;34m18,480\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,872</span> (397.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,872\u001b[0m (397.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,872</span> (397.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,872\u001b[0m (397.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748219168.669351  116339 service.cc:152] XLA service 0x7fd29c0049a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748219168.669379  116339 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-05-25 21:26:08.685422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748219168.765206  116339 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-05-25 21:26:09.858216: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1111', 720 bytes spill stores, 620 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  83/1592\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0556 - loss: 5.3964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748219170.590886  116339 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1574/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1548 - loss: 4.2126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 21:26:14.398640: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1111_0', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-05-25 21:26:14.401809: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1111', 696 bytes spill stores, 604 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1553 - loss: 4.2086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 21:26:15.868993: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-25 21:26:15.877381: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-05-25 21:26:17.127537: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-05-25 21:26:17.147796: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.1553 - loss: 4.2083 - val_accuracy: 0.2547 - val_loss: 3.3650 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2427 - loss: 3.4633 - val_accuracy: 0.2670 - val_loss: 3.2373 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2545 - loss: 3.3313 - val_accuracy: 0.2725 - val_loss: 3.2116 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2629 - loss: 3.2857 - val_accuracy: 0.2710 - val_loss: 3.2104 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2611 - loss: 3.2491 - val_accuracy: 0.2745 - val_loss: 3.1964 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2645 - loss: 3.2319 - val_accuracy: 0.2747 - val_loss: 3.1870 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2645 - loss: 3.1948 - val_accuracy: 0.2757 - val_loss: 3.1859 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2688 - loss: 3.1947 - val_accuracy: 0.2715 - val_loss: 3.1918 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2646 - loss: 3.1788 - val_accuracy: 0.2757 - val_loss: 3.1839 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2659 - loss: 3.1838 - val_accuracy: 0.2754 - val_loss: 3.1926 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2706 - loss: 3.1568 - val_accuracy: 0.2753 - val_loss: 3.1991 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2688 - loss: 3.1532 - val_accuracy: 0.2750 - val_loss: 3.1994 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2701 - loss: 3.1493 - val_accuracy: 0.2738 - val_loss: 3.2010 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1572/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2692 - loss: 3.1359\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2692 - loss: 3.1362 - val_accuracy: 0.2746 - val_loss: 3.2061 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2667 - loss: 3.1333 - val_accuracy: 0.2757 - val_loss: 3.1958 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2732 - loss: 3.0876 - val_accuracy: 0.2774 - val_loss: 3.1913 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2781 - loss: 3.0737 - val_accuracy: 0.2775 - val_loss: 3.1982 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2802 - loss: 3.0676 - val_accuracy: 0.2786 - val_loss: 3.1966 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2771 - loss: 3.0636\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2771 - loss: 3.0636 - val_accuracy: 0.2784 - val_loss: 3.1961 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2770 - loss: 3.0542 - val_accuracy: 0.2802 - val_loss: 3.1955 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2819 - loss: 3.0424 - val_accuracy: 0.2797 - val_loss: 3.1971 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2805 - loss: 3.0425 - val_accuracy: 0.2798 - val_loss: 3.2006 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2821 - loss: 3.0364 - val_accuracy: 0.2796 - val_loss: 3.1978 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1578/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2827 - loss: 3.0324\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2827 - loss: 3.0325 - val_accuracy: 0.2797 - val_loss: 3.1988 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2805 - loss: 3.0300 - val_accuracy: 0.2801 - val_loss: 3.1987 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2836 - loss: 3.0240 - val_accuracy: 0.2795 - val_loss: 3.2012 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2861 - loss: 3.0127 - val_accuracy: 0.2803 - val_loss: 3.2010 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2833 - loss: 3.0203 - val_accuracy: 0.2802 - val_loss: 3.2025 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1579/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2875 - loss: 3.0072\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2875 - loss: 3.0073 - val_accuracy: 0.2799 - val_loss: 3.2026 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2791 - loss: 3.0141 - val_accuracy: 0.2797 - val_loss: 3.2014 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2822 - loss: 3.0176 - val_accuracy: 0.2797 - val_loss: 3.2027 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2813 - loss: 3.0176 - val_accuracy: 0.2801 - val_loss: 3.2038 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2867 - loss: 3.0030 - val_accuracy: 0.2804 - val_loss: 3.2029 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1570/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2883 - loss: 2.9934\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2882 - loss: 2.9936 - val_accuracy: 0.2809 - val_loss: 3.2024 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2844 - loss: 3.0098 - val_accuracy: 0.2806 - val_loss: 3.2032 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2860 - loss: 3.0064 - val_accuracy: 0.2811 - val_loss: 3.2038 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2894 - loss: 2.9850 - val_accuracy: 0.2808 - val_loss: 3.2035 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2857 - loss: 2.9983 - val_accuracy: 0.2801 - val_loss: 3.2026 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1580/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2888 - loss: 2.9879\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2888 - loss: 2.9879 - val_accuracy: 0.2808 - val_loss: 3.2031 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 2.9896 - val_accuracy: 0.2808 - val_loss: 3.2037 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2827 - loss: 3.0033 - val_accuracy: 0.2802 - val_loss: 3.2036 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2890 - loss: 2.9876 - val_accuracy: 0.2801 - val_loss: 3.2040 - learning_rate: 1.5625e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2887 - loss: 3.0008 - val_accuracy: 0.2801 - val_loss: 3.2042 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1588/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2834 - loss: 3.0087\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2835 - loss: 3.0087 - val_accuracy: 0.2801 - val_loss: 3.2040 - learning_rate: 1.5625e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2856 - loss: 2.9954 - val_accuracy: 0.2800 - val_loss: 3.2040 - learning_rate: 7.8125e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1592/1592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2893 - loss: 2.9953 - val_accuracy: 0.2806 - val_loss: 3.2040 - learning_rate: 7.8125e-06\n",
      "Epoch 46: early stopping\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Acurácia: 28.01%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.00      0.00      0.00        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.21      0.22      0.21        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.27      0.17      0.21        24\n",
      "      uc0016       0.15      0.09      0.11      1016\n",
      "      uc0017       0.19      0.02      0.04       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.19      0.01      0.01       811\n",
      "      uc0020       0.14      0.12      0.13         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.85      0.96      0.91        55\n",
      "      uc0024       0.31      0.35      0.33       673\n",
      "      uc0025       0.50      0.33      0.40         9\n",
      "   uc0025_01       0.38      0.04      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.17      0.08      0.11        26\n",
      "      uc0028       0.25      0.03      0.05       109\n",
      "      uc0029       0.36      0.75      0.49       205\n",
      "      uc0030       0.50      0.59      0.54       252\n",
      "      uc0031       0.34      0.21      0.26       574\n",
      "      uc0032       0.08      0.04      0.06        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.78      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.28      0.41      0.33        99\n",
      "      uc0040       0.41      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.39      0.46      0.42       156\n",
      "      uc0043       0.22      0.71      0.34      5188\n",
      "      uc0044       0.35      0.28      0.31       741\n",
      "      uc0045       0.27      0.10      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.24      0.02      0.04       164\n",
      "      uc0049       0.33      0.16      0.22        31\n",
      "      uc0050       0.25      0.12      0.17         8\n",
      "      uc0052       0.20      0.02      0.04        46\n",
      "      uc0053       0.32      0.16      0.21       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.13      0.04      0.06       175\n",
      "      uc0059       0.19      0.20      0.19       116\n",
      "      uc0060       0.43      0.27      0.33       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       1.00      0.07      0.12        15\n",
      "      uc0068       0.09      0.01      0.02       102\n",
      "      uc0069       0.29      0.47      0.36      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.60      0.46      1357\n",
      "      uc0076       0.50      0.01      0.03       200\n",
      "      uc0077       0.41      0.25      0.31       681\n",
      "      uc0078       0.19      0.05      0.08        57\n",
      "      uc0079       0.44      0.03      0.06       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.38      0.36      0.37       196\n",
      "      uc0087       0.19      0.07      0.10       334\n",
      "      uc0089       0.14      0.02      0.04        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.19      0.20      0.19        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.26      0.18      0.21        49\n",
      "      uc0094       0.21      0.02      0.04       785\n",
      "      uc0096       0.27      0.45      0.34      2791\n",
      "      uc0097       0.30      0.10      0.15        92\n",
      "      uc0098       0.64      0.54      0.58       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.00      0.00      0.00        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.08      0.05      0.06        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.43      0.65      0.52       330\n",
      "      uc0108       0.59      0.56      0.57        52\n",
      "      uc0109       0.20      0.02      0.03        58\n",
      "      uc0110       0.50      0.17      0.26        63\n",
      "      uc0111       0.11      0.01      0.01      1182\n",
      "      uc0112       0.08      0.02      0.03        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       260\n",
      "      uc0115       0.24      0.09      0.13        79\n",
      "      uc0116       0.24      0.19      0.21        31\n",
      "      uc0117       0.47      0.73      0.58        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.41      0.11      0.17       772\n",
      "      uc0125       0.29      0.24      0.26       115\n",
      "      uc0126       0.30      0.02      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.26      0.02      0.04       631\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.33      0.09      0.14       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.06      0.04      0.05        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.31      0.14      0.20       431\n",
      "      uc0140       0.08      0.03      0.04        36\n",
      "      uc0141       0.36      0.56      0.43        18\n",
      "      uc0142       0.00      0.00      0.00         2\n",
      "      uc0146       0.17      0.12      0.14      1858\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.41      0.11      0.18        80\n",
      "      uc0150       0.18      0.01      0.02       541\n",
      "      uc0153       0.22      0.20      0.21       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.54      0.42      0.47       149\n",
      "      uc0158       0.22      0.08      0.11        26\n",
      "      uc0159       0.50      0.20      0.29        15\n",
      "      uc0161       0.00      0.00      0.00         7\n",
      "      uc0162       0.21      0.15      0.18      1069\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.26      0.06      0.10       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.30      0.03      0.05       114\n",
      "      uc0171       0.00      0.00      0.00        31\n",
      "      uc0172       0.36      0.29      0.32        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.21      0.09      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.15      0.02      0.04       482\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.13      0.04      0.06        47\n",
      "      uc0187       0.00      0.00      0.00        23\n",
      "      uc0189       0.23      0.15      0.18        46\n",
      "      uc0190       0.13      0.10      0.11        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.17      0.08      0.11        53\n",
      "      uc0195       0.45      0.58      0.51        66\n",
      "      uc0197       0.21      0.18      0.19        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.51      0.20      0.29       409\n",
      "      uc0211       0.12      0.03      0.05       298\n",
      "      uc0212       0.34      0.27      0.30       246\n",
      "      uc0215       0.27      0.07      0.11       647\n",
      "      uc0216       0.31      0.45      0.37       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.34      0.72      0.46       120\n",
      "      uc0220       0.52      0.51      0.51        89\n",
      "      uc0221       0.52      0.19      0.28        83\n",
      "      uc0222       0.34      0.28      0.31      1003\n",
      "      uc0223       0.38      0.15      0.21        20\n",
      "      uc0225       0.50      0.38      0.43        24\n",
      "      uc0226       0.45      0.73      0.55       235\n",
      "      uc0228       0.10      0.04      0.06        25\n",
      "      uc0229       0.30      0.29      0.30        94\n",
      "      uc0230       0.30      0.13      0.18        78\n",
      "      uc0232       0.37      0.40      0.39      2066\n",
      "      uc0233       0.00      0.00      0.00        27\n",
      "      uc0234       0.00      0.00      0.00        80\n",
      "      uc0235       0.00      0.00      0.00       300\n",
      "      uc0238       0.49      0.37      0.42        90\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        28\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.37      0.21      0.27       107\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.25      0.03      0.05       120\n",
      "      uc1007       0.00      0.00      0.00        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.16      0.22      0.19        50\n",
      "      uc1010       0.43      0.57      0.49        70\n",
      "      uc1011       0.55      0.27      0.36        41\n",
      "      uc1012       0.22      0.16      0.19        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.06      0.03      0.04        39\n",
      "      uc1015       0.17      0.27      0.21        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.48      0.28      0.35       282\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        13\n",
      "      uc2002       0.54      0.25      0.34        28\n",
      "      uc2005       0.89      0.80      0.84        10\n",
      "      uc2006       0.92      0.32      0.48        37\n",
      "      uc2007       0.00      0.00      0.00        10\n",
      "      uc2008       0.50      0.19      0.27        32\n",
      "      uc2009       0.00      0.00      0.00         1\n",
      "      uc2011       0.55      0.67      0.60         9\n",
      "      uc2012       0.55      0.50      0.52        12\n",
      "      uc2014       0.25      0.04      0.07        25\n",
      "      uc2017       0.00      0.00      0.00         9\n",
      "      uc2019       0.41      0.33      0.36        40\n",
      "      uc2020       0.76      0.25      0.38        63\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.25      0.11      0.16       184\n",
      "      uc2025       0.00      0.00      0.00        24\n",
      "      uc2026       0.00      0.00      0.00        11\n",
      "      uc2027       0.75      0.16      0.26        19\n",
      "      uc2028       0.82      1.00      0.90        18\n",
      "      uc2029       0.29      0.40      0.34       224\n",
      "      uc2030       0.71      0.52      0.60        29\n",
      "      uc2031       0.00      0.00      0.00         5\n",
      "      uc2032       0.00      0.00      0.00        10\n",
      "      uc2033       0.00      0.00      0.00        88\n",
      "      uc2034       0.00      0.00      0.00        36\n",
      "      uc2036       0.62      0.24      0.35        33\n",
      "      uc2037       0.40      0.10      0.16        20\n",
      "      uc2039       0.40      0.17      0.24        35\n",
      "      uc2040       0.00      0.00      0.00        18\n",
      "      uc2041       0.00      0.00      0.00        19\n",
      "      uc2043       1.00      0.41      0.58        17\n",
      "      uc2044       0.00      0.00      0.00         2\n",
      "      uc2045       0.25      0.39      0.30        33\n",
      "      uc2046       0.00      0.00      0.00        18\n",
      "      uc2048       0.37      0.50      0.42        14\n",
      "      uc2049       0.92      0.63      0.75        19\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.57      0.40      0.47        10\n",
      "      uc2054       0.00      0.00      0.00         2\n",
      "      uc2055       0.44      0.57      0.50        14\n",
      "      uc2056       0.77      0.45      0.57        22\n",
      "      uc2059       0.25      0.66      0.37       380\n",
      "      uc2060       0.00      0.00      0.00        15\n",
      "      uc2061       0.00      0.00      0.00        22\n",
      "      uc2062       0.67      0.33      0.44        24\n",
      "      uc2063       0.89      0.62      0.73        13\n",
      "      uc2064       0.67      0.11      0.20        35\n",
      "      uc2065       0.47      0.46      0.46        35\n",
      "      uc2066       0.43      0.15      0.22        66\n",
      "      uc2067       0.00      0.00      0.00        20\n",
      "      uc2068       0.00      0.00      0.00        12\n",
      "      uc2071       0.17      0.05      0.08        19\n",
      "      uc2072       0.00      0.00      0.00         9\n",
      "      uc2073       0.62      0.48      0.54        21\n",
      "      uc2074       0.00      0.00      0.00         3\n",
      "      uc2075       0.55      0.43      0.48        14\n",
      "      uc2076       0.00      0.00      0.00        12\n",
      "      uc2077       0.00      0.00      0.00        11\n",
      "      uc2078       0.74      0.45      0.56        31\n",
      "      uc2079       0.00      0.00      0.00         7\n",
      "      uc2081       0.00      0.00      0.00         1\n",
      "      uc2082       0.71      0.42      0.53        12\n",
      "      uc2083       0.00      0.00      0.00        16\n",
      "      uc2084       0.24      0.27      0.25        15\n",
      "      uc2085       0.00      0.00      0.00         1\n",
      "      uc2086       0.00      0.00      0.00         8\n",
      "      uc2092       0.00      0.00      0.00        31\n",
      "      uc2095       0.10      0.21      0.14        19\n",
      "      uc2096       0.33      0.10      0.15        20\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2101       0.60      0.86      0.71        14\n",
      "\n",
      "    accuracy                           0.28     42442\n",
      "   macro avg       0.20      0.14      0.15     42442\n",
      "weighted avg       0.26      0.28      0.23     42442\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pipeline concluído com sucesso!\n",
      "Acurácia final: 28.01%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIniciando pipeline de Machine Learning ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar todo o pipeline principal\n",
    "modelo, historico, resultados = main(\n",
    "    data_path='../dados/processados/Dados_TechChallenge_Fase3.csv', \n",
    "    usuario='*',  \n",
    "    usuarios_exclusao=[\"usuario_00\", \"usuario_01\", \"usuario_02\", \"usuario_04\", \"usuario_06\", \"usuario_08\", \"usuario_11\", \"usuario_12\", \"usuario_13\"],\n",
    "    use_lstm=False,  \n",
    "    epochs=50,\n",
    "    plotar_resultado=False,\n",
    "    salvar_modelo=False\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nPipeline concluído com sucesso!\")\n",
    "print(f\"Acurácia final: {resultados['accuracy']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
