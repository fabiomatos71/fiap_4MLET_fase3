{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b74fd31",
   "metadata": {},
   "source": [
    "## Testa a utilização dos modelos LSTM ou Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9acc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate, Dropout, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f38ac",
   "metadata": {},
   "source": [
    "## Função filtra_usuario_separa_x_epoca_y()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b8acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_usuario_separa_x_epoca_y(df, username=\"*\", usuarios_exclusao=[]):\n",
    "    \"\"\"\n",
    "    Prepara os dados filtrando por usuário e criando features históricas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com os dados originais\n",
    "        username: Nome do usuário para filtrar (\"*\" para todos os usuários)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (features_sequenciais, features_contextuais, target)\n",
    "    \"\"\"\n",
    "    print(f\"Processando dados para usuário: {username}\")\n",
    "    \n",
    "    # Filtrar por usuário se especificado\n",
    "\n",
    "    if username != \"*\":\n",
    "        df_filtro = df[df[\"usuario\"] == username].copy()\n",
    "        print(f\"Registros após filtro de usuário: {len(df_filtro)}\")\n",
    "    else:\n",
    "        df_filtro = df[~df[\"usuario\"].isin(usuarios_exclusao)].copy()\n",
    "        print(f\"Processando todos os usuários (excluindo {len(usuarios_exclusao)} usuários): {len(df_filtro)} registros\")\n",
    "\n",
    "        # Ordenar por data/hora para manter sequência temporal\n",
    "        df_filtro = df_filtro.sort_values([\"usuario\", \"Dia\", \"Mes\", \"Ano\", \"DataHoraCriacao\"])\n",
    "\n",
    "    # Criar features históricas (últimos 3 casos de uso)\n",
    "    print(\"Criando features históricas...\")\n",
    "    for shift in range(1, 3):\n",
    "        df_filtro[f\"casoDeUso_{shift}\"] = df_filtro.groupby([\"usuario\", \"Dia\", \"Mes\", \"Ano\"])[\"casoDeUso\"].shift(shift).fillna(\"vazio\")\n",
    "\n",
    "    # Remover registros com NaN e resetar índice\n",
    "    df_filtro = df_filtro.dropna().reset_index(drop=True)\n",
    "    print(f\"Registros após limpeza: {len(df_filtro)}\")\n",
    "\n",
    "    # Separar target\n",
    "    df_target = df_filtro[\"casoDeUso\"]\n",
    "\n",
    "    # Preparar features sequenciais (remover colunas não necessárias)\n",
    "    cols_a_remover = [\"DataHoraCriacao\", \"Dia\", \"Mes\", \"Ano\", \"casoDeUso\", \"usuario\", \"PeriodoDoMes\"]\n",
    "    df_x = df_filtro.drop(columns=cols_a_remover)\n",
    "\n",
    "    # Preparar features contextuais (período do mês)\n",
    "    df_epoca = df_filtro[[\"PeriodoDoMes\"]].copy()\n",
    "\n",
    "    return df_x, df_epoca, df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a3b8f",
   "metadata": {},
   "source": [
    "## Função aplica_OneHotEncoder_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccfd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_OneHotEncoder_x(df, cols):\n",
    "    \"\"\"\n",
    "    Aplica One-Hot Encoding nas colunas especificadas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame para processar\n",
    "        cols: Lista de colunas para aplicar encoding\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame com encoding aplicado, OneHotEncoder fitted)\n",
    "    \"\"\"\n",
    "    print(f\"Aplicando One-Hot Encoding em: {cols}\")\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    df_ohe = pd.DataFrame(\n",
    "        ohe.fit_transform(df[cols]), \n",
    "        columns=ohe.get_feature_names_out(cols), \n",
    "        index=df.index\n",
    "    )\n",
    "    return pd.concat([df.drop(columns=cols), df_ohe], axis=1), ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf53f3",
   "metadata": {},
   "source": [
    "## Função preprocessar_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632f5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessar_dados(data_path, username=\"*\", usuarios_exclusao=[], test_size=0.4, random_state=42):\n",
    "    \"\"\"\n",
    "    Pipeline completo de preprocessamento de dados\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para o arquivo CSV\n",
    "        username: Usuário para filtrar\n",
    "        test_size: Proporção para teste\n",
    "        random_state: Seed para reprodutibilidade\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Dados de treino e teste preprocessados\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO PREPROCESSAMENTO ===\")\n",
    "    \n",
    "    # Carregar dados\n",
    "    print(f\"Carregando dados de: {data_path}\")\n",
    "    df = pd.read_csv(\n",
    "        data_path, \n",
    "        sep=';', \n",
    "        encoding='utf-8', \n",
    "        parse_dates=['DataHoraCriacao'], \n",
    "        dayfirst=True\n",
    "    )\n",
    "    print(f\"Dataset carregado: {df.shape}\")\n",
    "    \n",
    "    # Verificar distribuição das classes\n",
    "    print(\"\\nDistribuição das classes:\")\n",
    "    print(df['casoDeUso'].value_counts().head(10))\n",
    "    \n",
    "    # Processar dados\n",
    "    df_x, df_epoca, serie_y = filtra_usuario_separa_x_epoca_y(df, username, usuarios_exclusao)\n",
    "    \n",
    "    # Aplicar One-Hot Encoding nas features históricas\n",
    "    # df_x, ohe_x = aplica_OneHotEncoder_x(df_x, [\"casoDeUso_1\", \"casoDeUso_2\", \"casoDeUso_3\"])\n",
    "    df_x, ohe_x = aplica_OneHotEncoder_x(df_x, [\"casoDeUso_1\", \"casoDeUso_2\"])\n",
    "    \n",
    "    # Mapear período do mês para valores numéricos e normalizar\n",
    "    print(\"Processando features contextuais...\")\n",
    "    df_epoca[\"PeriodoDoMes\"] = df_epoca[\"PeriodoDoMes\"].map({\n",
    "        'antes_folha': 0, \n",
    "        'dia_folha': 1, \n",
    "        'apos_folha': 2\n",
    "    })\n",
    "    \n",
    "    # Normalizar features contextuais\n",
    "    scaler_epoca = StandardScaler()\n",
    "    df_epoca_scaled = pd.DataFrame(\n",
    "        scaler_epoca.fit_transform(df_epoca), \n",
    "        columns=df_epoca.columns,\n",
    "        index=df_epoca.index\n",
    "    )\n",
    "    \n",
    "    # Converter target para One-Hot\n",
    "    y_one_hot, ohe_y = aplica_OneHotEncoder_x(serie_y.to_frame(), [\"casoDeUso\"])\n",
    "    \n",
    "    # Normalizar features sequenciais\n",
    "    print(\"Normalizando features sequenciais...\")\n",
    "    scaler_seq = StandardScaler()\n",
    "    df_x_scaled = pd.DataFrame(\n",
    "        scaler_seq.fit_transform(df_x),\n",
    "        columns=df_x.columns,\n",
    "        index=df_x.index\n",
    "    )\n",
    "    \n",
    "    # Verificar se é possível fazer divisão estratificada\n",
    "    print(\"Dividindo dados em treino e teste...\")\n",
    "    \n",
    "    # Verificar classes com poucas amostras\n",
    "    class_counts = serie_y.value_counts()\n",
    "    classes_com_poucas_amostras = class_counts[class_counts < 2]\n",
    "    \n",
    "    if len(classes_com_poucas_amostras) > 0:\n",
    "        print(f\"⚠️ Aviso: {len(classes_com_poucas_amostras)} classes com apenas 1 amostra:\")\n",
    "        print(classes_com_poucas_amostras.head())\n",
    "        print(\"Removendo classes com poucas amostras para permitir estratificação...\")\n",
    "        \n",
    "        # Filtrar classes com pelo menos 2 amostras\n",
    "        classes_validas = class_counts[class_counts >= 2].index\n",
    "        mask = serie_y.isin(classes_validas)\n",
    "        \n",
    "        df_x_scaled = df_x_scaled[mask]\n",
    "        df_epoca_scaled = df_epoca_scaled[mask]\n",
    "        y_one_hot = y_one_hot[mask]\n",
    "        serie_y = serie_y[mask]\n",
    "        \n",
    "        print(f\"Dados após filtro: {len(df_x_scaled)} amostras, {len(serie_y.unique())} classes\")\n",
    "        \n",
    "        # Recriar one-hot encoding para classes restantes\n",
    "        y_one_hot, ohe_y = aplica_OneHotEncoder_x(serie_y.to_frame(), [\"casoDeUso\"])\n",
    "    \n",
    "    # Tentar divisão estratificada, se falhar usar divisão simples\n",
    "    try:\n",
    "        X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, y_train, y_test = train_test_split(\n",
    "            df_x_scaled, df_epoca_scaled, y_one_hot, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=serie_y  # Estratificação para manter proporção das classes\n",
    "        )\n",
    "        print(\"✅ Divisão estratificada realizada com sucesso\")\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Não foi possível fazer divisão estratificada: {str(e)}\")\n",
    "        print(\"Realizando divisão simples...\")\n",
    "        X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, y_train, y_test = train_test_split(\n",
    "            df_x_scaled, df_epoca_scaled, y_one_hot, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    print(f\"Treino: {X_train_seq.shape[0]} samples\")\n",
    "    print(f\"Teste: {X_test_seq.shape[0]} samples\")\n",
    "    \n",
    "    return (X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, \n",
    "            y_train, y_test, scaler_seq, scaler_epoca, ohe_x, ohe_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99b950",
   "metadata": {},
   "source": [
    "## função criar_modelo_hibrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a2a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_modelo_hibrido(input_seq_shape, input_epoch_shape, output_shape, \n",
    "                        use_lstm=True, lstm_units=64, dense_units=[128, 64], \n",
    "                        dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Cria modelo híbrido com opção de usar LSTM ou Dense\n",
    "    \n",
    "    Args:\n",
    "        input_seq_shape: Shape das features sequenciais\n",
    "        input_epoch_shape: Shape das features contextuais\n",
    "        output_shape: Número de classes de saída\n",
    "        use_lstm: Se True, usa LSTM; se False, usa Dense\n",
    "        lstm_units: Unidades LSTM\n",
    "        dense_units: Lista com unidades das camadas Dense\n",
    "        dropout_rate: Taxa de dropout\n",
    "    \n",
    "    Returns:\n",
    "        Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    print(f\"=== CRIANDO MODELO {'LSTM' if use_lstm else 'DENSE'} HÍBRIDO ===\")\n",
    "    \n",
    "    # Input para features sequenciais\n",
    "    input_seq = Input(shape=(input_seq_shape,), name='input_sequence')\n",
    "    input_epoch = Input(shape=(input_epoch_shape,), name='input_epoch')\n",
    "    \n",
    "    if use_lstm:\n",
    "        # Para LSTM, precisamos reshapear para 3D (samples, timesteps, features)\n",
    "        # Assumindo que cada feature é um timestep\n",
    "        x = Reshape((input_seq_shape, 1))(input_seq)\n",
    "        x = LSTM(lstm_units, return_sequences=False, dropout=dropout_rate)(x)\n",
    "        print(f\"Usando LSTM com {lstm_units} unidades\")\n",
    "    else:\n",
    "        # Usar camadas Dense tradicionais\n",
    "        x = input_seq\n",
    "        for i, units in enumerate(dense_units):\n",
    "            x = Dense(units, activation='relu', name=f'dense_{i+1}')(x)\n",
    "            x = Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "        print(f\"Usando Dense layers: {dense_units}\")\n",
    "    \n",
    "    # Combinar features sequenciais com contextuais\n",
    "    combined = Concatenate(name='concatenate')([x, input_epoch])\n",
    "    \n",
    "    # Camada de saída\n",
    "    output = Dense(output_shape, activation='softmax', name='output')(combined)\n",
    "    \n",
    "    # Criar modelo\n",
    "    model = Model(inputs=[input_seq, input_epoch], outputs=output)\n",
    "    \n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo criado e compilado!\")\n",
    "    print(f\"Parâmetros totais: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a396405",
   "metadata": {},
   "source": [
    "## função treinar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b83292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo(model, X_train_seq, X_train_epoch, y_train, \n",
    "                  epochs=50, validation_split=0.2, verbose=1):\n",
    "    \"\"\"\n",
    "    Treina o modelo com callbacks para early stopping e redução de learning rate\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser treinado\n",
    "        X_train_seq, X_train_epoch: Features de treino\n",
    "        y_train: Target de treino\n",
    "        epochs: Número máximo de épocas\n",
    "        validation_split: Proporção para validação\n",
    "        verbose: Verbosidade do treinamento\n",
    "    \n",
    "    Returns:\n",
    "        History: Histórico do treinamento\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO TREINAMENTO ===\")\n",
    "    \n",
    "    # Callbacks para melhorar o treinamento\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Treinar modelo\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_epoch], \n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    print(\"Treinamento concluído!\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa75ba6",
   "metadata": {},
   "source": [
    "## Função avaliar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1538f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(model, X_test_seq, X_test_epoch, y_test, ohe_y):\n",
    "    \"\"\"\n",
    "    Avalia o modelo com múltiplas métricas\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo treinado\n",
    "        X_test_seq, X_test_epoch: Features de teste\n",
    "        y_test: Target de teste\n",
    "        ohe_y: OneHotEncoder do target para obter nomes das classes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário com métricas de avaliação\n",
    "    \"\"\"\n",
    "    print(\"=== AVALIANDO MODELO ===\")\n",
    "    \n",
    "    # Predições\n",
    "    y_pred_proba = model.predict([X_test_seq, X_test_epoch])\n",
    "    y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "    y_test_labels = np.argmax(y_test.values, axis=-1)\n",
    "    \n",
    "    # Métricas básicas\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Relatório de classificação\n",
    "    class_names = ohe_y.get_feature_names_out(['casoDeUso'])\n",
    "    class_names = [name.replace('casoDeUso_', '') for name in class_names]\n",
    "    \n",
    "    print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
    "    print(classification_report(y_test_labels, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'y_test': y_test_labels,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_names': class_names,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68b79c",
   "metadata": {},
   "source": [
    "## Função salvar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7880be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_modelo(model, scaler_seq, scaler_epoca, ohe_x, ohe_y, path_base='modelos_notebook'):\n",
    "    \"\"\"\n",
    "    Salva o modelo Keras e os transformadores necessários para inferência futura.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras treinado.\n",
    "        scaler_seq: Scaler das features sequenciais.\n",
    "        scaler_epoca: Scaler das features contextuais.\n",
    "        ohe_x: OneHotEncoder das features históricas.\n",
    "        ohe_y: OneHotEncoder do target.\n",
    "        path_base: Pasta/caminho base para salvar os arquivos.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(path_base, exist_ok=True)\n",
    "\n",
    "    # Salva o modelo keras\n",
    "    model.save(os.path.join(path_base, 'modelo.keras'))\n",
    "\n",
    "    # Salva os transformadores com pickle\n",
    "    with open(os.path.join(path_base, 'scaler_seq.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_seq, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'scaler_epoca.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler_epoca, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'ohe_x.pkl'), 'wb') as f:\n",
    "        pickle.dump(ohe_x, f)\n",
    "\n",
    "    with open(os.path.join(path_base, 'ohe_y.pkl'), 'wb') as f:\n",
    "        pickle.dump(ohe_y, f)\n",
    "\n",
    "    print(f\"\\n✅ Modelo e transformadores salvos na pasta '{path_base}'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3f9fb",
   "metadata": {},
   "source": [
    "## Função plotar_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_resultados(history, results):\n",
    "    \"\"\"\n",
    "    Plota gráficos de treinamento e matriz de confusão\n",
    "    \n",
    "    Args:\n",
    "        history: Histórico do treinamento\n",
    "        results: Resultados da avaliação\n",
    "    \"\"\"\n",
    "    print(\"Gerando visualizações...\")\n",
    "    \n",
    "    # Configurar estilo dos gráficos\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Acurácia durante treinamento\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Validação', linewidth=2)\n",
    "    plt.title('Acurácia Durante o Treinamento', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Perda durante treinamento\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validação', linewidth=2)\n",
    "    plt.title('Perda Durante o Treinamento', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Learning Rate (se disponível)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    if 'lr' in history.history:\n",
    "        plt.plot(history.history['lr'], linewidth=2, color='red')\n",
    "        plt.title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Learning Rate\\nnão disponível', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Matriz de confusão\n",
    "    ax4 = plt.subplot(2, 3, (4, 6))\n",
    "    sns.heatmap(\n",
    "        results['confusion_matrix'], \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=results['class_names'],\n",
    "        yticklabels=results['class_names']\n",
    "    )\n",
    "    plt.title('Matriz de Confusão', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    \n",
    "    # 5. Distribuição de confiança das predições\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    max_proba = np.max(results['y_pred_proba'], axis=1)\n",
    "    plt.hist(max_proba, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribuição de Confiança das Predições', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Confiança Máxima')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af53c5",
   "metadata": {},
   "source": [
    "## Função Principal que executa todo o pipeline\n",
    "\n",
    "Esta função:\n",
    "\n",
    "1 - Préprocessa os dados:\n",
    "\n",
    "1.1 - filtrando os usuários que serão utilizados para treinamento\n",
    "\n",
    "1.2 - Gerando linhas com dois casos de uso históricos maio o caso de uso alvo/target\n",
    "\n",
    "1.3 - Separa a massa de dados de treino e avaliação do treino\n",
    "\n",
    "2 - Cria um modelo hibrido, podendo ser:\n",
    "\n",
    "2.1 - LSTM\n",
    "\n",
    "2.2 - Dense layers\n",
    "\n",
    "3 - Treina o modelo hibrido com os dados do passo 1\n",
    "\n",
    "4 - Avaliar o modelo treinado com a base de dados separada para avaliação no passo 1\n",
    "\n",
    "5 - Plota gráficos de avaliação dos resultados\n",
    "\n",
    "6 - Salva o modelo e seus parâmetros de configuração para posterior utilização nas previsões (aqui no notebook, será salvo abaixo da pasta notebooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc2ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='dados.csv', \n",
    "         usuario='*', # Nome do usuário ou '*' para todos\n",
    "         use_lstm=False,  # Usar Dense layers ou LSTM\n",
    "         epochs=50,\n",
    "         plotar_resultado=True, # Se deve ou não chamar o método plotar_resultados()\n",
    "         usuarios_exclusao=[], # No caso de usar '*' em usuario, lista de exclusão da base de dados.  Não carregar os dados desses.\n",
    "         salvar_modelo=False\n",
    "         ):  \n",
    "    \"\"\"\n",
    "    Função principal que executa todo o pipeline\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para os dados\n",
    "        usuario: Usuário para filtrar\n",
    "        use_lstm: Se usar LSTM ou Dense layers\n",
    "        epochs: Número de épocas para treinamento\n",
    "        plotar_resultado: Se deve ou não chamar o método plotar_resultados() ao final do treino e testes\n",
    "        usuarios_exclusao: Lista de usuários a não serem considerados se o usuario='*'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocessamento\n",
    "        (X_train_seq, X_test_seq, X_train_epoch, X_test_epoch, \n",
    "         y_train, y_test, scaler_seq, scaler_epoca, ohe_x, ohe_y) = preprocessar_dados(data_path, usuario, usuarios_exclusao)\n",
    "        \n",
    "        # Criar modelo\n",
    "        modelo = criar_modelo_hibrido(\n",
    "            input_seq_shape=X_train_seq.shape[1],\n",
    "            input_epoch_shape=X_train_epoch.shape[1],\n",
    "            output_shape=y_train.shape[1],\n",
    "            use_lstm=use_lstm\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\\n=== ARQUITETURA DO MODELO ===\")\n",
    "        modelo.summary()\n",
    "        \n",
    "        # Treinar modelo\n",
    "        historico = treinar_modelo(\n",
    "            modelo, X_train_seq, X_train_epoch, y_train, \n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        # Avaliar modelo\n",
    "        resultados = avaliar_modelo(\n",
    "            modelo, X_test_seq, X_test_epoch, y_test, ohe_y\n",
    "        )\n",
    "        \n",
    "        # Plotar resultados\n",
    "        if plotar_resultado:\n",
    "            plotar_resultados(historico, resultados)\n",
    "        \n",
    "        if salvar_modelo:\n",
    "            salvar_modelo(modelo, scaler_seq, scaler_epoca, ohe_x, ohe_y, path_base='modelos_notebook')\n",
    "\n",
    "        return modelo, historico, resultados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante execução: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7e821",
   "metadata": {},
   "source": [
    "## Execução do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0734399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando pipeline de Machine Learning ...\n",
      "============================================================\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 114874 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 114874\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "uc2013    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 114864 amostras, 293 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 68918 samples\n",
      "Teste: 45946 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 105,034\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 14:35:33.081907: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">604</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,440</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">293</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,338</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m604\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,440\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m293\u001b[0m)       │     \u001b[38;5;34m19,338\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,034</span> (410.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,034\u001b[0m (410.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,034</span> (410.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,034\u001b[0m (410.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1535 - loss: 4.2225 - val_accuracy: 0.2673 - val_loss: 3.3327 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2457 - loss: 3.4265 - val_accuracy: 0.2797 - val_loss: 3.2343 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2584 - loss: 3.3085 - val_accuracy: 0.2774 - val_loss: 3.2052 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2624 - loss: 3.2658 - val_accuracy: 0.2824 - val_loss: 3.1793 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2671 - loss: 3.2090 - val_accuracy: 0.2847 - val_loss: 3.1785 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2688 - loss: 3.1933 - val_accuracy: 0.2865 - val_loss: 3.1750 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2689 - loss: 3.1739 - val_accuracy: 0.2858 - val_loss: 3.1734 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27985us/step - accuracy: 0.2676 - loss: 3.1569 - val_accuracy: 0.2842 - val_loss: 3.1754 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2676 - loss: 3.1692 - val_accuracy: 0.2864 - val_loss: 3.1820 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2688 - loss: 3.1468 - val_accuracy: 0.2866 - val_loss: 3.1743 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2704 - loss: 3.1359 - val_accuracy: 0.2856 - val_loss: 3.1826 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1143/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 49ms/step - accuracy: 0.2700 - loss: 3.1432\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2703 - loss: 3.1440 - val_accuracy: 0.2876 - val_loss: 3.1845 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2774 - loss: 3.0931 - val_accuracy: 0.2881 - val_loss: 3.1718 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2754 - loss: 3.0859 - val_accuracy: 0.2874 - val_loss: 3.1753 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2795 - loss: 3.0731 - val_accuracy: 0.2908 - val_loss: 3.1784 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2792 - loss: 3.0568 - val_accuracy: 0.2901 - val_loss: 3.1788 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2790 - loss: 3.0571 - val_accuracy: 0.2915 - val_loss: 3.1794 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m 402/1723\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:57\u001b[0m 134ms/step - accuracy: 0.2808 - loss: 3.0457\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2806 - loss: 3.0480 - val_accuracy: 0.2916 - val_loss: 3.1770 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2865 - loss: 3.0267 - val_accuracy: 0.2924 - val_loss: 3.1740 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2845 - loss: 3.0186 - val_accuracy: 0.2932 - val_loss: 3.1799 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2826 - loss: 3.0220 - val_accuracy: 0.2914 - val_loss: 3.1783 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2855 - loss: 3.0137 - val_accuracy: 0.2918 - val_loss: 3.1782 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1710/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2843 - loss: 3.0005\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2843 - loss: 3.0007 - val_accuracy: 0.2909 - val_loss: 3.1839 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2837 - loss: 3.0142 - val_accuracy: 0.2917 - val_loss: 3.1832 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28447us/step - accuracy: 0.2875 - loss: 2.9963 - val_accuracy: 0.2921 - val_loss: 3.1811 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 3.0018 - val_accuracy: 0.2919 - val_loss: 3.1827 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2839 - loss: 2.9966 - val_accuracy: 0.2923 - val_loss: 3.1850 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1227/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.2915 - loss: 2.9757\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2906 - loss: 2.9808 - val_accuracy: 0.2927 - val_loss: 3.1840 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2901 - loss: 2.9820 - val_accuracy: 0.2929 - val_loss: 3.1846 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2874 - loss: 2.9873 - val_accuracy: 0.2937 - val_loss: 3.1849 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2889 - loss: 2.9769 - val_accuracy: 0.2929 - val_loss: 3.1852 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28484us/step - accuracy: 0.2863 - loss: 2.9903 - val_accuracy: 0.2947 - val_loss: 3.1851 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m 484/1723\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 112ms/step - accuracy: 0.2882 - loss: 2.9891\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2870 - loss: 2.9862 - val_accuracy: 0.2939 - val_loss: 3.1854 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2908 - loss: 2.9633 - val_accuracy: 0.2940 - val_loss: 3.1852 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2876 - loss: 2.9858 - val_accuracy: 0.2937 - val_loss: 3.1855 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2889 - loss: 2.9817 - val_accuracy: 0.2937 - val_loss: 3.1865 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2889 - loss: 2.9873 - val_accuracy: 0.2939 - val_loss: 3.1868 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1705/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2886 - loss: 2.9720\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9721 - val_accuracy: 0.2938 - val_loss: 3.1870 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2874 - loss: 2.9832 - val_accuracy: 0.2937 - val_loss: 3.1871 - learning_rate: 1.5625e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2865 - loss: 2.9839 - val_accuracy: 0.2940 - val_loss: 3.1877 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2874 - loss: 2.9879 - val_accuracy: 0.2938 - val_loss: 3.1879 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1723/1723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2918 - loss: 2.9676 - val_accuracy: 0.2935 - val_loss: 3.1882 - learning_rate: 1.5625e-05\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1436/1436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step \n",
      "Acurácia: 28.59%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.00      0.00      0.00        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.23      0.24      0.23        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.27      0.12      0.17        24\n",
      "      uc0016       0.16      0.08      0.11      1016\n",
      "      uc0017       0.15      0.03      0.05       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.14      0.01      0.02       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.86      1.00      0.92        55\n",
      "      uc0024       0.32      0.33      0.33       674\n",
      "      uc0025       1.00      0.44      0.62         9\n",
      "   uc0025_01       0.09      0.06      0.07        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.29      0.19      0.23        26\n",
      "      uc0028       0.08      0.01      0.02       109\n",
      "      uc0029       0.45      0.67      0.54       205\n",
      "      uc0030       0.49      0.60      0.54       252\n",
      "      uc0031       0.39      0.19      0.26       574\n",
      "      uc0032       0.26      0.15      0.19        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.58      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.28      0.38      0.32        99\n",
      "      uc0040       0.41      0.37      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.48      0.47      0.47       156\n",
      "      uc0043       0.21      0.70      0.33      5240\n",
      "      uc0044       0.41      0.28      0.34       741\n",
      "      uc0045       0.37      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.50      0.29      0.37        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.43      0.13      0.20       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.15      0.04      0.06       176\n",
      "      uc0059       0.21      0.22      0.21       116\n",
      "      uc0060       0.47      0.23      0.30       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.47      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.36      0.60      0.45      1358\n",
      "      uc0076       0.09      0.01      0.01       200\n",
      "      uc0077       0.46      0.25      0.32       681\n",
      "      uc0078       0.45      0.09      0.15        57\n",
      "      uc0079       0.18      0.02      0.04       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.12      0.01      0.02        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.34      0.38      0.36       196\n",
      "      uc0087       0.22      0.09      0.13       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.14      0.07      0.09        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.32      0.14      0.20        49\n",
      "      uc0094       0.30      0.01      0.01       785\n",
      "      uc0096       0.29      0.44      0.35      2817\n",
      "      uc0097       0.34      0.12      0.18        92\n",
      "      uc0098       0.67      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.23      0.15      0.18        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.40      0.61      0.48       330\n",
      "      uc0108       0.62      0.62      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.14      0.02      0.04      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.17      0.03      0.04        79\n",
      "      uc0116       0.12      0.06      0.08        31\n",
      "      uc0117       0.48      0.62      0.54        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.42      0.11      0.17       772\n",
      "      uc0125       0.32      0.27      0.29       115\n",
      "      uc0126       0.32      0.02      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.16      0.05      0.08       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.32      0.14      0.19       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.00      0.00      0.00        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.22      0.28      0.25       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.23      0.39      0.29        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.15      0.17      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.56      0.11      0.19        80\n",
      "      uc0150       0.17      0.01      0.02       541\n",
      "      uc0153       0.13      0.12      0.12       118\n",
      "      uc0155       0.00      0.00      0.00         1\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.44      0.36      0.39       149\n",
      "      uc0158       0.42      0.19      0.26        26\n",
      "      uc0159       0.00      0.00      0.00        15\n",
      "      uc0161       0.14      0.14      0.14         7\n",
      "      uc0162       0.24      0.18      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         1\n",
      "      uc0165       0.22      0.07      0.10       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.12      0.04      0.05       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.30      0.21      0.25        34\n",
      "      uc0173       0.29      0.12      0.17        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.23      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.24      0.07      0.11        54\n",
      "      uc0195       0.55      0.42      0.48        66\n",
      "      uc0197       0.33      0.16      0.21        38\n",
      "      uc0198       0.57      0.03      0.05       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         1\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         1\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.49      0.20      0.28       409\n",
      "      uc0211       0.26      0.09      0.14       298\n",
      "      uc0212       0.38      0.26      0.31       271\n",
      "      uc0215       0.23      0.08      0.12       647\n",
      "      uc0216       0.25      0.44      0.32       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.39      0.77      0.52       120\n",
      "      uc0220       0.37      0.51      0.43        89\n",
      "      uc0221       0.36      0.14      0.21        90\n",
      "      uc0222       0.36      0.36      0.36      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.29      0.17      0.21        24\n",
      "      uc0226       0.44      0.74      0.55       235\n",
      "      uc0228       0.12      0.12      0.12        25\n",
      "      uc0229       0.34      0.35      0.34        94\n",
      "      uc0230       0.31      0.13      0.18        78\n",
      "      uc0232       0.46      0.49      0.47      2959\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.23      0.05      0.08       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.57      0.63      0.60       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.50      0.05      0.10        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.20      0.05      0.08        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.50      0.06      0.11        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.29      0.17      0.21       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.18      0.18      0.18        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.14      0.14      0.14        50\n",
      "      uc1010       0.42      0.54      0.47        70\n",
      "      uc1011       0.38      0.34      0.36        41\n",
      "      uc1012       0.14      0.06      0.09        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.22      0.08      0.11        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.32      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        22\n",
      "      uc2002       0.73      0.38      0.50        42\n",
      "      uc2005       0.67      0.47      0.55        17\n",
      "      uc2006       0.65      0.31      0.42        49\n",
      "      uc2007       0.00      0.00      0.00        16\n",
      "      uc2008       0.36      0.36      0.36        47\n",
      "      uc2009       0.00      0.00      0.00         1\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.38      0.43      0.40        14\n",
      "      uc2012       0.70      0.32      0.44        22\n",
      "      uc2014       0.00      0.00      0.00        35\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.41      0.24      0.31        70\n",
      "      uc2020       0.55      0.17      0.26       102\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.12      0.08      0.10       347\n",
      "      uc2025       0.00      0.00      0.00        42\n",
      "      uc2026       0.25      0.06      0.10        16\n",
      "      uc2027       0.57      0.24      0.34        33\n",
      "      uc2028       0.74      0.85      0.79        34\n",
      "      uc2029       0.28      0.42      0.34       320\n",
      "      uc2030       0.46      0.29      0.35        42\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        28\n",
      "      uc2033       0.00      0.00      0.00       120\n",
      "      uc2034       0.00      0.00      0.00        74\n",
      "      uc2036       1.00      0.23      0.37        40\n",
      "      uc2037       0.29      0.15      0.20        33\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.00      0.00      0.00        52\n",
      "      uc2040       0.00      0.00      0.00        21\n",
      "      uc2041       0.00      0.00      0.00        28\n",
      "      uc2043       0.67      0.44      0.53        18\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.64      0.20      0.31        44\n",
      "      uc2046       0.00      0.00      0.00        24\n",
      "      uc2048       0.65      0.61      0.63        18\n",
      "      uc2049       0.42      0.50      0.46        34\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        15\n",
      "      uc2054       0.00      0.00      0.00         2\n",
      "      uc2055       0.30      0.17      0.21        18\n",
      "      uc2056       0.56      0.26      0.36        38\n",
      "      uc2057       0.00      0.00      0.00        31\n",
      "      uc2059       0.23      0.62      0.33       514\n",
      "      uc2060       0.25      0.10      0.14        20\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.41      0.35      0.38        31\n",
      "      uc2063       0.58      0.39      0.47        18\n",
      "      uc2064       0.00      0.00      0.00        56\n",
      "      uc2065       0.32      0.44      0.37        62\n",
      "      uc2066       0.43      0.20      0.27        95\n",
      "      uc2067       0.14      0.03      0.06        29\n",
      "      uc2068       0.00      0.00      0.00        19\n",
      "      uc2071       0.29      0.20      0.24        20\n",
      "      uc2072       1.00      0.19      0.32        16\n",
      "      uc2073       0.82      0.39      0.53        23\n",
      "      uc2074       0.00      0.00      0.00         7\n",
      "      uc2075       0.33      0.43      0.38        21\n",
      "      uc2076       0.00      0.00      0.00        26\n",
      "      uc2077       0.00      0.00      0.00        18\n",
      "      uc2078       0.44      0.37      0.40        46\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         1\n",
      "      uc2082       0.43      0.50      0.46        12\n",
      "      uc2083       0.00      0.00      0.00        31\n",
      "      uc2084       0.38      0.42      0.40        24\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        21\n",
      "      uc2087       0.76      0.13      0.23       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.35      0.37      0.36        19\n",
      "      uc2092       0.17      0.01      0.02        96\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.23      0.12      0.16        25\n",
      "      uc2096       0.50      0.03      0.06        29\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         4\n",
      "      uc2100       0.00      0.00      0.00         1\n",
      "      uc2101       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.29     45946\n",
      "   macro avg       0.18      0.13      0.14     45946\n",
      "weighted avg       0.27      0.29      0.24     45946\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pipeline concluído com sucesso!\n",
      "Acurácia final: 28.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIniciando pipeline de Machine Learning ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar todo o pipeline principal\n",
    "modelo, historico, resultados = main(\n",
    "    data_path='../dados/processados/Dados_TechChallenge_Fase3.csv', \n",
    "    usuario='*',  \n",
    "    usuarios_exclusao=[\"usuario_00\"],\n",
    "    use_lstm=False,  \n",
    "    epochs=50,\n",
    "    plotar_resultado=False,\n",
    "    salvar_modelo=False\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nPipeline concluído com sucesso!\")\n",
    "print(f\"Acurácia final: {resultados['accuracy']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
