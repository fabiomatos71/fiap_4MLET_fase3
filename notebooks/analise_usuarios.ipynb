{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate, Dropout, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função listar_usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_usuarios(data_path):\n",
    "    \"\"\"\n",
    "    retornar todos os usuários do arquivo\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para o arquivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        list: Nomes de todos os usuários do arquivo\n",
    "    \"\"\"\n",
    "    print(\"=== OBTENDO USUÁRIOS DO ARQUIVO ===\")\n",
    "    \n",
    "    # Carregar dados\n",
    "    df = pd.read_csv(\n",
    "        data_path, \n",
    "        sep=';', \n",
    "        encoding='utf-8', \n",
    "        parse_dates=['DataHoraCriacao'], \n",
    "        dayfirst=True\n",
    "    )\n",
    "    \n",
    "    return df['usuario'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função analisar_distribuicao_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_distribuicao_classes(serie_y, username):\n",
    "    \"\"\"\n",
    "    Analisa a distribuição das classes para o usuário\n",
    "    \n",
    "    Args:\n",
    "        serie_y: Série com os targets\n",
    "        username: Nome do usuário\n",
    "    \n",
    "    Returns:\n",
    "        dict: Estatísticas das classes\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== ANÁLISE DE CLASSES PARA {username} ===\")\n",
    "    \n",
    "    class_counts = serie_y.value_counts()\n",
    "    print(f\"Total de classes: {len(class_counts)}\")\n",
    "    print(f\"Total de amostras: {len(serie_y)}\")\n",
    "    print(f\"Média de amostras por classe: {class_counts.mean():.1f}\")\n",
    "    print(f\"Mediana de amostras por classe: {class_counts.median():.1f}\")\n",
    "    \n",
    "    # Classes com poucas amostras\n",
    "    classes_com_1_amostra = class_counts[class_counts == 1]\n",
    "    classes_com_2_5_amostras = class_counts[(class_counts >= 2) & (class_counts <= 5)]\n",
    "    \n",
    "    print(f\"\\nClasses com 1 amostra: {len(classes_com_1_amostra)}\")\n",
    "    print(f\"Classes com 2-5 amostras: {len(classes_com_2_5_amostras)}\")\n",
    "    print(f\"Classes com 6+ amostras: {len(class_counts[class_counts > 5])}\")\n",
    "    \n",
    "    if len(classes_com_1_amostra) > 0:\n",
    "        print(f\"\\n⚠️ Classes problemáticas (1 amostra):\")\n",
    "        print(classes_com_1_amostra.head(10))\n",
    "    \n",
    "    # Top classes\n",
    "    print(f\"\\n📊 Top 10 classes mais frequentes:\")\n",
    "    print(class_counts.head(10))\n",
    "    \n",
    "    return {\n",
    "        'total_classes': len(class_counts),\n",
    "        'total_samples': len(serie_y),\n",
    "        'classes_with_one_sample': len(classes_com_1_amostra),\n",
    "        'class_counts': class_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função recomendar_usuarios\n",
    "\n",
    "Analisa a quantidade de amostras e de classes(casos de uso) envolvidas \n",
    "\n",
    "- data_path: Caminho para o arquivo CSV\n",
    "- min_amostras: Número mínimo de amostras por usuário\n",
    "- min_classes: Número mínimo de classes diferentes por usuário\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_amostras=50\n",
    "min_classes=5\n",
    "\n",
    "print(\"=== ANALISANDO USUÁRIOS PARA RECOMENDAÇÃO ===\")\n",
    "\n",
    "# Carregar dados\n",
    "df = pd.read_csv(\n",
    "    data_path='../dados/processados/Dados_TechChallenge_Fase3.csv', \n",
    "    sep=';', \n",
    "    encoding='utf-8', \n",
    "    parse_dates=['DataHoraCriacao'], \n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "usuarios_stats = []\n",
    "\n",
    "for usuario in df['usuario'].unique():\n",
    "    df_user = df[df['usuario'] == usuario]\n",
    "    class_counts = df_user['casoDeUso'].value_counts()\n",
    "    \n",
    "    # Calcular métricas de qualidade\n",
    "    total_amostras = len(df_user)\n",
    "    total_classes = len(class_counts)\n",
    "    classes_com_uma_amostra = len(class_counts[class_counts == 1])\n",
    "    classes_com_multiplas_amostras = len(class_counts[class_counts > 1])\n",
    "    media_amostras_por_classe = class_counts.mean()\n",
    "    balanceamento = 1 - (class_counts.std() / class_counts.mean()) if class_counts.mean() > 0 else 0\n",
    "    \n",
    "    # Score de qualidade (0-1, onde 1 é melhor)\n",
    "    score_amostras = min(total_amostras / 200, 1.0)  # Normaliza até 200 amostras\n",
    "    score_classes = min(total_classes / 20, 1.0)  # Normaliza até 20 classes\n",
    "    score_balanceamento = max(0, balanceamento)  # Evita valores negativos\n",
    "    score_sem_singletons = classes_com_multiplas_amostras / total_classes if total_classes > 0 else 0\n",
    "    \n",
    "    # Score final ponderado\n",
    "    score_final = (\n",
    "        0.3 * score_amostras + \n",
    "        0.3 * score_classes + \n",
    "        0.2 * score_balanceamento + \n",
    "        0.2 * score_sem_singletons\n",
    "    )\n",
    "    \n",
    "    usuarios_stats.append({\n",
    "        'usuario': usuario,\n",
    "        'total_amostras': total_amostras,\n",
    "        'total_classes': total_classes,\n",
    "        'classes_com_uma_amostra': classes_com_uma_amostra,\n",
    "        'classes_com_multiplas_amostras': classes_com_multiplas_amostras,\n",
    "        'media_amostras_por_classe': media_amostras_por_classe,\n",
    "        'balanceamento': balanceamento,\n",
    "        'score_final': score_final\n",
    "    })\n",
    "\n",
    "# Ordenar por score final\n",
    "usuarios_stats.sort(key=lambda x: x['score_final'], reverse=True)\n",
    "\n",
    "# Filtrar usuários que atendem critérios mínimos\n",
    "usuarios_validos = [\n",
    "    u for u in usuarios_stats \n",
    "    if u['total_amostras'] >= min_amostras and u['total_classes'] >= min_classes\n",
    "]\n",
    "\n",
    "print(f\"\\n📊 Top 10 usuários recomendados:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Usuário':<12} {'Amostras':<9} {'Classes':<8} {'Singleton':<10} {'Score':<8} {'Qualidade'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, user_stats in enumerate(usuarios_validos[:10]):\n",
    "    qualidade = \"Excelente\" if user_stats['score_final'] > 0.7 else \"Boa\" if user_stats['score_final'] > 0.5 else \"Regular\"\n",
    "    print(f\"{user_stats['usuario']:<12} {user_stats['total_amostras']:<9} {user_stats['total_classes']:<8} \"\n",
    "            f\"{user_stats['classes_com_uma_amostra']:<10} {user_stats['score_final']:.3f}     {qualidade}\")\n",
    "\n",
    "if not usuarios_validos:\n",
    "    print(\"⚠️ Nenhum usuário atende os critérios mínimos especificados.\")\n",
    "    return usuarios_stats[:5]  # Retorna os 5 melhores mesmo que não atendam critérios\n",
    "\n",
    "return usuarios_validos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALISANDO USUÁRIOS PARA RECOMENDAÇÃO ===\n",
      "\n",
      "📊 Top 10 usuários recomendados:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Usuário      Amostras  Classes  Singleton  Score    Qualidade\n",
      "----------------------------------------------------------------------------------------------------\n",
      "usuario_04   2666      76       2          0.795     Excelente\n",
      "usuario_03   21832     189      10         0.789     Excelente\n",
      "usuario_05   15783     166      13         0.784     Excelente\n",
      "usuario_07   8189      180      16         0.782     Excelente\n",
      "usuario_14   13695     135      15         0.778     Excelente\n",
      "usuario_15   24020     190      22         0.777     Excelente\n",
      "usuario_00   717       69       8          0.777     Excelente\n",
      "usuario_10   20070     186      25         0.773     Excelente\n",
      "usuario_08   3291      21       3          0.771     Excelente\n",
      "usuario_09   2497      126      24         0.762     Excelente\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'usuario': 'usuario_04',\n",
       "  'total_amostras': 2666,\n",
       "  'total_classes': 76,\n",
       "  'classes_com_uma_amostra': 2,\n",
       "  'classes_com_multiplas_amostras': 74,\n",
       "  'media_amostras_por_classe': np.float64(35.078947368421055),\n",
       "  'balanceamento': np.float64(-0.6061146506119008),\n",
       "  'score_final': 0.7947368421052632},\n",
       " {'usuario': 'usuario_03',\n",
       "  'total_amostras': 21832,\n",
       "  'total_classes': 189,\n",
       "  'classes_com_uma_amostra': 10,\n",
       "  'classes_com_multiplas_amostras': 179,\n",
       "  'media_amostras_por_classe': np.float64(115.51322751322752),\n",
       "  'balanceamento': np.float64(-1.729370023793121),\n",
       "  'score_final': 0.7894179894179894},\n",
       " {'usuario': 'usuario_05',\n",
       "  'total_amostras': 15783,\n",
       "  'total_classes': 166,\n",
       "  'classes_com_uma_amostra': 13,\n",
       "  'classes_com_multiplas_amostras': 153,\n",
       "  'media_amostras_por_classe': np.float64(95.07831325301204),\n",
       "  'balanceamento': np.float64(-1.6502169926387826),\n",
       "  'score_final': 0.7843373493975904},\n",
       " {'usuario': 'usuario_07',\n",
       "  'total_amostras': 8189,\n",
       "  'total_classes': 180,\n",
       "  'classes_com_uma_amostra': 16,\n",
       "  'classes_com_multiplas_amostras': 164,\n",
       "  'media_amostras_por_classe': np.float64(45.49444444444445),\n",
       "  'balanceamento': np.float64(-1.36940539528482),\n",
       "  'score_final': 0.7822222222222222},\n",
       " {'usuario': 'usuario_14',\n",
       "  'total_amostras': 13695,\n",
       "  'total_classes': 135,\n",
       "  'classes_com_uma_amostra': 15,\n",
       "  'classes_com_multiplas_amostras': 120,\n",
       "  'media_amostras_por_classe': np.float64(101.44444444444444),\n",
       "  'balanceamento': np.float64(-2.1164321258856833),\n",
       "  'score_final': 0.7777777777777778},\n",
       " {'usuario': 'usuario_15',\n",
       "  'total_amostras': 24020,\n",
       "  'total_classes': 190,\n",
       "  'classes_com_uma_amostra': 22,\n",
       "  'classes_com_multiplas_amostras': 168,\n",
       "  'media_amostras_por_classe': np.float64(126.42105263157895),\n",
       "  'balanceamento': np.float64(-1.5495214565987285),\n",
       "  'score_final': 0.7768421052631579},\n",
       " {'usuario': 'usuario_00',\n",
       "  'total_amostras': 717,\n",
       "  'total_classes': 69,\n",
       "  'classes_com_uma_amostra': 8,\n",
       "  'classes_com_multiplas_amostras': 61,\n",
       "  'media_amostras_por_classe': np.float64(10.391304347826088),\n",
       "  'balanceamento': np.float64(-0.49479827142276545),\n",
       "  'score_final': 0.7768115942028986},\n",
       " {'usuario': 'usuario_10',\n",
       "  'total_amostras': 20070,\n",
       "  'total_classes': 186,\n",
       "  'classes_com_uma_amostra': 25,\n",
       "  'classes_com_multiplas_amostras': 161,\n",
       "  'media_amostras_por_classe': np.float64(107.90322580645162),\n",
       "  'balanceamento': np.float64(-2.3316247102137),\n",
       "  'score_final': 0.7731182795698924},\n",
       " {'usuario': 'usuario_08',\n",
       "  'total_amostras': 3291,\n",
       "  'total_classes': 21,\n",
       "  'classes_com_uma_amostra': 3,\n",
       "  'classes_com_multiplas_amostras': 18,\n",
       "  'media_amostras_por_classe': np.float64(156.71428571428572),\n",
       "  'balanceamento': np.float64(-0.955532051215696),\n",
       "  'score_final': 0.7714285714285714},\n",
       " {'usuario': 'usuario_09',\n",
       "  'total_amostras': 2497,\n",
       "  'total_classes': 126,\n",
       "  'classes_com_uma_amostra': 24,\n",
       "  'classes_com_multiplas_amostras': 102,\n",
       "  'media_amostras_por_classe': np.float64(19.817460317460316),\n",
       "  'balanceamento': np.float64(-1.6734486604129368),\n",
       "  'score_final': 0.7619047619047619},\n",
       " {'usuario': 'usuario_11',\n",
       "  'total_amostras': 298,\n",
       "  'total_classes': 27,\n",
       "  'classes_com_uma_amostra': 6,\n",
       "  'classes_com_multiplas_amostras': 21,\n",
       "  'media_amostras_por_classe': np.float64(11.037037037037036),\n",
       "  'balanceamento': np.float64(-0.8442865942942461),\n",
       "  'score_final': 0.7555555555555555},\n",
       " {'usuario': 'usuario_01',\n",
       "  'total_amostras': 1601,\n",
       "  'total_classes': 17,\n",
       "  'classes_com_uma_amostra': 2,\n",
       "  'classes_com_multiplas_amostras': 15,\n",
       "  'media_amostras_por_classe': np.float64(94.17647058823529),\n",
       "  'balanceamento': np.float64(-0.6298203759852092),\n",
       "  'score_final': 0.7314705882352941},\n",
       " {'usuario': 'usuario_12',\n",
       "  'total_amostras': 295,\n",
       "  'total_classes': 31,\n",
       "  'classes_com_uma_amostra': 14,\n",
       "  'classes_com_multiplas_amostras': 17,\n",
       "  'media_amostras_por_classe': np.float64(9.516129032258064),\n",
       "  'balanceamento': np.float64(-1.1523775905609455),\n",
       "  'score_final': 0.7096774193548387},\n",
       " {'usuario': 'usuario_13',\n",
       "  'total_amostras': 389,\n",
       "  'total_classes': 5,\n",
       "  'classes_com_uma_amostra': 0,\n",
       "  'classes_com_multiplas_amostras': 5,\n",
       "  'media_amostras_por_classe': np.float64(77.8),\n",
       "  'balanceamento': np.float64(-0.2659004497035635),\n",
       "  'score_final': 0.575}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendar_usuarios(data_path='../dados/processados/Dados_TechChallenge_Fase3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando treino por usuário\n",
    "\n",
    "Realiza o treino para cada usuário e compara qual usuário tem melhores condições de treino.\n",
    "\n",
    "O objetivo era avaliar a viabilidade de treinar um modelo para cada usuário, o que geraria um modelo por usuário. Tal abordagem poderia se mostrar vantajosa, considerando que usuários distintos realizam operações próprias no sistema, de acordo com suas funções.  Porém, alguns testes mostraram que seria melhor gerar um treino único, para todos os usuários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando pipeline de Machine Learning por usuário...\n",
      "============================================================\n",
      "=== OBTENDO USUÁRIOS DO ARQUIVO ===\n",
      "Usuários encontrados: ['usuario_00' 'usuario_01' 'usuario_02' 'usuario_03' 'usuario_04'\n",
      " 'usuario_05' 'usuario_06' 'usuario_07' 'usuario_08' 'usuario_09'\n",
      " 'usuario_10' 'usuario_11' 'usuario_12' 'usuario_13' 'usuario_14'\n",
      " 'usuario_15' 'usuario_16']\n",
      "============================================================\n",
      "\n",
      "🔄 Processando usuário 1/17: usuario_00\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 14:44:57.826992: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.1576 - loss: 4.2266 - val_accuracy: 0.2701 - val_loss: 3.3498 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2460 - loss: 3.4319 - val_accuracy: 0.2779 - val_loss: 3.2430 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2562 - loss: 3.3319 - val_accuracy: 0.2826 - val_loss: 3.2060 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2598 - loss: 3.2666 - val_accuracy: 0.2825 - val_loss: 3.1815 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2596 - loss: 3.2456 - val_accuracy: 0.2821 - val_loss: 3.1779 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2649 - loss: 3.2095 - val_accuracy: 0.2845 - val_loss: 3.1708 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2637 - loss: 3.1878 - val_accuracy: 0.2826 - val_loss: 3.1763 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27845us/step - accuracy: 0.2695 - loss: 3.1659 - val_accuracy: 0.2866 - val_loss: 3.1753 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2658 - loss: 3.1738 - val_accuracy: 0.2852 - val_loss: 3.1746 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2694 - loss: 3.1620 - val_accuracy: 0.2827 - val_loss: 3.1710 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m 803/1732\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 68ms/step - accuracy: 0.2699 - loss: 3.1440\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2690 - loss: 3.1510 - val_accuracy: 0.2859 - val_loss: 3.1768 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2727 - loss: 3.1263 - val_accuracy: 0.2881 - val_loss: 3.1606 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2767 - loss: 3.0891 - val_accuracy: 0.2884 - val_loss: 3.1569 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2772 - loss: 3.0837 - val_accuracy: 0.2906 - val_loss: 3.1591 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2759 - loss: 3.0786 - val_accuracy: 0.2898 - val_loss: 3.1574 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2774 - loss: 3.0859 - val_accuracy: 0.2883 - val_loss: 3.1610 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2776 - loss: 3.0733 - val_accuracy: 0.2901 - val_loss: 3.1658 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1359/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - accuracy: 0.2820 - loss: 3.0488\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2814 - loss: 3.0532 - val_accuracy: 0.2915 - val_loss: 3.1640 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2834 - loss: 3.0474 - val_accuracy: 0.2907 - val_loss: 3.1620 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2857 - loss: 3.0288 - val_accuracy: 0.2912 - val_loss: 3.1605 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2838 - loss: 3.0274 - val_accuracy: 0.2913 - val_loss: 3.1610 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28226us/step - accuracy: 0.2834 - loss: 3.0243 - val_accuracy: 0.2914 - val_loss: 3.1638 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m 463/1732\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 117ms/step - accuracy: 0.2869 - loss: 3.0073\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2846 - loss: 3.0189 - val_accuracy: 0.2919 - val_loss: 3.1644 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2861 - loss: 3.0062 - val_accuracy: 0.2916 - val_loss: 3.1616 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2900 - loss: 3.0038 - val_accuracy: 0.2929 - val_loss: 3.1636 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2889 - loss: 3.0074 - val_accuracy: 0.2922 - val_loss: 3.1617 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2855 - loss: 2.9967 - val_accuracy: 0.2923 - val_loss: 3.1624 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1525/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.2874 - loss: 2.9948\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 2.9957 - val_accuracy: 0.2933 - val_loss: 3.1631 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2834 - loss: 2.9971 - val_accuracy: 0.2929 - val_loss: 3.1632 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2881 - loss: 2.9924 - val_accuracy: 0.2919 - val_loss: 3.1628 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2891 - loss: 2.9940 - val_accuracy: 0.2930 - val_loss: 3.1640 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2897 - loss: 2.9942 - val_accuracy: 0.2929 - val_loss: 3.1638 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m 915/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 60ms/step - accuracy: 0.2834 - loss: 3.0100\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2855 - loss: 3.0023 - val_accuracy: 0.2925 - val_loss: 3.1644 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9825 - val_accuracy: 0.2921 - val_loss: 3.1643 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2875 - loss: 2.9896 - val_accuracy: 0.2926 - val_loss: 3.1647 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2882 - loss: 2.9826 - val_accuracy: 0.2927 - val_loss: 3.1646 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2863 - loss: 2.9894 - val_accuracy: 0.2930 - val_loss: 3.1649 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1333/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.2908 - loss: 2.9928\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2903 - loss: 2.9916 - val_accuracy: 0.2925 - val_loss: 3.1654 - learning_rate: 3.1250e-05\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.66%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.09      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.24      0.10      0.14        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.36      0.17      0.23        24\n",
      "      uc0016       0.16      0.10      0.12      1016\n",
      "      uc0017       0.10      0.02      0.03       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.17      0.01      0.01       812\n",
      "      uc0020       0.20      0.12      0.15         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.81      1.00      0.89        55\n",
      "      uc0024       0.28      0.35      0.31       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.00      0.00      0.00        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.14      0.08      0.10        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.47      0.69      0.56       205\n",
      "      uc0030       0.50      0.59      0.54       252\n",
      "      uc0031       0.40      0.20      0.26       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.29      0.49      0.36        99\n",
      "      uc0040       0.41      0.38      0.40       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.51      0.47      0.49       156\n",
      "      uc0043       0.21      0.71      0.33      5240\n",
      "      uc0044       0.42      0.27      0.33       741\n",
      "      uc0045       0.35      0.10      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.39      0.23      0.29        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.37      0.16      0.22       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.13      0.05      0.07       176\n",
      "      uc0059       0.19      0.15      0.17       116\n",
      "      uc0060       0.45      0.27      0.34       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.32      0.45      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.38      0.60      0.46      1357\n",
      "      uc0076       0.16      0.06      0.08       200\n",
      "      uc0077       0.42      0.22      0.29       681\n",
      "      uc0078       0.33      0.16      0.21        57\n",
      "      uc0079       0.25      0.04      0.07       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.18      0.06      0.09        32\n",
      "      uc0086       0.39      0.35      0.37       196\n",
      "      uc0087       0.23      0.10      0.14       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.23      0.19      0.21        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.24      0.18      0.21        49\n",
      "      uc0094       0.41      0.02      0.03       785\n",
      "      uc0096       0.28      0.44      0.34      2817\n",
      "      uc0097       0.27      0.10      0.14        92\n",
      "      uc0098       0.68      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.28      0.11      0.15        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.36      0.59      0.45       330\n",
      "      uc0108       0.72      0.56      0.63        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.09      0.00      0.01      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.30      0.08      0.12        79\n",
      "      uc0116       0.16      0.16      0.16        31\n",
      "      uc0117       0.41      0.65      0.51        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.35      0.13      0.19       772\n",
      "      uc0125       0.23      0.29      0.25       115\n",
      "      uc0126       0.27      0.03      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.20      0.05      0.09       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.15      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.28      0.20      0.24       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.35      0.39      0.37        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.13      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.52      0.15      0.23        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.17      0.20      0.19       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.44      0.36      0.40       149\n",
      "      uc0158       0.40      0.23      0.29        26\n",
      "      uc0159       0.29      0.13      0.18        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.24      0.19      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.26      0.17      0.21       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.00      0.00      0.00       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.25      0.18      0.21        34\n",
      "      uc0173       1.00      0.06      0.12        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.24      0.10      0.14      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.15      0.06      0.09        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.22      0.04      0.07        46\n",
      "      uc0190       0.11      0.03      0.05        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.25      0.07      0.11        54\n",
      "      uc0195       0.42      0.44      0.43        66\n",
      "      uc0197       0.29      0.11      0.15        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.50      0.19      0.28       409\n",
      "      uc0211       0.38      0.07      0.11       298\n",
      "      uc0212       0.38      0.24      0.30       274\n",
      "      uc0215       0.33      0.07      0.12       647\n",
      "      uc0216       0.20      0.36      0.26       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.39      0.69      0.50       120\n",
      "      uc0220       0.35      0.52      0.42        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.33      0.38      0.35      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.45      0.21      0.29        24\n",
      "      uc0226       0.45      0.75      0.56       235\n",
      "      uc0228       0.11      0.08      0.09        25\n",
      "      uc0229       0.41      0.31      0.35        94\n",
      "      uc0230       0.24      0.12      0.16        78\n",
      "      uc0232       0.48      0.48      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.08      0.00      0.00       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.30      0.17      0.22       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.12      0.02      0.03       120\n",
      "      uc1007       0.00      0.00      0.00        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.00      0.00      0.00        50\n",
      "      uc1010       0.37      0.56      0.45        70\n",
      "      uc1011       0.40      0.24      0.30        41\n",
      "      uc1012       0.25      0.26      0.25        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.24      0.27      0.25        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.52      0.30      0.38       375\n",
      "      uc1018       0.50      0.10      0.17        10\n",
      "      uc1019       0.12      0.12      0.12         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.60      0.38      0.46        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.52      0.31      0.39        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.32      0.44      0.37        16\n",
      "      uc2012       0.43      0.54      0.48        24\n",
      "      uc2014       0.39      0.17      0.24        40\n",
      "      uc2015       0.17      0.17      0.17         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.28      0.23      0.25        80\n",
      "      uc2020       0.70      0.18      0.29       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.21      0.11      0.14       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.52      0.43      0.47        35\n",
      "      uc2028       0.76      0.84      0.79        37\n",
      "      uc2029       0.26      0.46      0.33       341\n",
      "      uc2030       0.64      0.20      0.30        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.21      0.02      0.04       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.55      0.13      0.21        45\n",
      "      uc2037       0.50      0.03      0.06        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.16      0.05      0.07        61\n",
      "      uc2040       0.50      0.04      0.08        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.27      0.32      0.29        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.27      0.28      0.28        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.41      0.45      0.43        20\n",
      "      uc2049       0.75      0.42      0.54        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.25      0.10      0.14        20\n",
      "      uc2056       0.50      0.29      0.36        42\n",
      "      uc2059       0.24      0.60      0.34       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.41      0.32      0.36        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.25      0.05      0.08        61\n",
      "      uc2065       0.52      0.45      0.48        66\n",
      "      uc2066       0.18      0.24      0.21       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.11      0.14      0.12        22\n",
      "      uc2072       0.50      0.11      0.17        19\n",
      "      uc2073       0.62      0.40      0.49        25\n",
      "      uc2074       0.08      0.12      0.10         8\n",
      "      uc2075       0.33      0.08      0.13        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.55      0.47      0.51        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.67      0.43      0.52        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.25      0.38      0.30        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       0.89      0.20      0.33       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.39      0.63      0.48        19\n",
      "      uc2092       0.14      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.17      0.10      0.13        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_00 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 2/17: usuario_01\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m -27321us/step - accuracy: 0.1516 - loss: 4.2242 - val_accuracy: 0.2705 - val_loss: 3.3658 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.2474 - loss: 3.4370 - val_accuracy: 0.2753 - val_loss: 3.2392 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2553 - loss: 3.3227 - val_accuracy: 0.2834 - val_loss: 3.1906 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2638 - loss: 3.2516 - val_accuracy: 0.2847 - val_loss: 3.1732 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2599 - loss: 3.2377 - val_accuracy: 0.2877 - val_loss: 3.1651 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2659 - loss: 3.2006 - val_accuracy: 0.2808 - val_loss: 3.1750 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2676 - loss: 3.1838 - val_accuracy: 0.2845 - val_loss: 3.1743 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2646 - loss: 3.1841 - val_accuracy: 0.2863 - val_loss: 3.1714 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2685 - loss: 3.1661 - val_accuracy: 0.2860 - val_loss: 3.1645 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2693 - loss: 3.1569 - val_accuracy: 0.2846 - val_loss: 3.1687 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2695 - loss: 3.1480 - val_accuracy: 0.2844 - val_loss: 3.1760 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2673 - loss: 3.1618 - val_accuracy: 0.2873 - val_loss: 3.1791 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 34ms/step - accuracy: 0.2692 - loss: 3.1342 - val_accuracy: 0.2882 - val_loss: 3.1831 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m  51/1732\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2558 - loss: 3.1464\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28096us/step - accuracy: 0.2673 - loss: 3.1393 - val_accuracy: 0.2857 - val_loss: 3.1762 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2746 - loss: 3.0851 - val_accuracy: 0.2896 - val_loss: 3.1614 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2764 - loss: 3.0802 - val_accuracy: 0.2892 - val_loss: 3.1646 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2763 - loss: 3.0707 - val_accuracy: 0.2891 - val_loss: 3.1648 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2755 - loss: 3.0602 - val_accuracy: 0.2907 - val_loss: 3.1713 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2802 - loss: 3.0521 - val_accuracy: 0.2901 - val_loss: 3.1694 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1723/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2815 - loss: 3.0277\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2815 - loss: 3.0279 - val_accuracy: 0.2934 - val_loss: 3.1728 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2811 - loss: 3.0306 - val_accuracy: 0.2905 - val_loss: 3.1665 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2785 - loss: 3.0383 - val_accuracy: 0.2910 - val_loss: 3.1696 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2840 - loss: 3.0150 - val_accuracy: 0.2914 - val_loss: 3.1673 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2836 - loss: 3.0110 - val_accuracy: 0.2929 - val_loss: 3.1713 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m 284/1732\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:33\u001b[0m 189ms/step - accuracy: 0.2817 - loss: 2.9905\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2815 - loss: 3.0096 - val_accuracy: 0.2925 - val_loss: 3.1691 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2820 - loss: 3.0091 - val_accuracy: 0.2940 - val_loss: 3.1677 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2853 - loss: 3.0057 - val_accuracy: 0.2937 - val_loss: 3.1682 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2868 - loss: 2.9882 - val_accuracy: 0.2942 - val_loss: 3.1699 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2850 - loss: 3.0016 - val_accuracy: 0.2941 - val_loss: 3.1717 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1017/1732\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.2788 - loss: 3.0182\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2805 - loss: 3.0125 - val_accuracy: 0.2938 - val_loss: 3.1722 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2867 - loss: 2.9863 - val_accuracy: 0.2930 - val_loss: 3.1712 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2848 - loss: 2.9935 - val_accuracy: 0.2938 - val_loss: 3.1724 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2888 - loss: 2.9840 - val_accuracy: 0.2934 - val_loss: 3.1726 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28139us/step - accuracy: 0.2906 - loss: 2.9836 - val_accuracy: 0.2934 - val_loss: 3.1726 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m 514/1732\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 105ms/step - accuracy: 0.2917 - loss: 2.9615\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2902 - loss: 2.9757 - val_accuracy: 0.2935 - val_loss: 3.1722 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2895 - loss: 2.9737 - val_accuracy: 0.2944 - val_loss: 3.1712 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2858 - loss: 2.9814 - val_accuracy: 0.2938 - val_loss: 3.1717 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2863 - loss: 2.9909 - val_accuracy: 0.2943 - val_loss: 3.1716 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2856 - loss: 2.9861 - val_accuracy: 0.2938 - val_loss: 3.1724 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1726/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2899 - loss: 2.9628\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2899 - loss: 2.9629 - val_accuracy: 0.2934 - val_loss: 3.1727 - learning_rate: 3.1250e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28104us/step - accuracy: 0.2893 - loss: 2.9748 - val_accuracy: 0.2935 - val_loss: 3.1728 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2878 - loss: 2.9731 - val_accuracy: 0.2931 - val_loss: 3.1724 - learning_rate: 1.5625e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2898 - loss: 2.9810 - val_accuracy: 0.2935 - val_loss: 3.1729 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2877 - loss: 2.9775 - val_accuracy: 0.2932 - val_loss: 3.1735 - learning_rate: 1.5625e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1551/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.2867 - loss: 2.9807\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2869 - loss: 2.9807 - val_accuracy: 0.2931 - val_loss: 3.1732 - learning_rate: 1.5625e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2886 - loss: 2.9923 - val_accuracy: 0.2932 - val_loss: 3.1730 - learning_rate: 7.8125e-06\n",
      "Epoch 46: early stopping\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.57%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.10      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.20      0.10      0.13        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.44      0.17      0.24        24\n",
      "      uc0016       0.16      0.08      0.10      1016\n",
      "      uc0017       0.11      0.03      0.04       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.11      0.00      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.85      1.00      0.92        55\n",
      "      uc0024       0.32      0.33      0.32       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.07      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.32      0.23      0.27        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.47      0.67      0.55       205\n",
      "      uc0030       0.50      0.59      0.54       252\n",
      "      uc0031       0.36      0.19      0.25       574\n",
      "      uc0032       0.29      0.15      0.20        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.54      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       1.00      0.20      0.33         5\n",
      "      uc0039       0.30      0.38      0.33        99\n",
      "      uc0040       0.40      0.37      0.38       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.47      0.48      0.47       156\n",
      "      uc0043       0.21      0.72      0.33      5240\n",
      "      uc0044       0.38      0.29      0.33       741\n",
      "      uc0045       0.32      0.06      0.10       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.20      0.03      0.05       164\n",
      "      uc0049       0.47      0.23      0.30        31\n",
      "      uc0050       1.00      0.12      0.22         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.36      0.18      0.24       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.09      0.02      0.04       176\n",
      "      uc0059       0.23      0.16      0.19       116\n",
      "      uc0060       0.48      0.24      0.32       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.32      0.46      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.61      0.46      1357\n",
      "      uc0076       0.18      0.01      0.03       200\n",
      "      uc0077       0.46      0.21      0.29       681\n",
      "      uc0078       0.30      0.11      0.16        57\n",
      "      uc0079       0.20      0.01      0.01       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.22      0.06      0.10        32\n",
      "      uc0086       0.32      0.31      0.32       196\n",
      "      uc0087       0.27      0.06      0.10       334\n",
      "      uc0089       1.00      0.05      0.09        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.39      0.27      0.32        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.30      0.18      0.23        49\n",
      "      uc0094       0.27      0.01      0.01       785\n",
      "      uc0096       0.27      0.45      0.34      2817\n",
      "      uc0097       0.26      0.11      0.15        92\n",
      "      uc0098       0.67      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.60      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.39      0.61      0.47       330\n",
      "      uc0108       0.69      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.11      0.01      0.02      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.07      0.00      0.01       284\n",
      "      uc0115       0.21      0.10      0.14        79\n",
      "      uc0116       0.15      0.13      0.14        31\n",
      "      uc0117       0.38      0.35      0.36        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.40      0.12      0.18       772\n",
      "      uc0125       0.29      0.30      0.30       115\n",
      "      uc0126       0.28      0.04      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.50      0.04      0.07        25\n",
      "      uc0130       0.48      0.14      0.21        88\n",
      "      uc0131       0.23      0.01      0.01       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.11      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.29      0.23      0.25       431\n",
      "      uc0140       0.09      0.08      0.09        36\n",
      "      uc0141       0.33      0.39      0.36        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.13      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.38      0.07      0.12        80\n",
      "      uc0150       0.12      0.01      0.02       541\n",
      "      uc0153       0.14      0.17      0.15       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.47      0.33      0.39       149\n",
      "      uc0158       0.42      0.19      0.26        26\n",
      "      uc0159       1.00      0.13      0.24        15\n",
      "      uc0161       0.33      0.14      0.20         7\n",
      "      uc0162       0.24      0.18      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.27      0.14      0.18       136\n",
      "      uc0167       0.12      0.04      0.06        25\n",
      "      uc0169       0.17      0.01      0.03       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.29      0.18      0.22        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.21      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.22      0.03      0.06        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.47      0.42      0.44        66\n",
      "      uc0197       0.32      0.24      0.27        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.49      0.19      0.28       409\n",
      "      uc0211       0.22      0.07      0.11       298\n",
      "      uc0212       0.38      0.26      0.31       274\n",
      "      uc0215       0.37      0.07      0.12       647\n",
      "      uc0216       0.25      0.34      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.40      0.69      0.50       120\n",
      "      uc0220       0.38      0.54      0.45        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.31      0.38      0.34      1234\n",
      "      uc0223       0.00      0.00      0.00        20\n",
      "      uc0225       0.41      0.29      0.34        24\n",
      "      uc0226       0.45      0.74      0.56       235\n",
      "      uc0228       0.00      0.00      0.00        25\n",
      "      uc0229       0.39      0.33      0.36        94\n",
      "      uc0230       0.23      0.15      0.18        78\n",
      "      uc0232       0.48      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.12      0.01      0.02       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.33      0.04      0.06        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.33      0.06      0.10        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.28      0.14      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.18      0.12      0.14        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.20      0.19        50\n",
      "      uc1010       0.36      0.51      0.42        70\n",
      "      uc1011       0.45      0.24      0.32        41\n",
      "      uc1012       0.00      0.00      0.00        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.23      0.23      0.23        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.47      0.32      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.59      0.35      0.44        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.40      0.45      0.42        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.47      0.44      0.45        16\n",
      "      uc2012       0.62      0.42      0.50        24\n",
      "      uc2014       0.22      0.05      0.08        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.32      0.14      0.19        80\n",
      "      uc2020       0.44      0.22      0.29       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.15      0.08      0.11       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.61      0.40      0.48        35\n",
      "      uc2028       0.76      0.84      0.79        37\n",
      "      uc2029       0.30      0.34      0.32       341\n",
      "      uc2030       0.69      0.20      0.31        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.67      0.13      0.22        45\n",
      "      uc2037       0.25      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.20      0.07      0.10        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.43      0.27      0.33        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.35      0.30      0.32        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.33      0.40      0.36        20\n",
      "      uc2049       0.71      0.42      0.53        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.54      0.36      0.43        42\n",
      "      uc2059       0.22      0.66      0.33       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.41      0.30      0.34        37\n",
      "      uc2063       0.92      0.60      0.73        20\n",
      "      uc2064       0.26      0.10      0.14        61\n",
      "      uc2065       0.33      0.42      0.37        66\n",
      "      uc2066       0.26      0.26      0.26       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.38      0.23      0.29        22\n",
      "      uc2072       0.50      0.11      0.17        19\n",
      "      uc2073       0.62      0.40      0.49        25\n",
      "      uc2074       0.12      0.12      0.12         8\n",
      "      uc2075       0.50      0.08      0.14        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.37      0.45      0.40        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.62      0.36      0.45        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.22      0.27      0.24        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.18      0.31       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.28      0.26      0.27        19\n",
      "      uc2092       0.00      0.00      0.00       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.08      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.33      0.20      0.25         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.20      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_01 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 3/17: usuario_02\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1569 - loss: 4.2129 - val_accuracy: 0.2674 - val_loss: 3.3511 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2483 - loss: 3.4223 - val_accuracy: 0.2793 - val_loss: 3.2397 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2577 - loss: 3.3132 - val_accuracy: 0.2789 - val_loss: 3.1967 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2585 - loss: 3.2619 - val_accuracy: 0.2837 - val_loss: 3.1797 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2664 - loss: 3.2289 - val_accuracy: 0.2831 - val_loss: 3.1726 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2615 - loss: 3.2203 - val_accuracy: 0.2869 - val_loss: 3.1684 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m -28077us/step - accuracy: 0.2697 - loss: 3.1779 - val_accuracy: 0.2808 - val_loss: 3.1620 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2673 - loss: 3.1792 - val_accuracy: 0.2858 - val_loss: 3.1716 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2664 - loss: 3.1734 - val_accuracy: 0.2811 - val_loss: 3.1744 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2676 - loss: 3.1523 - val_accuracy: 0.2855 - val_loss: 3.1679 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2697 - loss: 3.1488 - val_accuracy: 0.2855 - val_loss: 3.1633 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m 944/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 58ms/step - accuracy: 0.2742 - loss: 3.1420\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2720 - loss: 3.1475 - val_accuracy: 0.2891 - val_loss: 3.1730 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2744 - loss: 3.0859 - val_accuracy: 0.2877 - val_loss: 3.1556 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2773 - loss: 3.0808 - val_accuracy: 0.2901 - val_loss: 3.1618 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2697 - loss: 3.0958 - val_accuracy: 0.2897 - val_loss: 3.1564 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2796 - loss: 3.0696 - val_accuracy: 0.2904 - val_loss: 3.1595 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2754 - loss: 3.0668 - val_accuracy: 0.2914 - val_loss: 3.1643 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1681/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2808 - loss: 3.0409\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2808 - loss: 3.0415 - val_accuracy: 0.2903 - val_loss: 3.1599 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2798 - loss: 3.0397 - val_accuracy: 0.2917 - val_loss: 3.1570 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -28011us/step - accuracy: 0.2808 - loss: 3.0324 - val_accuracy: 0.2917 - val_loss: 3.1562 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2801 - loss: 3.0464 - val_accuracy: 0.2919 - val_loss: 3.1542 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2860 - loss: 3.0148 - val_accuracy: 0.2928 - val_loss: 3.1571 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2849 - loss: 3.0123 - val_accuracy: 0.2925 - val_loss: 3.1559 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2888 - loss: 2.9954 - val_accuracy: 0.2926 - val_loss: 3.1568 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2832 - loss: 3.0289 - val_accuracy: 0.2923 - val_loss: 3.1557 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1255/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.2846 - loss: 3.0119\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2848 - loss: 3.0130 - val_accuracy: 0.2909 - val_loss: 3.1573 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2909 - loss: 2.9774 - val_accuracy: 0.2924 - val_loss: 3.1569 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2817 - loss: 3.0067 - val_accuracy: 0.2918 - val_loss: 3.1550 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2875 - loss: 2.9988 - val_accuracy: 0.2927 - val_loss: 3.1560 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2892 - loss: 2.9885 - val_accuracy: 0.2934 - val_loss: 3.1580 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1729/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2860 - loss: 2.9980\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2860 - loss: 2.9980 - val_accuracy: 0.2920 - val_loss: 3.1591 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2914 - loss: 2.9752 - val_accuracy: 0.2918 - val_loss: 3.1587 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2834 - loss: 2.9900 - val_accuracy: 0.2921 - val_loss: 3.1576 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2892 - loss: 2.9853 - val_accuracy: 0.2930 - val_loss: 3.1590 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2875 - loss: 2.9804 - val_accuracy: 0.2921 - val_loss: 3.1601 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m 843/1732\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 65ms/step - accuracy: 0.2868 - loss: 2.9850 \n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2877 - loss: 2.9840 - val_accuracy: 0.2927 - val_loss: 3.1598 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2888 - loss: 2.9808 - val_accuracy: 0.2927 - val_loss: 3.1590 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2887 - loss: 2.9746 - val_accuracy: 0.2928 - val_loss: 3.1598 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2952 - loss: 2.9625 - val_accuracy: 0.2930 - val_loss: 3.1590 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28155us/step - accuracy: 0.2937 - loss: 2.9610 - val_accuracy: 0.2924 - val_loss: 3.1601 - learning_rate: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.81%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.12      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.22      0.12      0.15        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.38      0.12      0.19        24\n",
      "      uc0016       0.16      0.08      0.10      1016\n",
      "      uc0017       0.12      0.02      0.03       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.00      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.82      1.00      0.90        55\n",
      "      uc0024       0.32      0.32      0.32       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.09      0.07      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.33      0.23      0.27        26\n",
      "      uc0028       0.33      0.04      0.07       109\n",
      "      uc0029       0.49      0.69      0.57       205\n",
      "      uc0030       0.48      0.59      0.53       252\n",
      "      uc0031       0.37      0.21      0.27       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.58      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       1.00      0.20      0.33         5\n",
      "      uc0039       0.31      0.40      0.35        99\n",
      "      uc0040       0.40      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.45      0.47      0.46       156\n",
      "      uc0043       0.22      0.70      0.33      5240\n",
      "      uc0044       0.36      0.30      0.33       741\n",
      "      uc0045       0.38      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.11      0.01      0.02       164\n",
      "      uc0049       0.44      0.23      0.30        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.38      0.15      0.22       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.16      0.05      0.08       176\n",
      "      uc0059       0.22      0.25      0.23       116\n",
      "      uc0060       0.45      0.27      0.34       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.47      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.61      0.46      1357\n",
      "      uc0076       0.17      0.01      0.03       200\n",
      "      uc0077       0.44      0.24      0.31       681\n",
      "      uc0078       0.45      0.09      0.15        57\n",
      "      uc0079       0.36      0.03      0.06       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.29      0.06      0.10        32\n",
      "      uc0086       0.35      0.35      0.35       196\n",
      "      uc0087       0.22      0.08      0.12       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.26      0.30      0.28        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.28      0.10      0.15        49\n",
      "      uc0094       0.28      0.01      0.03       785\n",
      "      uc0096       0.27      0.45      0.34      2817\n",
      "      uc0097       0.28      0.12      0.17        92\n",
      "      uc0098       0.66      0.49      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.23      0.14      0.17        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.39      0.61      0.47       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.12      0.01      0.02      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.35      0.08      0.12        79\n",
      "      uc0116       0.15      0.13      0.14        31\n",
      "      uc0117       0.40      0.65      0.50        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.34      0.11      0.17       772\n",
      "      uc0125       0.28      0.31      0.29       115\n",
      "      uc0126       0.21      0.04      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       1.00      0.04      0.08        25\n",
      "      uc0130       0.48      0.14      0.21        88\n",
      "      uc0131       0.19      0.01      0.03       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.42      0.11      0.18       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.12      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.29      0.20      0.23       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.24      0.44      0.31        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.13      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.44      0.09      0.15        80\n",
      "      uc0150       0.16      0.01      0.02       541\n",
      "      uc0153       0.18      0.21      0.20       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.38      0.41       149\n",
      "      uc0158       0.32      0.23      0.27        26\n",
      "      uc0159       1.00      0.13      0.24        15\n",
      "      uc0161       0.14      0.14      0.14         7\n",
      "      uc0162       0.24      0.19      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.27      0.14      0.18       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.11      0.01      0.03       142\n",
      "      uc0171       0.10      0.03      0.04        35\n",
      "      uc0172       0.23      0.21      0.22        34\n",
      "      uc0173       1.00      0.06      0.12        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.26      0.10      0.14      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.29      0.04      0.07        54\n",
      "      uc0195       0.47      0.42      0.44        66\n",
      "      uc0197       0.35      0.16      0.22        38\n",
      "      uc0198       0.50      0.03      0.05       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.47      0.22      0.30       409\n",
      "      uc0211       0.28      0.04      0.08       298\n",
      "      uc0212       0.39      0.24      0.30       274\n",
      "      uc0215       0.35      0.07      0.11       647\n",
      "      uc0216       0.25      0.34      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.42      0.72      0.53       120\n",
      "      uc0220       0.41      0.51      0.45        89\n",
      "      uc0221       0.42      0.19      0.26        90\n",
      "      uc0222       0.31      0.38      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.36      0.42      0.38        24\n",
      "      uc0226       0.46      0.74      0.56       235\n",
      "      uc0228       0.07      0.04      0.05        25\n",
      "      uc0229       0.38      0.33      0.35        94\n",
      "      uc0230       0.21      0.12      0.15        78\n",
      "      uc0232       0.46      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.16      0.02      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.50      0.12      0.19        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.28      0.15      0.20       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.08      0.02      0.03       120\n",
      "      uc1007       0.17      0.12      0.14        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.15      0.18      0.16        50\n",
      "      uc1010       0.39      0.50      0.44        70\n",
      "      uc1011       0.42      0.24      0.31        41\n",
      "      uc1012       0.38      0.19      0.26        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.20      0.12      0.15        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.44      0.33      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.11      0.12      0.12         8\n",
      "      uc2001       0.29      0.17      0.21        24\n",
      "      uc2002       0.71      0.35      0.47        48\n",
      "      uc2005       0.81      0.72      0.76        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.33      0.35      0.34        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.44      0.44      0.44        16\n",
      "      uc2012       0.50      0.46      0.48        24\n",
      "      uc2014       0.43      0.15      0.22        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.24      0.25      0.24        80\n",
      "      uc2020       0.48      0.20      0.28       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.12      0.08      0.10       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.61      0.40      0.48        35\n",
      "      uc2028       0.81      0.81      0.81        37\n",
      "      uc2029       0.30      0.43      0.35       341\n",
      "      uc2030       0.69      0.20      0.31        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.46      0.13      0.21        45\n",
      "      uc2037       0.00      0.00      0.00        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.20      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.38      0.27      0.32        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.42      0.30      0.35        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.39      0.45      0.42        20\n",
      "      uc2049       0.76      0.44      0.56        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.42      0.40      0.41        20\n",
      "      uc2056       0.54      0.31      0.39        42\n",
      "      uc2059       0.23      0.59      0.33       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.30      0.35      0.32        37\n",
      "      uc2063       0.92      0.55      0.69        20\n",
      "      uc2064       0.21      0.11      0.15        61\n",
      "      uc2065       0.38      0.41      0.39        66\n",
      "      uc2066       0.28      0.18      0.22       104\n",
      "      uc2067       0.44      0.13      0.20        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.19      0.23      0.21        22\n",
      "      uc2072       0.43      0.16      0.23        19\n",
      "      uc2073       0.64      0.36      0.46        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       1.00      0.08      0.15        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.56      0.43      0.49        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.55      0.43      0.48        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.26      0.27      0.26        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       0.96      0.20      0.33       120\n",
      "      uc2088       1.00      0.50      0.67         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.00      0.00      0.00       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.25      0.14      0.18        29\n",
      "      uc2096       0.16      0.13      0.15        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       1.00      0.20      0.33         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.21      0.14      0.15     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_02 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 4/17: usuario_03\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1553 - loss: 4.2038 - val_accuracy: 0.2671 - val_loss: 3.3433 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2447 - loss: 3.4319 - val_accuracy: 0.2777 - val_loss: 3.2372 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2553 - loss: 3.3232 - val_accuracy: 0.2811 - val_loss: 3.2054 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2597 - loss: 3.2568 - val_accuracy: 0.2843 - val_loss: 3.1806 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2648 - loss: 3.2301 - val_accuracy: 0.2849 - val_loss: 3.1793 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2662 - loss: 3.2120 - val_accuracy: 0.2844 - val_loss: 3.1777 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2685 - loss: 3.1895 - val_accuracy: 0.2868 - val_loss: 3.1781 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2657 - loss: 3.1902 - val_accuracy: 0.2858 - val_loss: 3.1644 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2675 - loss: 3.1762 - val_accuracy: 0.2883 - val_loss: 3.1695 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2681 - loss: 3.1622 - val_accuracy: 0.2879 - val_loss: 3.1673 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2690 - loss: 3.1529 - val_accuracy: 0.2854 - val_loss: 3.1682 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28280us/step - accuracy: 0.2730 - loss: 3.1444 - val_accuracy: 0.2880 - val_loss: 3.1686 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1724/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2655 - loss: 3.1603\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2655 - loss: 3.1603 - val_accuracy: 0.2920 - val_loss: 3.1752 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2763 - loss: 3.1009 - val_accuracy: 0.2913 - val_loss: 3.1578 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28036us/step - accuracy: 0.2755 - loss: 3.0997 - val_accuracy: 0.2906 - val_loss: 3.1589 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2796 - loss: 3.0842 - val_accuracy: 0.2911 - val_loss: 3.1635 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2792 - loss: 3.0734 - val_accuracy: 0.2923 - val_loss: 3.1630 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2774 - loss: 3.0686 - val_accuracy: 0.2924 - val_loss: 3.1627 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1550/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2785 - loss: 3.0474\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28249us/step - accuracy: 0.2782 - loss: 3.0504 - val_accuracy: 0.2946 - val_loss: 3.1605 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2816 - loss: 3.0395 - val_accuracy: 0.2909 - val_loss: 3.1605 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2810 - loss: 3.0425 - val_accuracy: 0.2920 - val_loss: 3.1587 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m 3ms/step - accuracy: 0.2805 - loss: 3.0316 - val_accuracy: 0.2937 - val_loss: 3.1597 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2858 - loss: 3.0204 - val_accuracy: 0.2917 - val_loss: 3.1596 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m 745/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 73ms/step - accuracy: 0.2838 - loss: 3.0253\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2840 - loss: 3.0237 - val_accuracy: 0.2926 - val_loss: 3.1587 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2881 - loss: 2.9939 - val_accuracy: 0.2930 - val_loss: 3.1606 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28167us/step - accuracy: 0.2834 - loss: 3.0109 - val_accuracy: 0.2934 - val_loss: 3.1618 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 3.0037 - val_accuracy: 0.2930 - val_loss: 3.1646 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2859 - loss: 2.9989 - val_accuracy: 0.2930 - val_loss: 3.1606 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1726/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2831 - loss: 3.0117\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2831 - loss: 3.0116 - val_accuracy: 0.2941 - val_loss: 3.1616 - learning_rate: 1.2500e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step    \n",
      "Acurácia: 28.42%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.20      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.50      0.08      0.14        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.33      0.17      0.22        24\n",
      "      uc0016       0.15      0.08      0.10      1016\n",
      "      uc0017       0.17      0.02      0.03       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.13      0.01      0.02       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.76      1.00      0.87        55\n",
      "      uc0024       0.32      0.32      0.32       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.18      0.03      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.28      0.27      0.27        26\n",
      "      uc0028       0.60      0.03      0.05       109\n",
      "      uc0029       0.46      0.70      0.56       205\n",
      "      uc0030       0.47      0.58      0.52       252\n",
      "      uc0031       0.33      0.20      0.25       574\n",
      "      uc0032       0.35      0.15      0.21        46\n",
      "      uc0033       0.20      0.08      0.11        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.25      0.51      0.33        99\n",
      "      uc0040       0.38      0.40      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.53      0.46      0.49       156\n",
      "      uc0043       0.21      0.71      0.33      5240\n",
      "      uc0044       0.39      0.29      0.33       741\n",
      "      uc0045       0.32      0.06      0.10       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.42      0.32      0.36        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.43      0.13      0.20       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.11      0.01      0.02       176\n",
      "      uc0059       0.22      0.16      0.19       116\n",
      "      uc0060       0.64      0.20      0.31       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.46      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.39      0.58      0.47      1357\n",
      "      uc0076       0.08      0.01      0.01       200\n",
      "      uc0077       0.47      0.21      0.29       681\n",
      "      uc0078       0.28      0.09      0.13        57\n",
      "      uc0079       0.67      0.02      0.03       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.20      0.01      0.02        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.32      0.34      0.33       196\n",
      "      uc0087       0.25      0.06      0.10       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.17      0.01      0.03        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.24      0.18      0.21        49\n",
      "      uc0094       0.31      0.01      0.02       785\n",
      "      uc0096       0.26      0.47      0.33      2817\n",
      "      uc0097       0.31      0.11      0.16        92\n",
      "      uc0098       0.66      0.49      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.33      0.02      0.03        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.40      0.59      0.47       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       1.00      0.02      0.03        58\n",
      "      uc0110       0.12      0.02      0.03        63\n",
      "      uc0111       0.12      0.02      0.04      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.24      0.06      0.10        79\n",
      "      uc0116       0.12      0.10      0.11        31\n",
      "      uc0117       0.50      0.54      0.52        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.36      0.13      0.19       772\n",
      "      uc0125       0.35      0.25      0.29       115\n",
      "      uc0126       0.25      0.00      0.01       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.21      0.04      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.08      0.04      0.05        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.29      0.19      0.23       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.25      0.39      0.30        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.13      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.43      0.07      0.13        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.22      0.20      0.21       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.46      0.32      0.38       149\n",
      "      uc0158       0.55      0.23      0.32        26\n",
      "      uc0159       0.60      0.20      0.30        15\n",
      "      uc0161       0.20      0.14      0.17         7\n",
      "      uc0162       0.25      0.18      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.24      0.13      0.17       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.09      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.50      0.03      0.06        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.21      0.10      0.13      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.16      0.11      0.13        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.46      0.42      0.44        66\n",
      "      uc0197       0.00      0.00      0.00        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.48      0.21      0.29       409\n",
      "      uc0211       0.21      0.06      0.09       298\n",
      "      uc0212       0.37      0.27      0.32       274\n",
      "      uc0215       0.35      0.07      0.11       647\n",
      "      uc0216       0.24      0.35      0.28       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.42      0.64      0.51       120\n",
      "      uc0220       0.43      0.56      0.49        89\n",
      "      uc0221       0.44      0.19      0.26        90\n",
      "      uc0222       0.31      0.38      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.35      0.58      0.44        24\n",
      "      uc0226       0.45      0.75      0.56       235\n",
      "      uc0228       0.08      0.04      0.05        25\n",
      "      uc0229       0.36      0.39      0.38        94\n",
      "      uc0230       0.25      0.13      0.17        78\n",
      "      uc0232       0.46      0.49      0.47      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.00      0.00      0.00       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.61      0.56       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.27      0.06      0.10       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.17      0.01      0.02       120\n",
      "      uc1007       0.29      0.24      0.26        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.00      0.00      0.00        50\n",
      "      uc1010       0.44      0.53      0.48        70\n",
      "      uc1011       0.43      0.22      0.29        41\n",
      "      uc1012       0.25      0.06      0.10        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.18      0.23      0.20        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.42      0.32      0.36       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.25      0.12      0.17        24\n",
      "      uc2002       0.74      0.35      0.48        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.65      0.20      0.31        54\n",
      "      uc2007       1.00      0.06      0.11        17\n",
      "      uc2008       0.38      0.35      0.36        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.39      0.44      0.41        16\n",
      "      uc2012       0.56      0.42      0.48        24\n",
      "      uc2014       0.30      0.17      0.22        40\n",
      "      uc2015       0.14      0.17      0.15         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.41      0.16      0.23        80\n",
      "      uc2020       0.67      0.18      0.28       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.23      0.06      0.09       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.20      0.29        35\n",
      "      uc2028       0.75      0.81      0.78        37\n",
      "      uc2029       0.28      0.48      0.35       341\n",
      "      uc2030       0.60      0.13      0.21        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.04      0.01      0.01       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.43      0.13      0.20        45\n",
      "      uc2037       0.50      0.03      0.06        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.27      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.83      0.23      0.36        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.29      0.30      0.29        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.30      0.45      0.36        20\n",
      "      uc2049       0.49      0.47      0.48        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.40      0.40      0.40        20\n",
      "      uc2056       0.61      0.26      0.37        42\n",
      "      uc2059       0.22      0.61      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.55      0.32      0.41        37\n",
      "      uc2063       0.92      0.55      0.69        20\n",
      "      uc2064       0.00      0.00      0.00        61\n",
      "      uc2065       0.50      0.44      0.47        66\n",
      "      uc2066       0.19      0.25      0.21       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.50      0.05      0.08        22\n",
      "      uc2072       0.00      0.00      0.00        19\n",
      "      uc2073       1.00      0.36      0.53        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.50      0.08      0.14        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.11      0.05      0.07        20\n",
      "      uc2078       0.51      0.39      0.44        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.71      0.36      0.48        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.18      0.42      0.25        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       0.88      0.18      0.30       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.31      0.26      0.29        19\n",
      "      uc2092       0.00      0.00      0.00       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.20      0.10      0.14        29\n",
      "      uc2096       0.13      0.07      0.09        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.28     46182\n",
      "   macro avg       0.19      0.12      0.13     46182\n",
      "weighted avg       0.27      0.28      0.23     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_03 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 5/17: usuario_04\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1555 - loss: 4.2047 - val_accuracy: 0.2698 - val_loss: 3.3264 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2477 - loss: 3.4179 - val_accuracy: 0.2790 - val_loss: 3.2195 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2588 - loss: 3.3029 - val_accuracy: 0.2810 - val_loss: 3.1911 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2605 - loss: 3.2646 - val_accuracy: 0.2833 - val_loss: 3.1728 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2679 - loss: 3.2202 - val_accuracy: 0.2834 - val_loss: 3.1618 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2660 - loss: 3.1953 - val_accuracy: 0.2843 - val_loss: 3.1623 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2698 - loss: 3.1962 - val_accuracy: 0.2860 - val_loss: 3.1631 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2629 - loss: 3.1830 - val_accuracy: 0.2841 - val_loss: 3.1599 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2725 - loss: 3.1490 - val_accuracy: 0.2845 - val_loss: 3.1628 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2707 - loss: 3.1467 - val_accuracy: 0.2863 - val_loss: 3.1639 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2718 - loss: 3.1322 - val_accuracy: 0.2883 - val_loss: 3.1674 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2711 - loss: 3.1380 - val_accuracy: 0.2865 - val_loss: 3.1668 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m 946/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 58ms/step - accuracy: 0.2754 - loss: 3.1206\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2723 - loss: 3.1298 - val_accuracy: 0.2847 - val_loss: 3.1731 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2710 - loss: 3.1112 - val_accuracy: 0.2896 - val_loss: 3.1595 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2784 - loss: 3.0651 - val_accuracy: 0.2882 - val_loss: 3.1633 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2769 - loss: 3.0723 - val_accuracy: 0.2898 - val_loss: 3.1619 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2769 - loss: 3.0608 - val_accuracy: 0.2922 - val_loss: 3.1602 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2767 - loss: 3.0541 - val_accuracy: 0.2925 - val_loss: 3.1664 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m 285/1732\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:31\u001b[0m 188ms/step - accuracy: 0.2843 - loss: 3.0218\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2781 - loss: 3.0517 - val_accuracy: 0.2918 - val_loss: 3.1685 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2863 - loss: 3.0219 - val_accuracy: 0.2913 - val_loss: 3.1646 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2857 - loss: 3.0136 - val_accuracy: 0.2922 - val_loss: 3.1643 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2875 - loss: 3.0148 - val_accuracy: 0.2925 - val_loss: 3.1642 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2824 - loss: 3.0109 - val_accuracy: 0.2923 - val_loss: 3.1681 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1731/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2825 - loss: 3.0228\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2825 - loss: 3.0228 - val_accuracy: 0.2914 - val_loss: 3.1697 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 3.0018 - val_accuracy: 0.2930 - val_loss: 3.1679 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2839 - loss: 3.0140 - val_accuracy: 0.2925 - val_loss: 3.1681 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2828 - loss: 3.0123 - val_accuracy: 0.2935 - val_loss: 3.1690 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2885 - loss: 2.9915 - val_accuracy: 0.2938 - val_loss: 3.1680 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1340/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.2842 - loss: 2.9999\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2845 - loss: 3.0006 - val_accuracy: 0.2956 - val_loss: 3.1697 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2875 - loss: 2.9809 - val_accuracy: 0.2939 - val_loss: 3.1687 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2860 - loss: 2.9906 - val_accuracy: 0.2939 - val_loss: 3.1687 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2861 - loss: 2.9879 - val_accuracy: 0.2943 - val_loss: 3.1695 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2864 - loss: 2.9837 - val_accuracy: 0.2943 - val_loss: 3.1682 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m 540/1732\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 100ms/step - accuracy: 0.2898 - loss: 2.9863\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2869 - loss: 2.9847 - val_accuracy: 0.2953 - val_loss: 3.1692 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2864 - loss: 2.9945 - val_accuracy: 0.2953 - val_loss: 3.1688 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9876 - val_accuracy: 0.2950 - val_loss: 3.1689 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2891 - loss: 2.9706 - val_accuracy: 0.2946 - val_loss: 3.1699 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2879 - loss: 2.9775 - val_accuracy: 0.2946 - val_loss: 3.1695 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m   1/1732\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.2500 - loss: 2.9798\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28164us/step - accuracy: 0.2880 - loss: 2.9787 - val_accuracy: 0.2948 - val_loss: 3.1697 - learning_rate: 3.1250e-05\n",
      "Epoch 39: early stopping\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step  \n",
      "Acurácia: 28.64%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.10      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.23      0.14      0.17        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.43      0.12      0.19        24\n",
      "      uc0016       0.16      0.07      0.10      1016\n",
      "      uc0017       0.23      0.02      0.04       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.13      0.00      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.85      1.00      0.92        55\n",
      "      uc0024       0.32      0.34      0.33       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.22      0.03      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.27      0.27      0.27        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.49      0.68      0.57       205\n",
      "      uc0030       0.49      0.60      0.54       252\n",
      "      uc0031       0.33      0.21      0.26       574\n",
      "      uc0032       0.12      0.04      0.06        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.26      0.47      0.34        99\n",
      "      uc0040       0.39      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.53      0.44      0.48       156\n",
      "      uc0043       0.22      0.73      0.33      5240\n",
      "      uc0044       0.37      0.31      0.34       741\n",
      "      uc0045       0.35      0.09      0.14       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.31      0.02      0.05       164\n",
      "      uc0049       0.45      0.29      0.35        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.43      0.15      0.22       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.16      0.05      0.07       176\n",
      "      uc0059       0.26      0.18      0.21       116\n",
      "      uc0060       0.45      0.22      0.30       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.47      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.38      0.59      0.47      1357\n",
      "      uc0076       0.23      0.03      0.05       200\n",
      "      uc0077       0.41      0.24      0.30       681\n",
      "      uc0078       0.53      0.14      0.22        57\n",
      "      uc0079       0.50      0.03      0.06       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.31      0.32      0.32       196\n",
      "      uc0087       0.24      0.06      0.10       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.24      0.21      0.23        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.28      0.18      0.22        49\n",
      "      uc0094       0.26      0.01      0.02       785\n",
      "      uc0096       0.29      0.41      0.34      2817\n",
      "      uc0097       0.31      0.11      0.16        92\n",
      "      uc0098       0.63      0.48      0.54       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.36      0.08      0.12        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.39      0.60      0.47       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.14      0.03      0.05      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.18      0.08      0.11        79\n",
      "      uc0116       0.17      0.16      0.16        31\n",
      "      uc0117       0.42      0.50      0.46        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.42      0.11      0.17       772\n",
      "      uc0125       0.32      0.28      0.30       115\n",
      "      uc0126       0.25      0.05      0.09       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.32      0.14      0.19        88\n",
      "      uc0131       0.15      0.01      0.02       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.10      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.23      0.28      0.25       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.27      0.39      0.32        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.14      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.58      0.14      0.22        80\n",
      "      uc0150       0.15      0.01      0.01       541\n",
      "      uc0153       0.19      0.22      0.20       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.36      0.40       149\n",
      "      uc0158       0.35      0.23      0.28        26\n",
      "      uc0159       0.67      0.13      0.22        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.23      0.18      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.23      0.15      0.19       136\n",
      "      uc0167       0.12      0.04      0.06        25\n",
      "      uc0169       0.00      0.00      0.00       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.26      0.21      0.23        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.23      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.04      0.02      0.03        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.25      0.07      0.10        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.33      0.06      0.10        17\n",
      "      uc0193       0.00      0.00      0.00        54\n",
      "      uc0195       0.53      0.42      0.47        66\n",
      "      uc0197       0.28      0.13      0.18        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.46      0.22      0.29       409\n",
      "      uc0211       0.27      0.07      0.11       298\n",
      "      uc0212       0.38      0.20      0.26       274\n",
      "      uc0215       0.34      0.07      0.11       647\n",
      "      uc0216       0.22      0.37      0.27       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.39      0.72      0.51       120\n",
      "      uc0220       0.42      0.53      0.47        89\n",
      "      uc0221       0.40      0.18      0.25        90\n",
      "      uc0222       0.32      0.37      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.34      0.46      0.39        24\n",
      "      uc0226       0.45      0.75      0.57       235\n",
      "      uc0228       0.00      0.00      0.00        25\n",
      "      uc0229       0.38      0.34      0.36        94\n",
      "      uc0230       0.27      0.14      0.18        78\n",
      "      uc0232       0.46      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.40      0.01      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.63      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.33      0.06      0.10        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.26      0.17      0.20       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.07      0.01      0.01       120\n",
      "      uc1007       0.17      0.12      0.14        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.00      0.00      0.00        50\n",
      "      uc1010       0.38      0.50      0.43        70\n",
      "      uc1011       0.41      0.22      0.29        41\n",
      "      uc1012       0.17      0.03      0.05        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.07      0.04      0.05        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.32      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.71      0.35      0.47        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.45      0.39      0.42        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.64      0.44      0.52        16\n",
      "      uc2012       0.33      0.50      0.40        24\n",
      "      uc2014       0.50      0.17      0.26        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.26      0.25      0.25        80\n",
      "      uc2020       0.47      0.20      0.28       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.14      0.06      0.08       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.20      0.29        35\n",
      "      uc2028       0.79      0.81      0.80        37\n",
      "      uc2029       0.26      0.40      0.32       341\n",
      "      uc2030       0.47      0.37      0.41        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.12      0.01      0.02       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.75      0.13      0.23        45\n",
      "      uc2037       0.10      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.14      0.07      0.09        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.24      0.32      0.27        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.48      0.30      0.37        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.24      0.50      0.32        20\n",
      "      uc2049       0.53      0.44      0.48        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.40      0.12      0.19        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.41      0.31      0.35        42\n",
      "      uc2059       0.23      0.60      0.33       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.65      0.30      0.41        37\n",
      "      uc2063       0.92      0.55      0.69        20\n",
      "      uc2064       0.24      0.10      0.14        61\n",
      "      uc2065       0.37      0.45      0.41        66\n",
      "      uc2066       0.27      0.19      0.22       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.08      0.09      0.08        22\n",
      "      uc2072       1.00      0.11      0.19        19\n",
      "      uc2073       0.39      0.44      0.42        25\n",
      "      uc2074       0.12      0.12      0.12         8\n",
      "      uc2075       0.38      0.20      0.26        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.52      0.45      0.48        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.64      0.50      0.56        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.28      0.27      0.27        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.33      0.04      0.08        23\n",
      "      uc2087       0.77      0.20      0.32       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.42      0.58      0.49        19\n",
      "      uc2092       0.12      0.02      0.03       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.25      0.10      0.15        29\n",
      "      uc2096       0.06      0.03      0.04        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_04 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 6/17: usuario_05\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1557 - loss: 4.2065 - val_accuracy: 0.2709 - val_loss: 3.3392 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2456 - loss: 3.4327 - val_accuracy: 0.2774 - val_loss: 3.2295 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28206us/step - accuracy: 0.2580 - loss: 3.3234 - val_accuracy: 0.2803 - val_loss: 3.2042 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2601 - loss: 3.2691 - val_accuracy: 0.2819 - val_loss: 3.1756 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2639 - loss: 3.2375 - val_accuracy: 0.2875 - val_loss: 3.1699 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-47s\u001b[0m -27431us/step - accuracy: 0.2614 - loss: 3.2182 - val_accuracy: 0.2826 - val_loss: 3.1736 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2679 - loss: 3.1994 - val_accuracy: 0.2845 - val_loss: 3.1731 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2677 - loss: 3.1714 - val_accuracy: 0.2845 - val_loss: 3.1722 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28215us/step - accuracy: 0.2641 - loss: 3.1598 - val_accuracy: 0.2865 - val_loss: 3.1707 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m 567/1732\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 95ms/step - accuracy: 0.2726 - loss: 3.1185 \n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2729 - loss: 3.1347 - val_accuracy: 0.2890 - val_loss: 3.1724 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2707 - loss: 3.1195 - val_accuracy: 0.2895 - val_loss: 3.1488 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2743 - loss: 3.1096 - val_accuracy: 0.2909 - val_loss: 3.1454 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2753 - loss: 3.0901 - val_accuracy: 0.2899 - val_loss: 3.1443 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2774 - loss: 3.0966 - val_accuracy: 0.2920 - val_loss: 3.1431 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2768 - loss: 3.0701 - val_accuracy: 0.2917 - val_loss: 3.1476 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28142us/step - accuracy: 0.2801 - loss: 3.0606 - val_accuracy: 0.2927 - val_loss: 3.1470 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2807 - loss: 3.0584 - val_accuracy: 0.2909 - val_loss: 3.1511 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2791 - loss: 3.0591 - val_accuracy: 0.2925 - val_loss: 3.1532 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1059/1732\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 52ms/step - accuracy: 0.2812 - loss: 3.0413\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2801 - loss: 3.0486 - val_accuracy: 0.2922 - val_loss: 3.1541 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2813 - loss: 3.0378 - val_accuracy: 0.2926 - val_loss: 3.1516 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2807 - loss: 3.0353 - val_accuracy: 0.2934 - val_loss: 3.1524 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2842 - loss: 3.0199 - val_accuracy: 0.2911 - val_loss: 3.1525 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2850 - loss: 3.0083 - val_accuracy: 0.2941 - val_loss: 3.1533 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1722/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2829 - loss: 3.0138\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2829 - loss: 3.0139 - val_accuracy: 0.2943 - val_loss: 3.1511 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28124us/step - accuracy: 0.2848 - loss: 3.0088 - val_accuracy: 0.2932 - val_loss: 3.1516 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2827 - loss: 2.9953 - val_accuracy: 0.2936 - val_loss: 3.1517 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2870 - loss: 3.0024 - val_accuracy: 0.2936 - val_loss: 3.1515 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2893 - loss: 2.9917 - val_accuracy: 0.2938 - val_loss: 3.1522 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1489/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.2873 - loss: 3.0043 \n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2875 - loss: 3.0039 - val_accuracy: 0.2954 - val_loss: 3.1532 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2853 - loss: 2.9974 - val_accuracy: 0.2940 - val_loss: 3.1550 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.2921 - loss: 2.9761 - val_accuracy: 0.2946 - val_loss: 3.1545 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28137us/step - accuracy: 0.2867 - loss: 2.9845 - val_accuracy: 0.2948 - val_loss: 3.1548 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2877 - loss: 2.9834 - val_accuracy: 0.2943 - val_loss: 3.1557 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m 833/1732\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.2827 - loss: 2.9994 \n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2859 - loss: 2.9943 - val_accuracy: 0.2935 - val_loss: 3.1549 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2862 - loss: 2.9910 - val_accuracy: 0.2941 - val_loss: 3.1553 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2883 - loss: 2.9784 - val_accuracy: 0.2940 - val_loss: 3.1551 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m -28273us/step - accuracy: 0.2868 - loss: 2.9836 - val_accuracy: 0.2943 - val_loss: 3.1554 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2898 - loss: 2.9848 - val_accuracy: 0.2939 - val_loss: 3.1547 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m 219/1732\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:09\u001b[0m 244ms/step - accuracy: 0.2863 - loss: 2.9884\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2893 - loss: 2.9743 - val_accuracy: 0.2937 - val_loss: 3.1545 - learning_rate: 3.1250e-05\n",
      "Epoch 39: early stopping\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step   \n",
      "Acurácia: 28.84%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.08      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.19      0.14      0.16        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.31      0.17      0.22        24\n",
      "      uc0016       0.16      0.08      0.10      1016\n",
      "      uc0017       0.12      0.03      0.05       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.00      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.86      1.00      0.92        55\n",
      "      uc0024       0.31      0.34      0.32       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.06      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.21      0.27      0.23        26\n",
      "      uc0028       0.38      0.03      0.05       109\n",
      "      uc0029       0.49      0.69      0.57       205\n",
      "      uc0030       0.48      0.59      0.53       252\n",
      "      uc0031       0.39      0.18      0.24       574\n",
      "      uc0032       0.29      0.15      0.20        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       1.00      0.20      0.33         5\n",
      "      uc0039       0.28      0.49      0.36        99\n",
      "      uc0040       0.42      0.39      0.40       136\n",
      "      uc0041       0.33      0.04      0.08        23\n",
      "      uc0042       0.52      0.47      0.49       156\n",
      "      uc0043       0.22      0.72      0.33      5240\n",
      "      uc0044       0.40      0.29      0.34       741\n",
      "      uc0045       0.42      0.11      0.17       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.30      0.04      0.07       164\n",
      "      uc0049       0.45      0.29      0.35        31\n",
      "      uc0050       0.50      0.12      0.20         8\n",
      "      uc0052       0.20      0.02      0.04        46\n",
      "      uc0053       0.40      0.17      0.24       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.16      0.03      0.06       176\n",
      "      uc0059       0.22      0.18      0.20       116\n",
      "      uc0060       0.44      0.30      0.36       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.30      0.48      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.38      0.59      0.46      1357\n",
      "      uc0076       0.18      0.06      0.09       200\n",
      "      uc0077       0.42      0.24      0.31       681\n",
      "      uc0078       0.53      0.14      0.22        57\n",
      "      uc0079       0.33      0.06      0.10       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.35      0.37      0.36       196\n",
      "      uc0087       0.25      0.08      0.13       334\n",
      "      uc0089       0.07      0.02      0.04        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.22      0.11      0.15        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.22      0.22      0.22        49\n",
      "      uc0094       0.19      0.02      0.03       785\n",
      "      uc0096       0.28      0.43      0.34      2817\n",
      "      uc0097       0.30      0.11      0.16        92\n",
      "      uc0098       0.68      0.48      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.46      0.09      0.15        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.40      0.60      0.48       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.12      0.01      0.03      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.27      0.08      0.12        79\n",
      "      uc0116       0.14      0.23      0.17        31\n",
      "      uc0117       0.46      0.65      0.54        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.42      0.11      0.17       772\n",
      "      uc0125       0.34      0.25      0.29       115\n",
      "      uc0126       0.27      0.02      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.19      0.05      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.40      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.11      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.24      0.28      0.26       431\n",
      "      uc0140       0.09      0.08      0.09        36\n",
      "      uc0141       0.33      0.39      0.36        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.22      0.12      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.43      0.12      0.19        80\n",
      "      uc0150       0.17      0.01      0.01       541\n",
      "      uc0153       0.20      0.23      0.21       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.46      0.38      0.41       149\n",
      "      uc0158       0.35      0.27      0.30        26\n",
      "      uc0159       1.00      0.07      0.12        15\n",
      "      uc0161       0.00      0.00      0.00         7\n",
      "      uc0162       0.24      0.17      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.25      0.18      0.21       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.33      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.26      0.18      0.21        34\n",
      "      uc0173       0.50      0.06      0.11        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.24      0.11      0.15      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.13      0.06      0.09        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.53      0.42      0.47        66\n",
      "      uc0197       0.30      0.16      0.21        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.11      0.04      0.06        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.50      0.19      0.28       409\n",
      "      uc0211       0.33      0.06      0.10       298\n",
      "      uc0212       0.37      0.26      0.31       274\n",
      "      uc0215       0.27      0.09      0.13       647\n",
      "      uc0216       0.24      0.36      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.41      0.70      0.52       120\n",
      "      uc0220       0.45      0.52      0.48        89\n",
      "      uc0221       0.41      0.19      0.26        90\n",
      "      uc0222       0.31      0.38      0.34      1234\n",
      "      uc0223       0.00      0.00      0.00        20\n",
      "      uc0225       0.33      0.29      0.31        24\n",
      "      uc0226       0.46      0.75      0.57       235\n",
      "      uc0228       0.09      0.04      0.06        25\n",
      "      uc0229       0.32      0.37      0.35        94\n",
      "      uc0230       0.18      0.09      0.12        78\n",
      "      uc0232       0.46      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.22      0.02      0.04       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.60      0.56       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.50      0.04      0.07        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.50      0.06      0.11        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.26      0.14      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.29      0.12      0.17        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.00      0.00      0.00        50\n",
      "      uc1010       0.33      0.51      0.40        70\n",
      "      uc1011       0.41      0.22      0.29        41\n",
      "      uc1012       0.06      0.03      0.04        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.27      0.23      0.25        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.47      0.32      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.68      0.35      0.47        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.48      0.39      0.43        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.35      0.44      0.39        16\n",
      "      uc2012       0.64      0.38      0.47        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.12      0.17      0.14         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.29      0.24      0.26        80\n",
      "      uc2020       0.65      0.18      0.28       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.17      0.09      0.12       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.54      0.37      0.44        35\n",
      "      uc2028       0.79      0.81      0.80        37\n",
      "      uc2029       0.29      0.43      0.35       341\n",
      "      uc2030       0.64      0.20      0.30        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.47      0.20      0.28        45\n",
      "      uc2037       0.08      0.03      0.04        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.15      0.07      0.09        61\n",
      "      uc2040       0.29      0.08      0.13        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.28      0.32      0.30        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.34      0.28      0.31        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.47      0.45      0.46        20\n",
      "      uc2049       0.74      0.39      0.51        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.36      0.25      0.30        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.50      0.30      0.38        20\n",
      "      uc2056       0.75      0.36      0.48        42\n",
      "      uc2059       0.23      0.63      0.33       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.35      0.38      0.36        37\n",
      "      uc2063       0.73      0.55      0.63        20\n",
      "      uc2064       0.20      0.10      0.13        61\n",
      "      uc2065       0.39      0.47      0.43        66\n",
      "      uc2066       0.36      0.17      0.23       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.24      0.23      0.23        22\n",
      "      uc2072       0.40      0.11      0.17        19\n",
      "      uc2073       0.48      0.40      0.43        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       1.00      0.16      0.28        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.47      0.47      0.47        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.50      0.43      0.46        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.23      0.35      0.28        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       0.88      0.18      0.30       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.09      0.11      0.10         9\n",
      "      uc2090       0.42      0.58      0.49        19\n",
      "      uc2092       0.17      0.03      0.05       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.25      0.10      0.15        29\n",
      "      uc2096       0.14      0.07      0.09        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.20      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_05 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 7/17: usuario_06\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1538 - loss: 4.2392 - val_accuracy: 0.2704 - val_loss: 3.3558 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2461 - loss: 3.4246 - val_accuracy: 0.2746 - val_loss: 3.2401 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m 2ms/step - accuracy: 0.2566 - loss: 3.3040 - val_accuracy: 0.2824 - val_loss: 3.2019 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2578 - loss: 3.2722 - val_accuracy: 0.2842 - val_loss: 3.1871 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2638 - loss: 3.2397 - val_accuracy: 0.2851 - val_loss: 3.1767 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2640 - loss: 3.2107 - val_accuracy: 0.2850 - val_loss: 3.1774 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2697 - loss: 3.1703 - val_accuracy: 0.2860 - val_loss: 3.1692 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2663 - loss: 3.1825 - val_accuracy: 0.2850 - val_loss: 3.1725 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2667 - loss: 3.1645 - val_accuracy: 0.2855 - val_loss: 3.1682 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2681 - loss: 3.1499 - val_accuracy: 0.2845 - val_loss: 3.1640 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2689 - loss: 3.1549 - val_accuracy: 0.2839 - val_loss: 3.1720 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2703 - loss: 3.1399 - val_accuracy: 0.2872 - val_loss: 3.1726 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2711 - loss: 3.1269 - val_accuracy: 0.2870 - val_loss: 3.1800 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2735 - loss: 3.1200 - val_accuracy: 0.2868 - val_loss: 3.1788 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1716/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2724 - loss: 3.1211\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2724 - loss: 3.1213 - val_accuracy: 0.2865 - val_loss: 3.1750 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2724 - loss: 3.1005 - val_accuracy: 0.2901 - val_loss: 3.1672 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2781 - loss: 3.0725 - val_accuracy: 0.2890 - val_loss: 3.1708 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2756 - loss: 3.0681 - val_accuracy: 0.2893 - val_loss: 3.1662 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2791 - loss: 3.0521 - val_accuracy: 0.2900 - val_loss: 3.1715 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m 698/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 79ms/step - accuracy: 0.2793 - loss: 3.0638\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2790 - loss: 3.0608 - val_accuracy: 0.2929 - val_loss: 3.1691 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2852 - loss: 3.0341 - val_accuracy: 0.2937 - val_loss: 3.1657 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2823 - loss: 3.0295 - val_accuracy: 0.2945 - val_loss: 3.1684 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2880 - loss: 3.0041 - val_accuracy: 0.2948 - val_loss: 3.1680 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28306us/step - accuracy: 0.2841 - loss: 3.0043 - val_accuracy: 0.2927 - val_loss: 3.1660 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1721/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2833 - loss: 3.0178\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2833 - loss: 3.0178 - val_accuracy: 0.2943 - val_loss: 3.1716 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2892 - loss: 2.9897 - val_accuracy: 0.2943 - val_loss: 3.1699 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2831 - loss: 3.0091 - val_accuracy: 0.2931 - val_loss: 3.1714 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2807 - loss: 3.0108 - val_accuracy: 0.2940 - val_loss: 3.1713 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2846 - loss: 2.9986 - val_accuracy: 0.2933 - val_loss: 3.1716 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1722/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2861 - loss: 2.9975\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2861 - loss: 2.9975 - val_accuracy: 0.2938 - val_loss: 3.1739 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2892 - loss: 2.9763 - val_accuracy: 0.2933 - val_loss: 3.1717 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2886 - loss: 3.0022 - val_accuracy: 0.2943 - val_loss: 3.1723 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2901 - loss: 2.9882 - val_accuracy: 0.2943 - val_loss: 3.1733 - learning_rate: 6.2500e-05\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 38ms/step\n",
      "Acurácia: 28.46%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.06      0.02      0.03        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.60      0.06      0.11        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.40      0.17      0.24        24\n",
      "      uc0016       0.16      0.08      0.10      1016\n",
      "      uc0017       0.11      0.02      0.03       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.17      0.01      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.85      1.00      0.92        55\n",
      "      uc0024       0.31      0.34      0.33       674\n",
      "      uc0025       0.75      0.33      0.46         9\n",
      "   uc0025_01       0.07      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.35      0.23      0.28        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.48      0.67      0.56       205\n",
      "      uc0030       0.50      0.57      0.53       252\n",
      "      uc0031       0.33      0.20      0.25       574\n",
      "      uc0032       0.32      0.15      0.21        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.47      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.32      0.40      0.36        99\n",
      "      uc0040       0.39      0.37      0.38       136\n",
      "      uc0041       0.17      0.04      0.07        23\n",
      "      uc0042       0.54      0.47      0.51       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.37      0.30      0.33       741\n",
      "      uc0045       0.32      0.05      0.08       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.18      0.04      0.07       164\n",
      "      uc0049       0.47      0.23      0.30        31\n",
      "      uc0050       0.50      0.12      0.20         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.42      0.18      0.25       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.12      0.05      0.07       176\n",
      "      uc0059       0.24      0.13      0.17       116\n",
      "      uc0060       0.39      0.28      0.32       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.46      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.59      0.46      1357\n",
      "      uc0076       0.12      0.01      0.02       200\n",
      "      uc0077       0.46      0.22      0.30       681\n",
      "      uc0078       0.45      0.09      0.15        57\n",
      "      uc0079       0.55      0.05      0.09       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.35      0.34      0.35       196\n",
      "      uc0087       0.25      0.09      0.13       334\n",
      "      uc0089       1.00      0.02      0.05        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.41      0.19      0.25        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.29      0.18      0.23        49\n",
      "      uc0094       0.40      0.01      0.01       785\n",
      "      uc0096       0.27      0.44      0.33      2817\n",
      "      uc0097       0.31      0.12      0.17        92\n",
      "      uc0098       0.67      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.38      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.38      0.60      0.47       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.02      0.00      0.00      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.05      0.00      0.01       284\n",
      "      uc0115       0.43      0.08      0.13        79\n",
      "      uc0116       0.20      0.19      0.20        31\n",
      "      uc0117       0.39      0.50      0.44        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.40      0.11      0.17       772\n",
      "      uc0125       0.31      0.29      0.30       115\n",
      "      uc0126       0.26      0.03      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.32      0.14      0.19        88\n",
      "      uc0131       0.22      0.04      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.43      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.13      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.22      0.26      0.24       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.28      0.39      0.33        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.14      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.44      0.15      0.22        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.12      0.13      0.13       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.33      0.38       149\n",
      "      uc0158       0.40      0.23      0.29        26\n",
      "      uc0159       0.00      0.00      0.00        15\n",
      "      uc0161       0.33      0.14      0.20         7\n",
      "      uc0162       0.22      0.18      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.30      0.15      0.20       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.08      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.21      0.18      0.19        34\n",
      "      uc0173       1.00      0.06      0.12        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.21      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.00      0.00      0.00        54\n",
      "      uc0195       0.48      0.41      0.44        66\n",
      "      uc0197       0.33      0.13      0.19        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.47      0.19      0.27       409\n",
      "      uc0211       0.23      0.08      0.12       298\n",
      "      uc0212       0.38      0.27      0.31       274\n",
      "      uc0215       0.27      0.09      0.13       647\n",
      "      uc0216       0.27      0.38      0.31       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.42      0.75      0.54       120\n",
      "      uc0220       0.41      0.52      0.46        89\n",
      "      uc0221       0.43      0.20      0.27        90\n",
      "      uc0222       0.32      0.36      0.34      1234\n",
      "      uc0223       0.00      0.00      0.00        20\n",
      "      uc0225       0.41      0.46      0.43        24\n",
      "      uc0226       0.46      0.74      0.57       235\n",
      "      uc0228       0.20      0.08      0.11        25\n",
      "      uc0229       0.36      0.34      0.35        94\n",
      "      uc0230       0.39      0.09      0.15        78\n",
      "      uc0232       0.45      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.12      0.01      0.02       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.54      0.61      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.14      0.06      0.08        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.21      0.06      0.10       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.21      0.24      0.22        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.18      0.18        50\n",
      "      uc1010       0.37      0.50      0.42        70\n",
      "      uc1011       0.40      0.24      0.30        41\n",
      "      uc1012       0.18      0.06      0.10        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.28      0.19      0.23        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.34      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.23      0.12      0.16        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.68      0.72      0.70        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.26      0.27      0.27        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.35      0.56      0.43        16\n",
      "      uc2012       0.53      0.42      0.47        24\n",
      "      uc2014       0.50      0.17      0.26        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.09      0.07      0.08        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.21      0.12      0.16        80\n",
      "      uc2020       0.50      0.18      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.17      0.08      0.11       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.23      0.31        35\n",
      "      uc2028       0.78      0.84      0.81        37\n",
      "      uc2029       0.32      0.36      0.34       341\n",
      "      uc2030       0.47      0.37      0.41        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.67      0.13      0.22        45\n",
      "      uc2037       0.09      0.03      0.04        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.19      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.46      0.27      0.34        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.36      0.28      0.31        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.35      0.45      0.39        20\n",
      "      uc2049       0.42      0.56      0.48        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.40      0.30      0.34        20\n",
      "      uc2056       0.73      0.26      0.39        42\n",
      "      uc2059       0.21      0.61      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.33      0.35      0.34        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.24      0.11      0.16        61\n",
      "      uc2065       0.24      0.39      0.30        66\n",
      "      uc2066       0.24      0.22      0.23       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.07      0.18      0.10        22\n",
      "      uc2072       0.50      0.11      0.17        19\n",
      "      uc2073       0.64      0.36      0.46        25\n",
      "      uc2074       0.09      0.12      0.11         8\n",
      "      uc2075       0.57      0.16      0.25        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.60      0.47      0.53        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.60      0.43      0.50        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.23      0.23      0.23        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.25      0.04      0.07        23\n",
      "      uc2087       1.00      0.18      0.31       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.29      0.26      0.28        19\n",
      "      uc2092       0.08      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.28     46182\n",
      "   macro avg       0.19      0.13      0.13     46182\n",
      "weighted avg       0.27      0.28      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_06 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 8/17: usuario_07\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.1564 - loss: 4.1862 - val_accuracy: 0.2685 - val_loss: 3.3611 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m 3ms/step - accuracy: 0.2446 - loss: 3.4260 - val_accuracy: 0.2773 - val_loss: 3.2454 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m -28229us/step - accuracy: 0.2521 - loss: 3.3217 - val_accuracy: 0.2829 - val_loss: 3.1933 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2629 - loss: 3.2641 - val_accuracy: 0.2826 - val_loss: 3.1789 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2585 - loss: 3.2419 - val_accuracy: 0.2853 - val_loss: 3.1609 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2640 - loss: 3.2024 - val_accuracy: 0.2852 - val_loss: 3.1808 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2678 - loss: 3.1956 - val_accuracy: 0.2846 - val_loss: 3.1694 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2702 - loss: 3.1773 - val_accuracy: 0.2831 - val_loss: 3.1770 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2663 - loss: 3.1746 - val_accuracy: 0.2843 - val_loss: 3.1669 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m 646/1732\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2672 - loss: 3.1590\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28244us/step - accuracy: 0.2675 - loss: 3.1614 - val_accuracy: 0.2857 - val_loss: 3.1734 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2741 - loss: 3.1213 - val_accuracy: 0.2897 - val_loss: 3.1556 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2721 - loss: 3.1047 - val_accuracy: 0.2901 - val_loss: 3.1561 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2779 - loss: 3.0820 - val_accuracy: 0.2900 - val_loss: 3.1526 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2743 - loss: 3.0911 - val_accuracy: 0.2901 - val_loss: 3.1535 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2770 - loss: 3.0690 - val_accuracy: 0.2891 - val_loss: 3.1533 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2780 - loss: 3.0659 - val_accuracy: 0.2904 - val_loss: 3.1584 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2760 - loss: 3.0578 - val_accuracy: 0.2928 - val_loss: 3.1613 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m 367/1732\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:19\u001b[0m 146ms/step - accuracy: 0.2791 - loss: 3.0511\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2775 - loss: 3.0641 - val_accuracy: 0.2908 - val_loss: 3.1567 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2812 - loss: 3.0418 - val_accuracy: 0.2943 - val_loss: 3.1534 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2807 - loss: 3.0308 - val_accuracy: 0.2940 - val_loss: 3.1550 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2819 - loss: 3.0208 - val_accuracy: 0.2939 - val_loss: 3.1524 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2812 - loss: 3.0318 - val_accuracy: 0.2951 - val_loss: 3.1556 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2842 - loss: 3.0181 - val_accuracy: 0.2946 - val_loss: 3.1572 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2788 - loss: 3.0366 - val_accuracy: 0.2942 - val_loss: 3.1585 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2834 - loss: 3.0228 - val_accuracy: 0.2951 - val_loss: 3.1624 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m 570/1732\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 95ms/step - accuracy: 0.2808 - loss: 3.0397\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2826 - loss: 3.0262 - val_accuracy: 0.2930 - val_loss: 3.1593 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2840 - loss: 3.0004 - val_accuracy: 0.2956 - val_loss: 3.1580 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2867 - loss: 3.0023 - val_accuracy: 0.2951 - val_loss: 3.1600 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2883 - loss: 2.9847 - val_accuracy: 0.2943 - val_loss: 3.1591 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2888 - loss: 2.9982 - val_accuracy: 0.2952 - val_loss: 3.1609 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m 153/1732\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:11\u001b[0m 349ms/step - accuracy: 0.2999 - loss: 2.9205 \n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2914 - loss: 2.9694 - val_accuracy: 0.2959 - val_loss: 3.1609 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2870 - loss: 2.9957 - val_accuracy: 0.2958 - val_loss: 3.1623 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2899 - loss: 2.9630 - val_accuracy: 0.2959 - val_loss: 3.1622 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2866 - loss: 2.9866 - val_accuracy: 0.2954 - val_loss: 3.1618 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9813 - val_accuracy: 0.2956 - val_loss: 3.1631 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1728/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2863 - loss: 2.9797\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2863 - loss: 2.9797 - val_accuracy: 0.2955 - val_loss: 3.1617 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2901 - loss: 2.9731 - val_accuracy: 0.2956 - val_loss: 3.1630 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m 2ms/step - accuracy: 0.2911 - loss: 2.9738 - val_accuracy: 0.2944 - val_loss: 3.1626 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 2.9871 - val_accuracy: 0.2954 - val_loss: 3.1625 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2911 - loss: 2.9619 - val_accuracy: 0.2945 - val_loss: 3.1624 - learning_rate: 3.1250e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1721/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9746\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9746 - val_accuracy: 0.2943 - val_loss: 3.1625 - learning_rate: 3.1250e-05\n",
      "Epoch 41: early stopping\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.64%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.06      0.02      0.03        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.55      0.12      0.19        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.36      0.17      0.23        24\n",
      "      uc0016       0.16      0.07      0.10      1016\n",
      "      uc0017       0.12      0.03      0.05       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.20      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.79      1.00      0.88        55\n",
      "      uc0024       0.33      0.33      0.33       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.09      0.07      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.27      0.23      0.25        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.47      0.66      0.55       205\n",
      "      uc0030       0.49      0.59      0.54       252\n",
      "      uc0031       0.35      0.21      0.26       574\n",
      "      uc0032       0.27      0.15      0.19        46\n",
      "      uc0033       0.24      0.13      0.17        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.28      0.52      0.36        99\n",
      "      uc0040       0.40      0.40      0.40       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.56      0.45      0.50       156\n",
      "      uc0043       0.22      0.71      0.34      5240\n",
      "      uc0044       0.39      0.30      0.34       741\n",
      "      uc0045       0.38      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.10      0.01      0.02       164\n",
      "      uc0049       0.45      0.32      0.38        31\n",
      "      uc0050       0.50      0.12      0.20         8\n",
      "      uc0052       0.22      0.04      0.07        46\n",
      "      uc0053       0.42      0.16      0.24       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.12      0.03      0.05       176\n",
      "      uc0059       0.21      0.15      0.17       116\n",
      "      uc0060       0.37      0.27      0.31       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.25      0.02      0.04       102\n",
      "      uc0069       0.29      0.48      0.36      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.61      0.46      1357\n",
      "      uc0076       0.23      0.03      0.05       200\n",
      "      uc0077       0.42      0.23      0.29       681\n",
      "      uc0078       0.45      0.09      0.15        57\n",
      "      uc0079       0.32      0.05      0.09       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.22      0.06      0.10        32\n",
      "      uc0086       0.34      0.35      0.34       196\n",
      "      uc0087       0.23      0.08      0.12       334\n",
      "      uc0089       1.00      0.02      0.05        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.23      0.24      0.23        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.31      0.18      0.23        49\n",
      "      uc0094       0.27      0.02      0.03       785\n",
      "      uc0096       0.28      0.43      0.34      2817\n",
      "      uc0097       0.32      0.12      0.17        92\n",
      "      uc0098       0.67      0.49      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.50      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.09      0.05      0.06        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.35      0.61      0.44       330\n",
      "      uc0108       0.69      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.12      0.02      0.03      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.14      0.01      0.03        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.35      0.10      0.16        79\n",
      "      uc0116       0.21      0.19      0.20        31\n",
      "      uc0117       0.43      0.62      0.51        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.38      0.11      0.17       772\n",
      "      uc0125       0.31      0.29      0.30       115\n",
      "      uc0126       0.23      0.04      0.07       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.00      0.00      0.00       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.43      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.20      0.08      0.11        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.22      0.25      0.23       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.30      0.39      0.34        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.13      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.42      0.14      0.21        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.12      0.14      0.13       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.43      0.34      0.38       149\n",
      "      uc0158       0.37      0.27      0.31        26\n",
      "      uc0159       1.00      0.07      0.12        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.23      0.18      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.30      0.15      0.20       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.14      0.02      0.04       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.27      0.21      0.23        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.22      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.25      0.00      0.01       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.25      0.03      0.06        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.42      0.42      0.42        66\n",
      "      uc0197       0.25      0.11      0.15        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.43      0.22      0.29       409\n",
      "      uc0211       0.16      0.04      0.06       298\n",
      "      uc0212       0.36      0.25      0.30       274\n",
      "      uc0215       0.25      0.09      0.13       647\n",
      "      uc0216       0.25      0.37      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.38      0.75      0.50       120\n",
      "      uc0220       0.39      0.52      0.44        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.31      0.38      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.43      0.38      0.40        24\n",
      "      uc0226       0.46      0.74      0.56       235\n",
      "      uc0228       0.10      0.04      0.06        25\n",
      "      uc0229       0.34      0.38      0.36        94\n",
      "      uc0230       0.23      0.14      0.18        78\n",
      "      uc0232       0.47      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.25      0.02      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.63      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.10      0.06      0.07        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.29      0.15      0.20       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.09      0.01      0.02       120\n",
      "      uc1007       0.21      0.18      0.19        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.33      0.02      0.04        50\n",
      "      uc1010       0.37      0.51      0.43        70\n",
      "      uc1011       0.45      0.24      0.32        41\n",
      "      uc1012       0.00      0.00      0.00        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.23      0.12      0.15        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.47      0.34      0.40       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.67      0.22      0.33        54\n",
      "      uc2007       0.25      0.06      0.10        17\n",
      "      uc2008       0.31      0.37      0.34        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.50      0.44      0.47        16\n",
      "      uc2012       0.60      0.50      0.55        24\n",
      "      uc2014       0.43      0.15      0.22        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.35      0.11      0.17        80\n",
      "      uc2020       0.49      0.18      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.20      0.14      0.16       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.56      0.26      0.35        35\n",
      "      uc2028       0.78      0.84      0.81        37\n",
      "      uc2029       0.32      0.40      0.35       341\n",
      "      uc2030       0.69      0.20      0.31        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.60      0.13      0.22        45\n",
      "      uc2037       1.00      0.03      0.06        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.19      0.05      0.08        61\n",
      "      uc2040       0.33      0.04      0.07        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.43      0.27      0.33        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.36      0.28      0.31        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.33      0.40      0.36        20\n",
      "      uc2049       0.47      0.44      0.46        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.20      0.05      0.08        20\n",
      "      uc2056       0.64      0.33      0.44        42\n",
      "      uc2059       0.21      0.59      0.31       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.46      0.35      0.40        37\n",
      "      uc2063       1.00      0.55      0.71        20\n",
      "      uc2064       0.24      0.11      0.16        61\n",
      "      uc2065       0.27      0.44      0.33        66\n",
      "      uc2066       0.20      0.22      0.21       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.13      0.14      0.13        22\n",
      "      uc2072       0.25      0.11      0.15        19\n",
      "      uc2073       0.69      0.36      0.47        25\n",
      "      uc2074       0.12      0.12      0.12         8\n",
      "      uc2075       0.14      0.08      0.10        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.52      0.45      0.48        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.75      0.43      0.55        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.24      0.31      0.27        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.20      0.33       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.22      0.04      0.07       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.08      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.20      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_07 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 9/17: usuario_08\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1575 - loss: 4.1949 - val_accuracy: 0.2681 - val_loss: 3.3426 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2456 - loss: 3.4300 - val_accuracy: 0.2749 - val_loss: 3.2490 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2539 - loss: 3.3229 - val_accuracy: 0.2842 - val_loss: 3.1987 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2587 - loss: 3.2605 - val_accuracy: 0.2836 - val_loss: 3.1858 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2650 - loss: 3.2119 - val_accuracy: 0.2867 - val_loss: 3.1735 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2650 - loss: 3.1987 - val_accuracy: 0.2866 - val_loss: 3.1671 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.2655 - loss: 3.1946 - val_accuracy: 0.2858 - val_loss: 3.1725 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28210us/step - accuracy: 0.2683 - loss: 3.1717 - val_accuracy: 0.2885 - val_loss: 3.1643 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2690 - loss: 3.1593 - val_accuracy: 0.2885 - val_loss: 3.1735 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2680 - loss: 3.1500 - val_accuracy: 0.2882 - val_loss: 3.1705 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2687 - loss: 3.1359 - val_accuracy: 0.2873 - val_loss: 3.1716 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2689 - loss: 3.1454 - val_accuracy: 0.2859 - val_loss: 3.1774 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1459/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.2689 - loss: 3.1265\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2688 - loss: 3.1295 - val_accuracy: 0.2857 - val_loss: 3.1778 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2744 - loss: 3.1021 - val_accuracy: 0.2894 - val_loss: 3.1634 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2773 - loss: 3.0773 - val_accuracy: 0.2898 - val_loss: 3.1632 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2794 - loss: 3.0716 - val_accuracy: 0.2894 - val_loss: 3.1682 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2775 - loss: 3.0665 - val_accuracy: 0.2924 - val_loss: 3.1645 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2752 - loss: 3.0667 - val_accuracy: 0.2898 - val_loss: 3.1653 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2792 - loss: 3.0543 - val_accuracy: 0.2895 - val_loss: 3.1710 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m 867/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 63ms/step - accuracy: 0.2814 - loss: 3.0295\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2807 - loss: 3.0386 - val_accuracy: 0.2886 - val_loss: 3.1711 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2798 - loss: 3.0409 - val_accuracy: 0.2899 - val_loss: 3.1718 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2840 - loss: 3.0228 - val_accuracy: 0.2903 - val_loss: 3.1721 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2823 - loss: 3.0174 - val_accuracy: 0.2912 - val_loss: 3.1707 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2833 - loss: 3.0095 - val_accuracy: 0.2914 - val_loss: 3.1731 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1668/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.2844 - loss: 3.0052\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2844 - loss: 3.0057 - val_accuracy: 0.2919 - val_loss: 3.1738 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2860 - loss: 3.0036 - val_accuracy: 0.2931 - val_loss: 3.1756 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2859 - loss: 3.0003 - val_accuracy: 0.2940 - val_loss: 3.1758 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28269us/step - accuracy: 0.2841 - loss: 3.0024 - val_accuracy: 0.2943 - val_loss: 3.1746 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2863 - loss: 2.9883 - val_accuracy: 0.2940 - val_loss: 3.1754 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m 633/1732\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 86ms/step - accuracy: 0.2897 - loss: 2.9611\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 2.9786 - val_accuracy: 0.2932 - val_loss: 3.1739 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2894 - loss: 2.9942 - val_accuracy: 0.2938 - val_loss: 3.1743 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2864 - loss: 2.9892 - val_accuracy: 0.2936 - val_loss: 3.1768 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2863 - loss: 2.9920 - val_accuracy: 0.2933 - val_loss: 3.1752 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2876 - loss: 2.9823 - val_accuracy: 0.2938 - val_loss: 3.1765 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1678/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2886 - loss: 2.9682\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9686 - val_accuracy: 0.2937 - val_loss: 3.1761 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2860 - loss: 2.9769 - val_accuracy: 0.2933 - val_loss: 3.1770 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2874 - loss: 2.9833 - val_accuracy: 0.2936 - val_loss: 3.1772 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2826 - loss: 2.9826 - val_accuracy: 0.2935 - val_loss: 3.1772 - learning_rate: 3.1250e-05\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.58%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.12      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.25      0.08      0.12        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.36      0.17      0.23        24\n",
      "      uc0016       0.17      0.08      0.11      1016\n",
      "      uc0017       0.17      0.03      0.05       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.12      0.00      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.85      1.00      0.92        55\n",
      "      uc0024       0.33      0.33      0.33       674\n",
      "      uc0025       0.75      0.33      0.46         9\n",
      "   uc0025_01       0.50      0.01      0.03        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.31      0.15      0.21        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.49      0.67      0.57       205\n",
      "      uc0030       0.49      0.59      0.53       252\n",
      "      uc0031       0.36      0.20      0.26       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.50      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.30      0.40      0.34        99\n",
      "      uc0040       0.41      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.54      0.47      0.50       156\n",
      "      uc0043       0.22      0.70      0.33      5240\n",
      "      uc0044       0.38      0.30      0.34       741\n",
      "      uc0045       0.44      0.10      0.16       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.40      0.26      0.31        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.30      0.15      0.20       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.19      0.04      0.07       176\n",
      "      uc0059       0.23      0.18      0.20       116\n",
      "      uc0060       0.38      0.28      0.32       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       1.00      0.07      0.12        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.29      0.48      0.36      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.59      0.46      1357\n",
      "      uc0076       0.20      0.01      0.03       200\n",
      "      uc0077       0.43      0.22      0.29       681\n",
      "      uc0078       0.43      0.16      0.23        57\n",
      "      uc0079       0.38      0.05      0.08       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.20      0.06      0.10        32\n",
      "      uc0086       0.32      0.36      0.34       196\n",
      "      uc0087       0.26      0.08      0.13       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.24      0.27      0.25        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.23      0.14      0.17        49\n",
      "      uc0094       0.40      0.01      0.02       785\n",
      "      uc0096       0.27      0.43      0.34      2817\n",
      "      uc0097       0.27      0.09      0.13        92\n",
      "      uc0098       0.67      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.26      0.09      0.13        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.37      0.61      0.46       330\n",
      "      uc0108       0.67      0.58      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.15      0.02      0.04      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.06      0.00      0.01       284\n",
      "      uc0115       0.17      0.08      0.10        79\n",
      "      uc0116       0.16      0.16      0.16        31\n",
      "      uc0117       0.41      0.50      0.45        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.40      0.11      0.18       772\n",
      "      uc0125       0.32      0.31      0.32       115\n",
      "      uc0126       0.23      0.06      0.09       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.19      0.04      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.44      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.10      0.04      0.06        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.23      0.27      0.25       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.32      0.39      0.35        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.14      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.25      0.07      0.12        80\n",
      "      uc0150       0.16      0.01      0.01       541\n",
      "      uc0153       0.17      0.18      0.17       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.36      0.40       149\n",
      "      uc0158       0.38      0.19      0.26        26\n",
      "      uc0159       0.50      0.13      0.21        15\n",
      "      uc0161       0.20      0.14      0.17         7\n",
      "      uc0162       0.23      0.17      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.28      0.15      0.20       136\n",
      "      uc0167       0.12      0.04      0.06        25\n",
      "      uc0169       0.18      0.01      0.03       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.27      0.26      0.27        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.21      0.10      0.13      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.08      0.06      0.07        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.30      0.07      0.11        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.29      0.12      0.17        17\n",
      "      uc0193       0.00      0.00      0.00        54\n",
      "      uc0195       0.45      0.45      0.45        66\n",
      "      uc0197       0.31      0.11      0.16        38\n",
      "      uc0198       0.50      0.03      0.05       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.33      0.04      0.07        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.45      0.20      0.27       409\n",
      "      uc0211       0.21      0.06      0.09       298\n",
      "      uc0212       0.38      0.27      0.31       274\n",
      "      uc0215       0.35      0.07      0.12       647\n",
      "      uc0216       0.23      0.36      0.28       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.40      0.74      0.52       120\n",
      "      uc0220       0.41      0.51      0.45        89\n",
      "      uc0221       0.42      0.19      0.26        90\n",
      "      uc0222       0.31      0.39      0.34      1234\n",
      "      uc0223       0.00      0.00      0.00        20\n",
      "      uc0225       0.35      0.33      0.34        24\n",
      "      uc0226       0.46      0.76      0.57       235\n",
      "      uc0228       0.17      0.08      0.11        25\n",
      "      uc0229       0.36      0.34      0.35        94\n",
      "      uc0230       0.21      0.12      0.15        78\n",
      "      uc0232       0.46      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.11      0.02      0.04       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.50      0.06      0.11        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.28      0.17      0.21       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.10      0.02      0.03       120\n",
      "      uc1007       0.33      0.12      0.17        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.18      0.18        50\n",
      "      uc1010       0.39      0.54      0.46        70\n",
      "      uc1011       0.43      0.24      0.31        41\n",
      "      uc1012       0.27      0.13      0.17        31\n",
      "      uc1013       0.50      0.06      0.11        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.14      0.04      0.06        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.45      0.33      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.14      0.12      0.13         8\n",
      "      uc2001       0.25      0.17      0.20        24\n",
      "      uc2002       0.63      0.35      0.45        48\n",
      "      uc2005       0.62      0.72      0.67        18\n",
      "      uc2006       0.55      0.20      0.30        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.47      0.33      0.39        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.28      0.56      0.38        16\n",
      "      uc2012       0.47      0.38      0.42        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.09      0.07      0.08        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.20      0.24      0.22        80\n",
      "      uc2020       0.36      0.21      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.16      0.12      0.14       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.31      0.39        35\n",
      "      uc2028       0.76      0.84      0.79        37\n",
      "      uc2029       0.29      0.33      0.31       341\n",
      "      uc2030       0.47      0.37      0.41        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.50      0.18      0.26        45\n",
      "      uc2037       0.33      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.23      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.33      0.32      0.33        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.31      0.30      0.30        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.38      0.50      0.43        20\n",
      "      uc2049       0.48      0.44      0.46        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.33      0.05      0.09        20\n",
      "      uc2056       0.67      0.33      0.44        42\n",
      "      uc2059       0.21      0.55      0.30       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.59      0.27      0.37        37\n",
      "      uc2063       0.92      0.55      0.69        20\n",
      "      uc2064       0.26      0.08      0.12        61\n",
      "      uc2065       0.45      0.44      0.45        66\n",
      "      uc2066       0.28      0.24      0.26       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.25      0.14      0.18        22\n",
      "      uc2072       0.33      0.11      0.16        19\n",
      "      uc2073       0.48      0.44      0.46        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.67      0.08      0.14        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.44      0.49      0.46        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.62      0.57      0.59        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.23      0.38      0.29        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.20      0.04      0.07        23\n",
      "      uc2087       0.85      0.18      0.30       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.31      0.26      0.29        19\n",
      "      uc2092       1.00      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.25      0.10      0.15        29\n",
      "      uc2096       0.08      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_08 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 10/17: usuario_09\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1603 - loss: 4.1869 - val_accuracy: 0.2663 - val_loss: 3.3435 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2491 - loss: 3.4201 - val_accuracy: 0.2773 - val_loss: 3.2364 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2570 - loss: 3.3200 - val_accuracy: 0.2812 - val_loss: 3.2025 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2617 - loss: 3.2608 - val_accuracy: 0.2833 - val_loss: 3.1866 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2620 - loss: 3.2242 - val_accuracy: 0.2857 - val_loss: 3.1786 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2654 - loss: 3.2096 - val_accuracy: 0.2821 - val_loss: 3.1680 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2661 - loss: 3.1896 - val_accuracy: 0.2848 - val_loss: 3.1683 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2696 - loss: 3.1599 - val_accuracy: 0.2857 - val_loss: 3.1681 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2660 - loss: 3.1555 - val_accuracy: 0.2858 - val_loss: 3.1701 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2710 - loss: 3.1487 - val_accuracy: 0.2863 - val_loss: 3.1784 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1278/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.2706 - loss: 3.1426\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2699 - loss: 3.1465 - val_accuracy: 0.2862 - val_loss: 3.1800 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2767 - loss: 3.1034 - val_accuracy: 0.2896 - val_loss: 3.1612 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2757 - loss: 3.0933 - val_accuracy: 0.2865 - val_loss: 3.1669 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2784 - loss: 3.0774 - val_accuracy: 0.2878 - val_loss: 3.1700 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2765 - loss: 3.0734 - val_accuracy: 0.2883 - val_loss: 3.1692 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2773 - loss: 3.0793 - val_accuracy: 0.2883 - val_loss: 3.1739 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m 231/1732\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:47\u001b[0m 232ms/step - accuracy: 0.2774 - loss: 3.0299\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2792 - loss: 3.0591 - val_accuracy: 0.2904 - val_loss: 3.1729 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2808 - loss: 3.0540 - val_accuracy: 0.2896 - val_loss: 3.1713 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2786 - loss: 3.0499 - val_accuracy: 0.2904 - val_loss: 3.1731 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2856 - loss: 3.0247 - val_accuracy: 0.2880 - val_loss: 3.1735 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2861 - loss: 3.0218 - val_accuracy: 0.2914 - val_loss: 3.1727 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1678/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.2831 - loss: 3.0203\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2831 - loss: 3.0205 - val_accuracy: 0.2899 - val_loss: 3.1748 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2803 - loss: 3.0237 - val_accuracy: 0.2915 - val_loss: 3.1736 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2852 - loss: 3.0049 - val_accuracy: 0.2913 - val_loss: 3.1745 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2872 - loss: 3.0059 - val_accuracy: 0.2909 - val_loss: 3.1756 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2869 - loss: 2.9991 - val_accuracy: 0.2925 - val_loss: 3.1752 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m  41/1732\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3009 - loss: 2.8432\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27825us/step - accuracy: 0.2880 - loss: 2.9974 - val_accuracy: 0.2909 - val_loss: 3.1772 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2897 - loss: 2.9924 - val_accuracy: 0.2913 - val_loss: 3.1770 - learning_rate: 6.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9921 - val_accuracy: 0.2917 - val_loss: 3.1789 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2868 - loss: 2.9938 - val_accuracy: 0.2922 - val_loss: 3.1789 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2903 - loss: 2.9875 - val_accuracy: 0.2922 - val_loss: 3.1788 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1443/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.2898 - loss: 2.9848\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2895 - loss: 2.9864 - val_accuracy: 0.2925 - val_loss: 3.1797 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2876 - loss: 2.9965 - val_accuracy: 0.2928 - val_loss: 3.1794 - learning_rate: 3.1250e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2875 - loss: 2.9838 - val_accuracy: 0.2929 - val_loss: 3.1791 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2868 - loss: 2.9928 - val_accuracy: 0.2926 - val_loss: 3.1792 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27928us/step - accuracy: 0.2898 - loss: 2.9842 - val_accuracy: 0.2925 - val_loss: 3.1793 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m 294/1732\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:22\u001b[0m 183ms/step - accuracy: 0.2853 - loss: 2.9744\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2910 - loss: 2.9721 - val_accuracy: 0.2935 - val_loss: 3.1799 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2906 - loss: 2.9802 - val_accuracy: 0.2933 - val_loss: 3.1794 - learning_rate: 1.5625e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2886 - loss: 2.9938 - val_accuracy: 0.2933 - val_loss: 3.1798 - learning_rate: 1.5625e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2896 - loss: 2.9905 - val_accuracy: 0.2933 - val_loss: 3.1796 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2841 - loss: 2.9958 - val_accuracy: 0.2932 - val_loss: 3.1794 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1701/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2882 - loss: 2.9823\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2882 - loss: 2.9824 - val_accuracy: 0.2931 - val_loss: 3.1793 - learning_rate: 1.5625e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2929 - loss: 2.9836 - val_accuracy: 0.2927 - val_loss: 3.1796 - learning_rate: 7.8125e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2887 - loss: 2.9866 - val_accuracy: 0.2925 - val_loss: 3.1799 - learning_rate: 7.8125e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28041us/step - accuracy: 0.2910 - loss: 2.9715 - val_accuracy: 0.2927 - val_loss: 3.1799 - learning_rate: 7.8125e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 2.9768 - val_accuracy: 0.2930 - val_loss: 3.1798 - learning_rate: 7.8125e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m 750/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 73ms/step - accuracy: 0.2908 - loss: 2.9620\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2899 - loss: 2.9756 - val_accuracy: 0.2929 - val_loss: 3.1798 - learning_rate: 7.8125e-06\n",
      "Epoch 47: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step \n",
      "Acurácia: 28.52%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.07      0.02      0.03        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.19      0.08      0.11        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.44      0.17      0.24        24\n",
      "      uc0016       0.14      0.09      0.11      1016\n",
      "      uc0017       0.15      0.03      0.05       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.16      0.01      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.79      1.00      0.88        55\n",
      "      uc0024       0.30      0.35      0.32       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.07      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.26      0.23      0.24        26\n",
      "      uc0028       0.40      0.04      0.07       109\n",
      "      uc0029       0.46      0.66      0.54       205\n",
      "      uc0030       0.50      0.59      0.54       252\n",
      "      uc0031       0.38      0.18      0.25       574\n",
      "      uc0032       0.32      0.15      0.21        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.58      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.29      0.52      0.38        99\n",
      "      uc0040       0.40      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.47      0.44      0.46       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.36      0.29      0.32       741\n",
      "      uc0045       0.38      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.22      0.01      0.02       164\n",
      "      uc0049       0.43      0.32      0.37        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.10      0.02      0.04        46\n",
      "      uc0053       0.39      0.14      0.20       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.14      0.05      0.07       176\n",
      "      uc0059       0.23      0.18      0.20       116\n",
      "      uc0060       0.36      0.29      0.32       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.32      0.47      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.38      0.61      0.46      1357\n",
      "      uc0076       0.14      0.03      0.05       200\n",
      "      uc0077       0.42      0.23      0.29       681\n",
      "      uc0078       0.42      0.09      0.14        57\n",
      "      uc0079       0.36      0.03      0.06       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.33      0.32      0.33       196\n",
      "      uc0087       0.26      0.10      0.15       334\n",
      "      uc0089       0.50      0.02      0.04        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.30      0.23      0.26        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.39      0.18      0.25        49\n",
      "      uc0094       0.43      0.01      0.02       785\n",
      "      uc0096       0.28      0.43      0.34      2817\n",
      "      uc0097       0.32      0.12      0.17        92\n",
      "      uc0098       0.68      0.48      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.50      0.09      0.15        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.14      0.05      0.07        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.39      0.61      0.47       330\n",
      "      uc0108       0.71      0.58      0.64        52\n",
      "      uc0109       1.00      0.02      0.03        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.10      0.00      0.01      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.23      0.08      0.11        79\n",
      "      uc0116       0.16      0.16      0.16        31\n",
      "      uc0117       0.43      0.62      0.51        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.35      0.11      0.17       772\n",
      "      uc0125       0.31      0.29      0.30       115\n",
      "      uc0126       0.46      0.02      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       1.00      0.04      0.08        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.24      0.01      0.03       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.44      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.13      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.26      0.25      0.25       431\n",
      "      uc0140       0.50      0.06      0.10        36\n",
      "      uc0141       0.35      0.44      0.39        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.18      0.14      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.44      0.15      0.22        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.13      0.14      0.14       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.39      0.42       149\n",
      "      uc0158       0.42      0.19      0.26        26\n",
      "      uc0159       0.00      0.00      0.00        15\n",
      "      uc0161       0.50      0.14      0.22         7\n",
      "      uc0162       0.24      0.18      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.23      0.16      0.19       136\n",
      "      uc0167       0.17      0.04      0.06        25\n",
      "      uc0169       0.07      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.26      0.18      0.21        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.23      0.08      0.12      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.28      0.11      0.16        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.40      0.12      0.18        17\n",
      "      uc0193       0.25      0.09      0.14        54\n",
      "      uc0195       0.48      0.45      0.47        66\n",
      "      uc0197       0.33      0.13      0.19        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.43      0.22      0.29       409\n",
      "      uc0211       0.13      0.03      0.05       298\n",
      "      uc0212       0.36      0.26      0.30       274\n",
      "      uc0215       0.28      0.09      0.13       647\n",
      "      uc0216       0.24      0.36      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.41      0.69      0.52       120\n",
      "      uc0220       0.37      0.56      0.45        89\n",
      "      uc0221       0.40      0.18      0.25        90\n",
      "      uc0222       0.33      0.38      0.35      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.40      0.50      0.44        24\n",
      "      uc0226       0.44      0.76      0.56       235\n",
      "      uc0228       0.20      0.12      0.15        25\n",
      "      uc0229       0.38      0.34      0.36        94\n",
      "      uc0230       0.23      0.09      0.13        78\n",
      "      uc0232       0.47      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.16      0.02      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.62      0.56       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.67      0.07      0.13        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.27      0.19      0.22       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.20      0.02      0.03       120\n",
      "      uc1007       0.33      0.06      0.10        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.14      0.02      0.04        50\n",
      "      uc1010       0.38      0.53      0.44        70\n",
      "      uc1011       0.42      0.24      0.31        41\n",
      "      uc1012       0.20      0.03      0.06        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.20      0.03      0.05        39\n",
      "      uc1015       0.17      0.27      0.21        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.44      0.33      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.65      0.35      0.46        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.52      0.20      0.29        54\n",
      "      uc2007       1.00      0.06      0.11        17\n",
      "      uc2008       0.30      0.45      0.36        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.41      0.44      0.42        16\n",
      "      uc2012       0.56      0.42      0.48        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.19      0.17      0.18        80\n",
      "      uc2020       0.61      0.19      0.29       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.15      0.07      0.10       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.53      0.26      0.35        35\n",
      "      uc2028       0.77      0.81      0.79        37\n",
      "      uc2029       0.28      0.41      0.33       341\n",
      "      uc2030       0.46      0.35      0.40        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.55      0.13      0.21        45\n",
      "      uc2037       0.17      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.15      0.05      0.07        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.38      0.41      0.39        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.33      0.30      0.31        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.43      0.50      0.47        20\n",
      "      uc2049       0.52      0.42      0.46        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.50      0.31      0.38        42\n",
      "      uc2059       0.22      0.60      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.40      0.27      0.32        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.37      0.11      0.17        61\n",
      "      uc2065       0.31      0.47      0.37        66\n",
      "      uc2066       0.37      0.24      0.29       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.12      0.32      0.18        22\n",
      "      uc2072       0.50      0.11      0.17        19\n",
      "      uc2073       0.62      0.40      0.49        25\n",
      "      uc2074       0.12      0.12      0.12         8\n",
      "      uc2075       0.40      0.08      0.13        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.45      0.49      0.47        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.62      0.36      0.45        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.21      0.27      0.24        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       0.96      0.20      0.33       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.14      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.28      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_09 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 11/17: usuario_10\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3ms/step - accuracy: 0.1554 - loss: 4.2158 - val_accuracy: 0.2709 - val_loss: 3.3643 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28139us/step - accuracy: 0.2443 - loss: 3.4417 - val_accuracy: 0.2826 - val_loss: 3.2312 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2623 - loss: 3.3085 - val_accuracy: 0.2793 - val_loss: 3.1966 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2554 - loss: 3.2807 - val_accuracy: 0.2826 - val_loss: 3.1788 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2673 - loss: 3.2260 - val_accuracy: 0.2843 - val_loss: 3.1732 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2660 - loss: 3.2086 - val_accuracy: 0.2855 - val_loss: 3.1660 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2647 - loss: 3.1869 - val_accuracy: 0.2855 - val_loss: 3.1698 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2679 - loss: 3.1764 - val_accuracy: 0.2831 - val_loss: 3.1655 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2664 - loss: 3.1733 - val_accuracy: 0.2883 - val_loss: 3.1653 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m -28048us/step - accuracy: 0.2719 - loss: 3.1568 - val_accuracy: 0.2837 - val_loss: 3.1811 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2678 - loss: 3.1418 - val_accuracy: 0.2863 - val_loss: 3.1765 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2688 - loss: 3.1434 - val_accuracy: 0.2870 - val_loss: 3.1732 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2693 - loss: 3.1296 - val_accuracy: 0.2869 - val_loss: 3.1872 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1347/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - accuracy: 0.2682 - loss: 3.1242\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2681 - loss: 3.1277 - val_accuracy: 0.2879 - val_loss: 3.1886 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2750 - loss: 3.0945 - val_accuracy: 0.2891 - val_loss: 3.1666 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2765 - loss: 3.0763 - val_accuracy: 0.2908 - val_loss: 3.1624 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2748 - loss: 3.0739 - val_accuracy: 0.2905 - val_loss: 3.1645 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2777 - loss: 3.0646 - val_accuracy: 0.2924 - val_loss: 3.1667 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m -27885us/step - accuracy: 0.2795 - loss: 3.0504 - val_accuracy: 0.2909 - val_loss: 3.1684 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2840 - loss: 3.0421 - val_accuracy: 0.2902 - val_loss: 3.1666 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m 801/1732\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 68ms/step - accuracy: 0.2804 - loss: 3.0559\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2800 - loss: 3.0569 - val_accuracy: 0.2916 - val_loss: 3.1663 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2826 - loss: 3.0320 - val_accuracy: 0.2922 - val_loss: 3.1646 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2836 - loss: 3.0207 - val_accuracy: 0.2914 - val_loss: 3.1650 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2830 - loss: 3.0214 - val_accuracy: 0.2924 - val_loss: 3.1723 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2860 - loss: 3.0039 - val_accuracy: 0.2894 - val_loss: 3.1719 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1714/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2821 - loss: 3.0152\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2821 - loss: 3.0152 - val_accuracy: 0.2922 - val_loss: 3.1734 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2850 - loss: 3.0086 - val_accuracy: 0.2927 - val_loss: 3.1725 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2898 - loss: 2.9931 - val_accuracy: 0.2938 - val_loss: 3.1723 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2849 - loss: 2.9849 - val_accuracy: 0.2927 - val_loss: 3.1735 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2904 - loss: 2.9931 - val_accuracy: 0.2935 - val_loss: 3.1726 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1139/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 49ms/step - accuracy: 0.2897 - loss: 2.9971\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2886 - loss: 2.9967 - val_accuracy: 0.2938 - val_loss: 3.1729 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2837 - loss: 2.9949 - val_accuracy: 0.2942 - val_loss: 3.1736 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2903 - loss: 2.9668 - val_accuracy: 0.2933 - val_loss: 3.1742 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2885 - loss: 2.9876 - val_accuracy: 0.2927 - val_loss: 3.1741 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2912 - loss: 2.9640 - val_accuracy: 0.2933 - val_loss: 3.1744 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m  43/1732\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2852 - loss: 2.9167\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28173us/step - accuracy: 0.2893 - loss: 2.9796 - val_accuracy: 0.2938 - val_loss: 3.1741 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2859 - loss: 2.9992 - val_accuracy: 0.2937 - val_loss: 3.1740 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2867 - loss: 2.9769 - val_accuracy: 0.2933 - val_loss: 3.1743 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2904 - loss: 2.9764 - val_accuracy: 0.2937 - val_loss: 3.1749 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2909 - loss: 2.9671 - val_accuracy: 0.2933 - val_loss: 3.1742 - learning_rate: 3.1250e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1696/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2910 - loss: 2.9751\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2910 - loss: 2.9752 - val_accuracy: 0.2930 - val_loss: 3.1741 - learning_rate: 3.1250e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2928 - loss: 2.9616 - val_accuracy: 0.2927 - val_loss: 3.1740 - learning_rate: 1.5625e-05\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.70%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.09      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.29      0.08      0.12        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.50      0.12      0.20        24\n",
      "      uc0016       0.16      0.08      0.10      1016\n",
      "      uc0017       0.15      0.04      0.06       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.11      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.82      1.00      0.90        55\n",
      "      uc0024       0.31      0.35      0.33       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.09      0.07      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.20      0.04      0.06        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.48      0.68      0.57       205\n",
      "      uc0030       0.51      0.59      0.54       252\n",
      "      uc0031       0.37      0.21      0.26       574\n",
      "      uc0032       0.32      0.15      0.21        46\n",
      "      uc0033       0.17      0.08      0.11        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.27      0.52      0.35        99\n",
      "      uc0040       0.42      0.40      0.41       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.53      0.46      0.49       156\n",
      "      uc0043       0.22      0.71      0.34      5240\n",
      "      uc0044       0.36      0.29      0.33       741\n",
      "      uc0045       0.38      0.11      0.17       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.47      0.26      0.33        31\n",
      "      uc0050       0.50      0.12      0.20         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.37      0.18      0.24       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.15      0.07      0.10       176\n",
      "      uc0059       0.17      0.09      0.12       116\n",
      "      uc0060       0.43      0.23      0.30       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.13      0.02      0.03       102\n",
      "      uc0069       0.30      0.47      0.36      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.61      0.46      1357\n",
      "      uc0076       0.06      0.01      0.01       200\n",
      "      uc0077       0.46      0.22      0.29       681\n",
      "      uc0078       0.44      0.14      0.21        57\n",
      "      uc0079       0.38      0.04      0.07       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.25      0.06      0.10        32\n",
      "      uc0086       0.34      0.32      0.33       196\n",
      "      uc0087       0.24      0.10      0.15       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.21      0.19      0.20        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.30      0.18      0.23        49\n",
      "      uc0094       0.24      0.01      0.03       785\n",
      "      uc0096       0.27      0.46      0.34      2817\n",
      "      uc0097       0.31      0.12      0.17        92\n",
      "      uc0098       0.68      0.48      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.38      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.14      0.05      0.07        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.38      0.60      0.47       330\n",
      "      uc0108       0.63      0.56      0.59        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.08      0.01      0.01      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.04      0.00      0.01       284\n",
      "      uc0115       0.28      0.06      0.10        79\n",
      "      uc0116       0.16      0.16      0.16        31\n",
      "      uc0117       0.37      0.62      0.46        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.38      0.13      0.19       772\n",
      "      uc0125       0.29      0.30      0.30       115\n",
      "      uc0126       0.22      0.04      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.20      0.05      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.44      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.10      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.30      0.23      0.26       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.35      0.39      0.37        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.12      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.53      0.12      0.20        80\n",
      "      uc0150       0.13      0.01      0.01       541\n",
      "      uc0153       0.18      0.20      0.19       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.36      0.40       149\n",
      "      uc0158       0.39      0.27      0.32        26\n",
      "      uc0159       0.00      0.00      0.00        15\n",
      "      uc0161       0.50      0.14      0.22         7\n",
      "      uc0162       0.23      0.17      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.28      0.15      0.20       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.12      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.30      0.18      0.22        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.25      0.09      0.13      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.05      0.02      0.03        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.40      0.04      0.08        46\n",
      "      uc0190       0.22      0.03      0.06        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.48      0.42      0.45        66\n",
      "      uc0197       0.32      0.16      0.21        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.49      0.20      0.29       409\n",
      "      uc0211       0.20      0.06      0.09       298\n",
      "      uc0212       0.36      0.26      0.30       274\n",
      "      uc0215       0.32      0.07      0.12       647\n",
      "      uc0216       0.27      0.34      0.30       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.38      0.63      0.47       120\n",
      "      uc0220       0.37      0.60      0.45        89\n",
      "      uc0221       0.42      0.18      0.25        90\n",
      "      uc0222       0.31      0.37      0.34      1234\n",
      "      uc0223       0.00      0.00      0.00        20\n",
      "      uc0225       0.38      0.25      0.30        24\n",
      "      uc0226       0.43      0.77      0.55       235\n",
      "      uc0228       0.25      0.08      0.12        25\n",
      "      uc0229       0.39      0.34      0.36        94\n",
      "      uc0230       0.19      0.14      0.16        78\n",
      "      uc0232       0.46      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.00      0.00      0.00       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.61      0.56       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.26      0.18      0.22       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.10      0.06      0.07        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.05      0.02      0.03        50\n",
      "      uc1010       0.42      0.51      0.46        70\n",
      "      uc1011       0.43      0.22      0.29        41\n",
      "      uc1012       0.25      0.03      0.06        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.12      0.12      0.12        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.46      0.32      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.20      0.12      0.15        24\n",
      "      uc2002       0.59      0.35      0.44        48\n",
      "      uc2005       0.72      0.72      0.72        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.38      0.35      0.36        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.64      0.44      0.52        16\n",
      "      uc2012       0.55      0.46      0.50        24\n",
      "      uc2014       0.31      0.10      0.15        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.27      0.26      0.27        80\n",
      "      uc2020       0.35      0.22      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.15      0.03      0.05       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.58      0.40      0.47        35\n",
      "      uc2028       0.81      0.81      0.81        37\n",
      "      uc2029       0.30      0.45      0.36       341\n",
      "      uc2030       0.71      0.22      0.33        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.50      0.01      0.02       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.55      0.13      0.21        45\n",
      "      uc2037       0.45      0.15      0.22        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.20      0.05      0.08        61\n",
      "      uc2040       0.12      0.04      0.06        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.75      0.27      0.40        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.33      0.30      0.32        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.37      0.50      0.43        20\n",
      "      uc2049       0.62      0.42      0.50        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.11      0.05      0.07        20\n",
      "      uc2056       0.38      0.33      0.35        42\n",
      "      uc2059       0.21      0.57      0.31       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.31      0.38      0.34        37\n",
      "      uc2063       0.69      0.55      0.61        20\n",
      "      uc2064       0.21      0.07      0.10        61\n",
      "      uc2065       0.27      0.50      0.35        66\n",
      "      uc2066       0.25      0.25      0.25       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.27      0.14      0.18        22\n",
      "      uc2072       0.33      0.11      0.16        19\n",
      "      uc2073       0.69      0.36      0.47        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.83      0.20      0.32        25\n",
      "      uc2076       0.17      0.04      0.06        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.47      0.39      0.43        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.39      0.50      0.44        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.23      0.23      0.23        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.19      0.32       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.38      0.33      0.35         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.10      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.33      0.10      0.16        29\n",
      "      uc2096       0.08      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.18      0.13      0.13     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_10 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 12/17: usuario_11\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1589 - loss: 4.1906 - val_accuracy: 0.2668 - val_loss: 3.3506 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2451 - loss: 3.4231 - val_accuracy: 0.2784 - val_loss: 3.2443 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2537 - loss: 3.3201 - val_accuracy: 0.2825 - val_loss: 3.2072 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2628 - loss: 3.2520 - val_accuracy: 0.2840 - val_loss: 3.1858 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2604 - loss: 3.2357 - val_accuracy: 0.2852 - val_loss: 3.1646 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2667 - loss: 3.2166 - val_accuracy: 0.2828 - val_loss: 3.1705 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2690 - loss: 3.1908 - val_accuracy: 0.2842 - val_loss: 3.1637 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2654 - loss: 3.1731 - val_accuracy: 0.2866 - val_loss: 3.1688 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2645 - loss: 3.1632 - val_accuracy: 0.2879 - val_loss: 3.1725 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28288us/step - accuracy: 0.2684 - loss: 3.1556 - val_accuracy: 0.2847 - val_loss: 3.1776 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2656 - loss: 3.1598 - val_accuracy: 0.2876 - val_loss: 3.1700 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m 828/1732\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.2703 - loss: 3.1331 \n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2688 - loss: 3.1425 - val_accuracy: 0.2872 - val_loss: 3.1727 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2742 - loss: 3.1119 - val_accuracy: 0.2891 - val_loss: 3.1551 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2775 - loss: 3.0769 - val_accuracy: 0.2911 - val_loss: 3.1514 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2788 - loss: 3.0784 - val_accuracy: 0.2895 - val_loss: 3.1517 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2795 - loss: 3.0602 - val_accuracy: 0.2923 - val_loss: 3.1549 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28250us/step - accuracy: 0.2785 - loss: 3.0634 - val_accuracy: 0.2883 - val_loss: 3.1555 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2810 - loss: 3.0520 - val_accuracy: 0.2927 - val_loss: 3.1581 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m 747/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 73ms/step - accuracy: 0.2828 - loss: 3.0518\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2813 - loss: 3.0563 - val_accuracy: 0.2915 - val_loss: 3.1605 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2817 - loss: 3.0254 - val_accuracy: 0.2925 - val_loss: 3.1535 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2795 - loss: 3.0200 - val_accuracy: 0.2921 - val_loss: 3.1536 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2783 - loss: 3.0404 - val_accuracy: 0.2920 - val_loss: 3.1554 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2855 - loss: 3.0166 - val_accuracy: 0.2922 - val_loss: 3.1537 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m   1/1732\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 24ms/step - accuracy: 0.3438 - loss: 2.7280\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28444us/step - accuracy: 0.2852 - loss: 3.0141 - val_accuracy: 0.2937 - val_loss: 3.1532 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2871 - loss: 3.0035 - val_accuracy: 0.2912 - val_loss: 3.1523 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2837 - loss: 3.0108 - val_accuracy: 0.2926 - val_loss: 3.1559 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2897 - loss: 2.9958 - val_accuracy: 0.2928 - val_loss: 3.1570 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2882 - loss: 2.9874 - val_accuracy: 0.2919 - val_loss: 3.1570 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1717/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2870 - loss: 2.9921\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2870 - loss: 2.9922 - val_accuracy: 0.2920 - val_loss: 3.1593 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2816 - loss: 3.0104 - val_accuracy: 0.2935 - val_loss: 3.1601 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2873 - loss: 2.9891 - val_accuracy: 0.2930 - val_loss: 3.1588 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2903 - loss: 2.9783 - val_accuracy: 0.2938 - val_loss: 3.1594 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2881 - loss: 2.9882 - val_accuracy: 0.2931 - val_loss: 3.1595 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m 937/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - accuracy: 0.2887 - loss: 2.9778\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2885 - loss: 2.9809 - val_accuracy: 0.2937 - val_loss: 3.1581 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2867 - loss: 2.9779 - val_accuracy: 0.2933 - val_loss: 3.1592 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 2.9829 - val_accuracy: 0.2939 - val_loss: 3.1587 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2872 - loss: 2.9776 - val_accuracy: 0.2941 - val_loss: 3.1590 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2900 - loss: 2.9711 - val_accuracy: 0.2946 - val_loss: 3.1597 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m 141/1732\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:04\u001b[0m 380ms/step - accuracy: 0.2895 - loss: 3.0457\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2902 - loss: 2.9780 - val_accuracy: 0.2937 - val_loss: 3.1591 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2881 - loss: 2.9836 - val_accuracy: 0.2937 - val_loss: 3.1596 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2880 - loss: 2.9741 - val_accuracy: 0.2936 - val_loss: 3.1591 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2865 - loss: 2.9877 - val_accuracy: 0.2938 - val_loss: 3.1589 - learning_rate: 1.5625e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9724 - val_accuracy: 0.2941 - val_loss: 3.1592 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1721/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2867 - loss: 2.9766\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2867 - loss: 2.9766 - val_accuracy: 0.2938 - val_loss: 3.1594 - learning_rate: 1.5625e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2917 - loss: 2.9780 - val_accuracy: 0.2938 - val_loss: 3.1594 - learning_rate: 7.8125e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27943us/step - accuracy: 0.2865 - loss: 2.9832 - val_accuracy: 0.2936 - val_loss: 3.1594 - learning_rate: 7.8125e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2879 - loss: 2.9742 - val_accuracy: 0.2938 - val_loss: 3.1598 - learning_rate: 7.8125e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2918 - loss: 2.9697 - val_accuracy: 0.2938 - val_loss: 3.1598 - learning_rate: 7.8125e-06\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.63%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.14      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.28      0.10      0.14        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.44      0.17      0.24        24\n",
      "      uc0016       0.16      0.07      0.10      1016\n",
      "      uc0017       0.14      0.03      0.05       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.00      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.82      1.00      0.90        55\n",
      "      uc0024       0.31      0.34      0.33       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.11      0.09      0.10        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.32      0.27      0.29        26\n",
      "      uc0028       0.27      0.03      0.05       109\n",
      "      uc0029       0.45      0.66      0.53       205\n",
      "      uc0030       0.48      0.61      0.54       252\n",
      "      uc0031       0.38      0.20      0.26       574\n",
      "      uc0032       0.33      0.15      0.21        46\n",
      "      uc0033       0.19      0.08      0.11        63\n",
      "      uc0034       0.58      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.31      0.44      0.36        99\n",
      "      uc0040       0.41      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.48      0.48      0.48       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.38      0.29      0.33       741\n",
      "      uc0045       0.41      0.11      0.17       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.40      0.19      0.26        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.12      0.02      0.04        46\n",
      "      uc0053       0.40      0.15      0.21       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.10      0.05      0.06       176\n",
      "      uc0059       0.21      0.15      0.17       116\n",
      "      uc0060       0.46      0.27      0.34       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.48      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.59      0.45      1357\n",
      "      uc0076       0.06      0.01      0.02       200\n",
      "      uc0077       0.41      0.24      0.30       681\n",
      "      uc0078       0.32      0.11      0.16        57\n",
      "      uc0079       0.14      0.01      0.01       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.34      0.36      0.35       196\n",
      "      uc0087       0.25      0.10      0.14       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.24      0.14      0.18        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.32      0.18      0.23        49\n",
      "      uc0094       0.24      0.01      0.01       785\n",
      "      uc0096       0.28      0.44      0.34      2817\n",
      "      uc0097       0.30      0.13      0.18        92\n",
      "      uc0098       0.68      0.48      0.56       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.43      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.37      0.60      0.46       330\n",
      "      uc0108       0.63      0.56      0.59        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.14      0.03      0.05      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.28      0.06      0.10        79\n",
      "      uc0116       0.17      0.13      0.15        31\n",
      "      uc0117       0.48      0.50      0.49        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.39      0.11      0.17       772\n",
      "      uc0125       0.31      0.30      0.30       115\n",
      "      uc0126       0.36      0.01      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.19      0.03      0.06       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.46      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.15      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.25      0.23      0.24       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.24      0.39      0.30        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.14      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.50      0.14      0.22        80\n",
      "      uc0150       0.15      0.01      0.01       541\n",
      "      uc0153       0.18      0.19      0.18       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.45      0.37      0.41       149\n",
      "      uc0158       0.42      0.19      0.26        26\n",
      "      uc0159       1.00      0.07      0.12        15\n",
      "      uc0161       0.20      0.14      0.17         7\n",
      "      uc0162       0.24      0.18      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.27      0.15      0.20       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.21      0.04      0.06       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.26      0.24      0.25        34\n",
      "      uc0173       0.25      0.12      0.17        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.24      0.11      0.15      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.12      0.12      0.12        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.37      0.45      0.41        66\n",
      "      uc0197       0.36      0.13      0.19        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.46      0.21      0.29       409\n",
      "      uc0211       0.26      0.11      0.15       298\n",
      "      uc0212       0.38      0.28      0.32       274\n",
      "      uc0215       0.32      0.07      0.12       647\n",
      "      uc0216       0.26      0.35      0.30       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.40      0.74      0.52       120\n",
      "      uc0220       0.44      0.54      0.49        89\n",
      "      uc0221       0.41      0.19      0.26        90\n",
      "      uc0222       0.35      0.37      0.36      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.36      0.17      0.23        24\n",
      "      uc0226       0.47      0.74      0.57       235\n",
      "      uc0228       0.12      0.08      0.10        25\n",
      "      uc0229       0.31      0.29      0.30        94\n",
      "      uc0230       0.27      0.14      0.18        78\n",
      "      uc0232       0.47      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.12      0.01      0.02       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.54      0.63      0.58       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.18      0.07      0.10        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.33      0.06      0.10        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.26      0.15      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.14      0.02      0.03       120\n",
      "      uc1007       0.00      0.00      0.00        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.18      0.18        50\n",
      "      uc1010       0.41      0.50      0.45        70\n",
      "      uc1011       0.40      0.24      0.30        41\n",
      "      uc1012       0.31      0.26      0.28        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.22      0.15      0.18        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.32      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.27      0.17      0.21        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.65      0.72      0.68        18\n",
      "      uc2006       0.71      0.22      0.34        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.39      0.43      0.41        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.54      0.44      0.48        16\n",
      "      uc2012       0.55      0.46      0.50        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.38      0.10      0.16        80\n",
      "      uc2020       0.44      0.19      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.14      0.05      0.07       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.40      0.23      0.29        35\n",
      "      uc2028       0.81      0.81      0.81        37\n",
      "      uc2029       0.30      0.37      0.33       341\n",
      "      uc2030       0.47      0.37      0.41        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.70      0.16      0.25        45\n",
      "      uc2037       0.31      0.12      0.17        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.23      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.29      0.32      0.30        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.39      0.28      0.33        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.34      0.55      0.42        20\n",
      "      uc2049       0.48      0.42      0.45        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.50      0.30      0.38        20\n",
      "      uc2056       0.81      0.31      0.45        42\n",
      "      uc2059       0.21      0.63      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.33      0.38      0.35        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.25      0.10      0.14        61\n",
      "      uc2065       0.39      0.44      0.41        66\n",
      "      uc2066       0.19      0.23      0.21       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.00      0.00      0.00        22\n",
      "      uc2072       0.67      0.11      0.18        19\n",
      "      uc2073       0.53      0.36      0.43        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.80      0.16      0.27        25\n",
      "      uc2076       0.06      0.04      0.04        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.49      0.45      0.47        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.46      0.43      0.44        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.27      0.27      0.27        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.18      0.31       120\n",
      "      uc2088       1.00      0.50      0.67         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.00      0.00      0.00       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.18      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_11 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 13/17: usuario_12\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1516 - loss: 4.2303 - val_accuracy: 0.2746 - val_loss: 3.3340 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2483 - loss: 3.4208 - val_accuracy: 0.2776 - val_loss: 3.2543 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2574 - loss: 3.3157 - val_accuracy: 0.2848 - val_loss: 3.1963 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2610 - loss: 3.2543 - val_accuracy: 0.2837 - val_loss: 3.1764 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2635 - loss: 3.2365 - val_accuracy: 0.2875 - val_loss: 3.1693 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2629 - loss: 3.2083 - val_accuracy: 0.2866 - val_loss: 3.1645 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2637 - loss: 3.1949 - val_accuracy: 0.2866 - val_loss: 3.1587 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2713 - loss: 3.1669 - val_accuracy: 0.2875 - val_loss: 3.1682 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2693 - loss: 3.1636 - val_accuracy: 0.2876 - val_loss: 3.1676 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -27865us/step - accuracy: 0.2654 - loss: 3.1598 - val_accuracy: 0.2902 - val_loss: 3.1623 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2689 - loss: 3.1537 - val_accuracy: 0.2867 - val_loss: 3.1693 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m 773/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 70ms/step - accuracy: 0.2764 - loss: 3.1075\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2733 - loss: 3.1266 - val_accuracy: 0.2835 - val_loss: 3.1716 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2734 - loss: 3.0992 - val_accuracy: 0.2890 - val_loss: 3.1536 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2730 - loss: 3.0994 - val_accuracy: 0.2899 - val_loss: 3.1519 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2768 - loss: 3.0793 - val_accuracy: 0.2910 - val_loss: 3.1562 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2800 - loss: 3.0633 - val_accuracy: 0.2925 - val_loss: 3.1545 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2756 - loss: 3.0700 - val_accuracy: 0.2924 - val_loss: 3.1580 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28346us/step - accuracy: 0.2787 - loss: 3.0675 - val_accuracy: 0.2925 - val_loss: 3.1650 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m 627/1732\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 86ms/step - accuracy: 0.2856 - loss: 3.0201\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2815 - loss: 3.0438 - val_accuracy: 0.2884 - val_loss: 3.1607 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2778 - loss: 3.0422 - val_accuracy: 0.2915 - val_loss: 3.1590 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2803 - loss: 3.0345 - val_accuracy: 0.2939 - val_loss: 3.1576 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2846 - loss: 3.0188 - val_accuracy: 0.2925 - val_loss: 3.1615 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2853 - loss: 3.0156 - val_accuracy: 0.2925 - val_loss: 3.1631 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1716/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2849 - loss: 3.0099\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2849 - loss: 3.0100 - val_accuracy: 0.2951 - val_loss: 3.1663 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2855 - loss: 3.0053 - val_accuracy: 0.2939 - val_loss: 3.1644 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2895 - loss: 2.9912 - val_accuracy: 0.2940 - val_loss: 3.1645 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 3.0009 - val_accuracy: 0.2943 - val_loss: 3.1655 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2853 - loss: 2.9950 - val_accuracy: 0.2947 - val_loss: 3.1639 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1279/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.2906 - loss: 2.9875\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2895 - loss: 2.9903 - val_accuracy: 0.2950 - val_loss: 3.1662 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2905 - loss: 2.9786 - val_accuracy: 0.2954 - val_loss: 3.1665 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2856 - loss: 2.9861 - val_accuracy: 0.2949 - val_loss: 3.1659 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2895 - loss: 2.9798 - val_accuracy: 0.2944 - val_loss: 3.1649 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28052us/step - accuracy: 0.2865 - loss: 2.9882 - val_accuracy: 0.2944 - val_loss: 3.1661 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m 358/1732\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 150ms/step - accuracy: 0.2871 - loss: 2.9708\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2891 - loss: 2.9812 - val_accuracy: 0.2947 - val_loss: 3.1669 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2884 - loss: 2.9861 - val_accuracy: 0.2945 - val_loss: 3.1670 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2865 - loss: 2.9794 - val_accuracy: 0.2950 - val_loss: 3.1668 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2862 - loss: 2.9960 - val_accuracy: 0.2954 - val_loss: 3.1665 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2911 - loss: 2.9719 - val_accuracy: 0.2948 - val_loss: 3.1664 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1718/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2901 - loss: 2.9859\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2901 - loss: 2.9859 - val_accuracy: 0.2949 - val_loss: 3.1671 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2871 - loss: 2.9838 - val_accuracy: 0.2952 - val_loss: 3.1671 - learning_rate: 1.5625e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.58%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.08      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.16      0.08      0.11        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.40      0.17      0.24        24\n",
      "      uc0016       0.15      0.08      0.11      1016\n",
      "      uc0017       0.11      0.01      0.02       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.15      0.00      0.01       812\n",
      "      uc0020       0.14      0.12      0.13         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.81      0.98      0.89        55\n",
      "      uc0024       0.34      0.32      0.33       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.22      0.03      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.29      0.27      0.28        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.46      0.67      0.55       205\n",
      "      uc0030       0.61      0.50      0.55       252\n",
      "      uc0031       0.33      0.22      0.26       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.18      0.08      0.11        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.27      0.48      0.35        99\n",
      "      uc0040       0.42      0.38      0.40       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.50      0.47      0.49       156\n",
      "      uc0043       0.22      0.72      0.33      5240\n",
      "      uc0044       0.39      0.28      0.33       741\n",
      "      uc0045       0.40      0.10      0.16       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.46      0.35      0.40        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.00      0.00      0.00        46\n",
      "      uc0053       0.29      0.15      0.20       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.16      0.04      0.06       176\n",
      "      uc0059       0.21      0.20      0.20       116\n",
      "      uc0060       0.48      0.29      0.36       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.46      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.61      0.46      1357\n",
      "      uc0076       0.09      0.01      0.01       200\n",
      "      uc0077       0.39      0.26      0.31       681\n",
      "      uc0078       0.36      0.09      0.14        57\n",
      "      uc0079       0.23      0.05      0.09       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.33      0.34      0.33       196\n",
      "      uc0087       0.23      0.08      0.12       334\n",
      "      uc0089       1.00      0.02      0.05        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.42      0.11      0.18        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.30      0.18      0.23        49\n",
      "      uc0094       0.22      0.03      0.05       785\n",
      "      uc0096       0.28      0.45      0.34      2817\n",
      "      uc0097       0.30      0.10      0.15        92\n",
      "      uc0098       0.67      0.47      0.55       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.60      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.20      0.05      0.08        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.37      0.60      0.46       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.08      0.00      0.01      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.21      0.09      0.12        79\n",
      "      uc0116       0.23      0.35      0.28        31\n",
      "      uc0117       0.47      0.62      0.53        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.41      0.10      0.17       772\n",
      "      uc0125       0.27      0.25      0.26       115\n",
      "      uc0126       0.17      0.05      0.08       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.18      0.04      0.06       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.12      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.29      0.19      0.23       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.00      0.00      0.00        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.13      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.42      0.06      0.11        80\n",
      "      uc0150       0.21      0.01      0.02       541\n",
      "      uc0153       0.17      0.17      0.17       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.44      0.37      0.40       149\n",
      "      uc0158       0.38      0.23      0.29        26\n",
      "      uc0159       1.00      0.13      0.24        15\n",
      "      uc0161       0.25      0.14      0.18         7\n",
      "      uc0162       0.24      0.18      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.27      0.18      0.21       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.14      0.01      0.01       142\n",
      "      uc0171       0.14      0.03      0.05        35\n",
      "      uc0172       0.32      0.18      0.23        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.20      0.06      0.09      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.30      0.06      0.09        54\n",
      "      uc0195       0.47      0.42      0.45        66\n",
      "      uc0197       0.22      0.13      0.16        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.51      0.20      0.28       409\n",
      "      uc0211       0.14      0.04      0.07       298\n",
      "      uc0212       0.37      0.26      0.30       274\n",
      "      uc0215       0.26      0.09      0.13       647\n",
      "      uc0216       0.22      0.36      0.28       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.38      0.74      0.51       120\n",
      "      uc0220       0.37      0.53      0.43        89\n",
      "      uc0221       0.41      0.19      0.26        90\n",
      "      uc0222       0.33      0.38      0.35      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.35      0.46      0.40        24\n",
      "      uc0226       0.46      0.75      0.57       235\n",
      "      uc0228       0.08      0.04      0.05        25\n",
      "      uc0229       0.38      0.35      0.36        94\n",
      "      uc0230       0.24      0.14      0.18        78\n",
      "      uc0232       0.46      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.20      0.03      0.05       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.25      0.05      0.09        19\n",
      "      uc0242       0.67      0.07      0.13        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.27      0.14      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.10      0.03      0.04       120\n",
      "      uc1007       0.21      0.18      0.19        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.50      0.02      0.04        50\n",
      "      uc1010       0.40      0.50      0.44        70\n",
      "      uc1011       0.42      0.24      0.31        41\n",
      "      uc1012       0.20      0.03      0.06        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.21      0.19      0.20        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.44      0.32      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.20      0.12      0.15        24\n",
      "      uc2002       0.74      0.35      0.48        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.32      0.35      0.34        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.64      0.44      0.52        16\n",
      "      uc2012       0.44      0.50      0.47        24\n",
      "      uc2014       0.29      0.10      0.15        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.20      0.14      0.17        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.18      0.25      0.21        80\n",
      "      uc2020       0.52      0.19      0.28       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.17      0.12      0.14       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.59      0.37      0.46        35\n",
      "      uc2028       0.79      0.84      0.82        37\n",
      "      uc2029       0.37      0.32      0.34       341\n",
      "      uc2030       0.67      0.22      0.33        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.60      0.13      0.22        45\n",
      "      uc2037       0.12      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.18      0.07      0.10        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.62      0.23      0.33        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.27      0.30      0.28        50\n",
      "      uc2046       1.00      0.04      0.07        27\n",
      "      uc2048       0.31      0.45      0.37        20\n",
      "      uc2049       0.65      0.42      0.51        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.29      0.12      0.17        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.71      0.29      0.41        42\n",
      "      uc2059       0.21      0.58      0.31       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.33      0.32      0.33        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.26      0.11      0.16        61\n",
      "      uc2065       0.36      0.45      0.40        66\n",
      "      uc2066       0.21      0.23      0.22       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.40      0.09      0.15        22\n",
      "      uc2072       0.25      0.11      0.15        19\n",
      "      uc2073       0.67      0.40      0.50        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.80      0.16      0.27        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.12      0.05      0.07        20\n",
      "      uc2078       0.53      0.39      0.45        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.38      0.36      0.37        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.23      0.35      0.28        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.25      0.04      0.07        23\n",
      "      uc2087       0.96      0.20      0.33       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.06      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.31      0.14      0.19        29\n",
      "      uc2096       0.13      0.07      0.09        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.20      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_12 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 14/17: usuario_13\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.1551 - loss: 4.2109 - val_accuracy: 0.2696 - val_loss: 3.3630 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2461 - loss: 3.4295 - val_accuracy: 0.2760 - val_loss: 3.2346 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2532 - loss: 3.3151 - val_accuracy: 0.2813 - val_loss: 3.2030 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2571 - loss: 3.2749 - val_accuracy: 0.2828 - val_loss: 3.1854 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2610 - loss: 3.2340 - val_accuracy: 0.2841 - val_loss: 3.1815 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2645 - loss: 3.2036 - val_accuracy: 0.2872 - val_loss: 3.1734 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2672 - loss: 3.1960 - val_accuracy: 0.2842 - val_loss: 3.1808 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2669 - loss: 3.1810 - val_accuracy: 0.2836 - val_loss: 3.1770 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2650 - loss: 3.1665 - val_accuracy: 0.2857 - val_loss: 3.1718 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2706 - loss: 3.1450 - val_accuracy: 0.2827 - val_loss: 3.1732 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2677 - loss: 3.1493 - val_accuracy: 0.2856 - val_loss: 3.1821 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2680 - loss: 3.1439 - val_accuracy: 0.2860 - val_loss: 3.1802 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2689 - loss: 3.1264 - val_accuracy: 0.2842 - val_loss: 3.1880 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 245/1732\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:24\u001b[0m 219ms/step - accuracy: 0.2681 - loss: 3.1384\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2687 - loss: 3.1360 - val_accuracy: 0.2892 - val_loss: 3.1820 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2749 - loss: 3.0901 - val_accuracy: 0.2898 - val_loss: 3.1751 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2769 - loss: 3.0704 - val_accuracy: 0.2891 - val_loss: 3.1741 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2783 - loss: 3.0589 - val_accuracy: 0.2910 - val_loss: 3.1702 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2770 - loss: 3.0648 - val_accuracy: 0.2901 - val_loss: 3.1723 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2846 - loss: 3.0444 - val_accuracy: 0.2906 - val_loss: 3.1777 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2789 - loss: 3.0597 - val_accuracy: 0.2901 - val_loss: 3.1827 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2795 - loss: 3.0398 - val_accuracy: 0.2904 - val_loss: 3.1827 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m 158/1732\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:52\u001b[0m 338ms/step - accuracy: 0.2714 - loss: 3.0684 \n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2787 - loss: 3.0442 - val_accuracy: 0.2913 - val_loss: 3.1818 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2810 - loss: 3.0414 - val_accuracy: 0.2914 - val_loss: 3.1828 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2784 - loss: 3.0415 - val_accuracy: 0.2933 - val_loss: 3.1840 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2857 - loss: 3.0133 - val_accuracy: 0.2926 - val_loss: 3.1843 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2816 - loss: 3.0161 - val_accuracy: 0.2929 - val_loss: 3.1866 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1722/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2840 - loss: 3.0123\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2840 - loss: 3.0123 - val_accuracy: 0.2940 - val_loss: 3.1847 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2849 - loss: 3.0066 - val_accuracy: 0.2934 - val_loss: 3.1836 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28241us/step - accuracy: 0.2889 - loss: 2.9871 - val_accuracy: 0.2928 - val_loss: 3.1828 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2865 - loss: 2.9875 - val_accuracy: 0.2944 - val_loss: 3.1843 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2849 - loss: 2.9925 - val_accuracy: 0.2933 - val_loss: 3.1846 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1130/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.2910 - loss: 2.9629\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2894 - loss: 2.9728 - val_accuracy: 0.2935 - val_loss: 3.1844 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2887 - loss: 2.9896 - val_accuracy: 0.2930 - val_loss: 3.1855 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2863 - loss: 2.9990 - val_accuracy: 0.2934 - val_loss: 3.1879 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9823 - val_accuracy: 0.2918 - val_loss: 3.1868 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2910 - loss: 2.9783 - val_accuracy: 0.2925 - val_loss: 3.1873 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m 336/1732\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:42\u001b[0m 160ms/step - accuracy: 0.2954 - loss: 2.9667\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2902 - loss: 2.9775 - val_accuracy: 0.2926 - val_loss: 3.1891 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2859 - loss: 2.9791 - val_accuracy: 0.2925 - val_loss: 3.1894 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2864 - loss: 2.9897 - val_accuracy: 0.2926 - val_loss: 3.1885 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2874 - loss: 2.9847 - val_accuracy: 0.2918 - val_loss: 3.1885 - learning_rate: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.67%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.08      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.43      0.06      0.10        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.30      0.12      0.18        24\n",
      "      uc0016       0.16      0.07      0.10      1016\n",
      "      uc0017       0.15      0.04      0.06       247\n",
      "     uc0018b       0.00      0.00      0.00       105\n",
      "      uc0019       0.22      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.86      1.00      0.92        55\n",
      "      uc0024       0.33      0.34      0.33       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.07      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.32      0.23      0.27        26\n",
      "      uc0028       0.18      0.03      0.05       109\n",
      "      uc0029       0.46      0.66      0.54       205\n",
      "      uc0030       0.49      0.60      0.54       252\n",
      "      uc0031       0.35      0.22      0.27       574\n",
      "      uc0032       0.32      0.15      0.21        46\n",
      "      uc0033       0.00      0.00      0.00        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       1.00      0.20      0.33         5\n",
      "      uc0039       0.28      0.39      0.33        99\n",
      "      uc0040       0.39      0.40      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.54      0.47      0.50       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.41      0.30      0.35       741\n",
      "      uc0045       0.34      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.11      0.01      0.02       164\n",
      "      uc0049       0.41      0.23      0.29        31\n",
      "      uc0050       0.33      0.12      0.18         8\n",
      "      uc0052       0.11      0.02      0.04        46\n",
      "      uc0053       0.43      0.14      0.21       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.17      0.05      0.07       176\n",
      "      uc0059       0.22      0.18      0.20       116\n",
      "      uc0060       0.44      0.29      0.35       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.49      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.58      0.46      1357\n",
      "      uc0076       0.15      0.03      0.05       200\n",
      "      uc0077       0.42      0.23      0.30       681\n",
      "      uc0078       0.43      0.16      0.23        57\n",
      "      uc0079       0.62      0.04      0.07       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.36      0.37      0.37       196\n",
      "      uc0087       0.20      0.09      0.12       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.25      0.20      0.22        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.27      0.18      0.22        49\n",
      "      uc0094       0.27      0.01      0.03       785\n",
      "      uc0096       0.28      0.43      0.34      2817\n",
      "      uc0097       0.28      0.11      0.16        92\n",
      "      uc0098       0.68      0.47      0.55       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.35      0.09      0.14        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.38      0.61      0.47       330\n",
      "      uc0108       0.65      0.58      0.61        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.13      0.03      0.05      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.26      0.06      0.10        79\n",
      "      uc0116       0.15      0.13      0.14        31\n",
      "      uc0117       0.43      0.62      0.51        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.40      0.11      0.17       772\n",
      "      uc0125       0.31      0.31      0.31       115\n",
      "      uc0126       0.24      0.03      0.05       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.32      0.14      0.19        88\n",
      "      uc0131       0.19      0.04      0.06       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.49      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.14      0.04      0.06        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.24      0.26      0.25       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.29      0.44      0.35        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.18      0.14      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.38      0.07      0.12        80\n",
      "      uc0150       0.00      0.00      0.00       541\n",
      "      uc0153       0.20      0.19      0.19       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.48      0.32      0.38       149\n",
      "      uc0158       0.30      0.23      0.26        26\n",
      "      uc0159       1.00      0.13      0.24        15\n",
      "      uc0161       0.00      0.00      0.00         7\n",
      "      uc0162       0.23      0.17      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.24      0.15      0.19       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.09      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.21      0.18      0.19        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.24      0.11      0.15      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.25      0.02      0.04        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.33      0.02      0.04        54\n",
      "      uc0195       0.49      0.48      0.49        66\n",
      "      uc0197       0.28      0.13      0.18        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.42      0.23      0.30       409\n",
      "      uc0211       0.21      0.08      0.12       298\n",
      "      uc0212       0.39      0.26      0.31       274\n",
      "      uc0215       0.36      0.07      0.11       647\n",
      "      uc0216       0.23      0.38      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.41      0.72      0.52       120\n",
      "      uc0220       0.38      0.52      0.44        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.33      0.37      0.35      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.38      0.12      0.19        24\n",
      "      uc0226       0.46      0.76      0.57       235\n",
      "      uc0228       0.06      0.04      0.05        25\n",
      "      uc0229       0.39      0.34      0.36        94\n",
      "      uc0230       0.21      0.09      0.12        78\n",
      "      uc0232       0.47      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.32      0.03      0.05       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.60      0.56       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       1.00      0.04      0.07        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.14      0.06      0.08        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.25      0.13      0.17       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.00      0.00      0.00       120\n",
      "      uc1007       0.40      0.12      0.18        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.19      0.20      0.19        50\n",
      "      uc1010       0.40      0.51      0.45        70\n",
      "      uc1011       0.35      0.22      0.27        41\n",
      "      uc1012       0.38      0.10      0.15        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.12      0.12      0.12        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.33      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.10      0.12      0.11         8\n",
      "      uc2001       0.29      0.17      0.21        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.76      0.72      0.74        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.33      0.35      0.34        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.35      0.56      0.43        16\n",
      "      uc2012       0.44      0.50      0.47        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.27      0.15      0.19        80\n",
      "      uc2020       0.46      0.18      0.26       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.19      0.13      0.15       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.64      0.40      0.49        35\n",
      "      uc2028       0.81      0.81      0.81        37\n",
      "      uc2029       0.27      0.33      0.30       341\n",
      "      uc2030       0.46      0.35      0.40        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.06      0.01      0.01       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.41      0.20      0.27        45\n",
      "      uc2037       0.17      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.17      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.26      0.32      0.29        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.33      0.28      0.30        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.47      0.40      0.43        20\n",
      "      uc2049       0.70      0.44      0.54        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.38      0.40      0.39        20\n",
      "      uc2056       0.73      0.26      0.39        42\n",
      "      uc2059       0.22      0.61      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.39      0.32      0.35        37\n",
      "      uc2063       0.73      0.55      0.63        20\n",
      "      uc2064       0.20      0.03      0.06        61\n",
      "      uc2065       0.55      0.39      0.46        66\n",
      "      uc2066       0.23      0.31      0.26       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.36      0.23      0.28        22\n",
      "      uc2072       1.00      0.11      0.19        19\n",
      "      uc2073       0.90      0.36      0.51        25\n",
      "      uc2074       0.00      0.00      0.00         8\n",
      "      uc2075       0.45      0.52      0.48        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.65      0.39      0.49        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.45      0.36      0.40        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.32      0.23      0.27        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.25      0.04      0.07        23\n",
      "      uc2087       0.92      0.18      0.31       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.20      0.11      0.14         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.36      0.04      0.07       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.07      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.75      0.86      0.80        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_13 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 15/17: usuario_14\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1543 - loss: 4.2051 - val_accuracy: 0.2698 - val_loss: 3.3416 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2444 - loss: 3.4347 - val_accuracy: 0.2787 - val_loss: 3.2286 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2579 - loss: 3.3109 - val_accuracy: 0.2826 - val_loss: 3.1973 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2628 - loss: 3.2674 - val_accuracy: 0.2810 - val_loss: 3.1804 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2663 - loss: 3.2171 - val_accuracy: 0.2863 - val_loss: 3.1774 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2661 - loss: 3.2049 - val_accuracy: 0.2852 - val_loss: 3.1667 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2679 - loss: 3.1922 - val_accuracy: 0.2841 - val_loss: 3.1707 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28306us/step - accuracy: 0.2701 - loss: 3.1620 - val_accuracy: 0.2865 - val_loss: 3.1699 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2688 - loss: 3.1669 - val_accuracy: 0.2889 - val_loss: 3.1636 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2674 - loss: 3.1592 - val_accuracy: 0.2873 - val_loss: 3.1721 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2689 - loss: 3.1455 - val_accuracy: 0.2857 - val_loss: 3.1710 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2718 - loss: 3.1289 - val_accuracy: 0.2874 - val_loss: 3.1706 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2680 - loss: 3.1451 - val_accuracy: 0.2880 - val_loss: 3.1791 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 879/1732\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.2714 - loss: 3.1313\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2701 - loss: 3.1405 - val_accuracy: 0.2871 - val_loss: 3.1760 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2751 - loss: 3.1018 - val_accuracy: 0.2907 - val_loss: 3.1641 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2790 - loss: 3.0708 - val_accuracy: 0.2918 - val_loss: 3.1589 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2810 - loss: 3.0712 - val_accuracy: 0.2907 - val_loss: 3.1688 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2787 - loss: 3.0565 - val_accuracy: 0.2918 - val_loss: 3.1661 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2772 - loss: 3.0595 - val_accuracy: 0.2906 - val_loss: 3.1672 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2800 - loss: 3.0526 - val_accuracy: 0.2905 - val_loss: 3.1705 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m 766/1732\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 71ms/step - accuracy: 0.2898 - loss: 3.0013\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2857 - loss: 3.0241 - val_accuracy: 0.2900 - val_loss: 3.1702 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 33ms/step - accuracy: 0.2838 - loss: 3.0295 - val_accuracy: 0.2908 - val_loss: 3.1675 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28038us/step - accuracy: 0.2843 - loss: 3.0141 - val_accuracy: 0.2912 - val_loss: 3.1702 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2843 - loss: 3.0258 - val_accuracy: 0.2918 - val_loss: 3.1701 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2835 - loss: 3.0177 - val_accuracy: 0.2934 - val_loss: 3.1706 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1092/1732\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.2881 - loss: 3.0016\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2871 - loss: 3.0079 - val_accuracy: 0.2922 - val_loss: 3.1689 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2818 - loss: 3.0182 - val_accuracy: 0.2930 - val_loss: 3.1708 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2815 - loss: 3.0121 - val_accuracy: 0.2930 - val_loss: 3.1697 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2863 - loss: 3.0005 - val_accuracy: 0.2938 - val_loss: 3.1692 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2883 - loss: 2.9888 - val_accuracy: 0.2941 - val_loss: 3.1717 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m 106/1732\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2907 - loss: 2.9560\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m -28313us/step - accuracy: 0.2863 - loss: 2.9876 - val_accuracy: 0.2941 - val_loss: 3.1714 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9833 - val_accuracy: 0.2935 - val_loss: 3.1732 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2906 - loss: 2.9870 - val_accuracy: 0.2933 - val_loss: 3.1736 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2896 - loss: 2.9755 - val_accuracy: 0.2932 - val_loss: 3.1730 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2862 - loss: 2.9810 - val_accuracy: 0.2933 - val_loss: 3.1737 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1720/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2845 - loss: 2.9932\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2845 - loss: 2.9931 - val_accuracy: 0.2931 - val_loss: 3.1755 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-48s\u001b[0m -28017us/step - accuracy: 0.2847 - loss: 2.9930 - val_accuracy: 0.2935 - val_loss: 3.1746 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2886 - loss: 2.9789 - val_accuracy: 0.2929 - val_loss: 3.1742 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2864 - loss: 3.0022 - val_accuracy: 0.2938 - val_loss: 3.1746 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2855 - loss: 2.9803 - val_accuracy: 0.2935 - val_loss: 3.1745 - learning_rate: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step  \n",
      "Acurácia: 28.58%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.10      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.29      0.04      0.07        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.31      0.17      0.22        24\n",
      "      uc0016       0.16      0.07      0.10      1016\n",
      "      uc0017       0.15      0.01      0.02       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.21      0.01      0.01       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.82      0.96      0.88        55\n",
      "      uc0024       0.28      0.36      0.32       674\n",
      "      uc0025       0.00      0.00      0.00         9\n",
      "   uc0025_01       0.30      0.04      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.17      0.04      0.06        26\n",
      "      uc0028       0.00      0.00      0.00       109\n",
      "      uc0029       0.51      0.65      0.57       205\n",
      "      uc0030       0.49      0.59      0.54       252\n",
      "      uc0031       0.35      0.19      0.25       574\n",
      "      uc0032       0.32      0.15      0.21        46\n",
      "      uc0033       0.26      0.14      0.18        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.30      0.43      0.36        99\n",
      "      uc0040       0.38      0.38      0.38       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.52      0.44      0.48       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.38      0.30      0.34       741\n",
      "      uc0045       0.39      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.29      0.03      0.06       164\n",
      "      uc0049       0.36      0.16      0.22        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.20      0.02      0.04        46\n",
      "      uc0053       0.37      0.15      0.22       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.14      0.03      0.05       176\n",
      "      uc0059       0.23      0.20      0.21       116\n",
      "      uc0060       0.55      0.19      0.28       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.31      0.47      0.38      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.36      0.61      0.45      1357\n",
      "      uc0076       0.05      0.01      0.01       200\n",
      "      uc0077       0.44      0.22      0.30       681\n",
      "      uc0078       0.47      0.14      0.22        57\n",
      "      uc0079       0.38      0.02      0.04       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.00      0.00      0.00        32\n",
      "      uc0086       0.32      0.34      0.33       196\n",
      "      uc0087       0.27      0.09      0.13       334\n",
      "      uc0089       1.00      0.05      0.09        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.41      0.26      0.32        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.28      0.16      0.21        49\n",
      "      uc0094       0.43      0.01      0.02       785\n",
      "      uc0096       0.27      0.45      0.34      2817\n",
      "      uc0097       0.27      0.13      0.18        92\n",
      "      uc0098       0.60      0.48      0.53       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.29      0.06      0.10        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.17      0.05      0.07        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.40      0.57      0.47       330\n",
      "      uc0108       0.71      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.58      0.17      0.27        63\n",
      "      uc0111       0.06      0.00      0.00      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.31      0.06      0.11        79\n",
      "      uc0116       0.17      0.16      0.17        31\n",
      "      uc0117       0.52      0.58      0.55        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.42      0.10      0.16       772\n",
      "      uc0125       0.36      0.29      0.32       115\n",
      "      uc0126       0.29      0.01      0.03       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.21      0.04      0.06       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.44      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.09      0.08      0.08        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.27      0.22      0.24       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.27      0.39      0.32        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.19      0.12      0.15      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.50      0.12      0.20        80\n",
      "      uc0150       0.17      0.01      0.01       541\n",
      "      uc0153       0.19      0.18      0.19       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.33      0.12      0.18         8\n",
      "      uc0157       0.46      0.37      0.41       149\n",
      "      uc0158       0.43      0.23      0.30        26\n",
      "      uc0159       0.75      0.20      0.32        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.24      0.16      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.26      0.15      0.19       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.36      0.03      0.05       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.24      0.21      0.22        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.25      0.10      0.14      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.21      0.07      0.10        46\n",
      "      uc0190       0.25      0.03      0.06        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.20      0.06      0.09        17\n",
      "      uc0193       0.00      0.00      0.00        54\n",
      "      uc0195       0.50      0.42      0.46        66\n",
      "      uc0197       0.31      0.13      0.19        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.44      0.21      0.28       409\n",
      "      uc0211       0.15      0.03      0.05       298\n",
      "      uc0212       0.37      0.26      0.30       274\n",
      "      uc0215       0.28      0.09      0.13       647\n",
      "      uc0216       0.23      0.38      0.29       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.37      0.71      0.49       120\n",
      "      uc0220       0.40      0.52      0.45        89\n",
      "      uc0221       0.35      0.18      0.24        90\n",
      "      uc0222       0.30      0.39      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.41      0.29      0.34        24\n",
      "      uc0226       0.43      0.77      0.55       235\n",
      "      uc0228       0.14      0.08      0.10        25\n",
      "      uc0229       0.39      0.34      0.36        94\n",
      "      uc0230       0.26      0.12      0.16        78\n",
      "      uc0232       0.45      0.50      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.16      0.02      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.54      0.62      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.29      0.07      0.11        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.33      0.06      0.10        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.31      0.14      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.23      0.04      0.07       120\n",
      "      uc1007       0.12      0.06      0.08        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.19      0.20      0.20        50\n",
      "      uc1010       0.38      0.51      0.43        70\n",
      "      uc1011       0.40      0.24      0.30        41\n",
      "      uc1012       0.33      0.03      0.06        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.23      0.19      0.21        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.47      0.32      0.38       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.08      0.12      0.10         8\n",
      "      uc2001       0.23      0.12      0.16        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.72      0.72      0.72        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.33      0.51      0.40        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.44      0.44      0.44        16\n",
      "      uc2012       0.69      0.46      0.55        24\n",
      "      uc2014       0.46      0.15      0.23        40\n",
      "      uc2015       0.20      0.17      0.18         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.17      0.21      0.19        80\n",
      "      uc2020       0.35      0.20      0.26       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.14      0.07      0.09       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.56      0.40      0.47        35\n",
      "      uc2028       0.79      0.84      0.82        37\n",
      "      uc2029       0.32      0.40      0.35       341\n",
      "      uc2030       0.67      0.17      0.28        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.50      0.13      0.21        45\n",
      "      uc2037       0.33      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.20      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.41      0.32      0.36        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.25      0.28      0.27        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.33      0.45      0.38        20\n",
      "      uc2049       0.70      0.44      0.54        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.61      0.33      0.43        42\n",
      "      uc2059       0.21      0.59      0.31       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.48      0.27      0.34        37\n",
      "      uc2063       0.85      0.55      0.67        20\n",
      "      uc2064       0.15      0.03      0.05        61\n",
      "      uc2065       0.38      0.44      0.41        66\n",
      "      uc2066       0.21      0.25      0.23       104\n",
      "      uc2067       0.44      0.13      0.20        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.25      0.27      0.26        22\n",
      "      uc2072       0.00      0.00      0.00        19\n",
      "      uc2073       0.71      0.40      0.51        25\n",
      "      uc2074       0.11      0.12      0.12         8\n",
      "      uc2075       0.33      0.08      0.13        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.56      0.45      0.50        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.42      0.36      0.38        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.22      0.31      0.25        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.20      0.33       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.00      0.00      0.00       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.08      0.03      0.05        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.23     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_14 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 16/17: usuario_15\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.1591 - loss: 4.2159 - val_accuracy: 0.2660 - val_loss: 3.3546 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2476 - loss: 3.4296 - val_accuracy: 0.2785 - val_loss: 3.2399 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2557 - loss: 3.3191 - val_accuracy: 0.2817 - val_loss: 3.2006 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2607 - loss: 3.2655 - val_accuracy: 0.2828 - val_loss: 3.1846 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2628 - loss: 3.2388 - val_accuracy: 0.2876 - val_loss: 3.1698 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2664 - loss: 3.2041 - val_accuracy: 0.2865 - val_loss: 3.1653 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2632 - loss: 3.1915 - val_accuracy: 0.2842 - val_loss: 3.1738 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2660 - loss: 3.1712 - val_accuracy: 0.2858 - val_loss: 3.1731 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2649 - loss: 3.1784 - val_accuracy: 0.2851 - val_loss: 3.1754 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2682 - loss: 3.1585 - val_accuracy: 0.2861 - val_loss: 3.1813 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1653/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.2719 - loss: 3.1322\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2716 - loss: 3.1335 - val_accuracy: 0.2854 - val_loss: 3.1780 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2710 - loss: 3.1206 - val_accuracy: 0.2878 - val_loss: 3.1671 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2753 - loss: 3.0953 - val_accuracy: 0.2881 - val_loss: 3.1677 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2764 - loss: 3.0758 - val_accuracy: 0.2910 - val_loss: 3.1642 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2764 - loss: 3.0686 - val_accuracy: 0.2894 - val_loss: 3.1694 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2797 - loss: 3.0641 - val_accuracy: 0.2897 - val_loss: 3.1742 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2754 - loss: 3.0653 - val_accuracy: 0.2896 - val_loss: 3.1731 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2785 - loss: 3.0599 - val_accuracy: 0.2889 - val_loss: 3.1701 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1711/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2854 - loss: 3.0379\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2853 - loss: 3.0383 - val_accuracy: 0.2911 - val_loss: 3.1736 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2811 - loss: 3.0467 - val_accuracy: 0.2916 - val_loss: 3.1725 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28353us/step - accuracy: 0.2821 - loss: 3.0379 - val_accuracy: 0.2922 - val_loss: 3.1756 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2839 - loss: 3.0248 - val_accuracy: 0.2925 - val_loss: 3.1758 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2820 - loss: 3.0197 - val_accuracy: 0.2936 - val_loss: 3.1805 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1293/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.2871 - loss: 3.0273\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2865 - loss: 3.0269 - val_accuracy: 0.2915 - val_loss: 3.1745 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2862 - loss: 3.0053 - val_accuracy: 0.2935 - val_loss: 3.1768 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2851 - loss: 2.9886 - val_accuracy: 0.2946 - val_loss: 3.1760 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2834 - loss: 3.0106 - val_accuracy: 0.2923 - val_loss: 3.1774 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2851 - loss: 3.0025 - val_accuracy: 0.2930 - val_loss: 3.1789 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m 253/1732\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:12\u001b[0m 212ms/step - accuracy: 0.2957 - loss: 2.9498\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2889 - loss: 2.9847 - val_accuracy: 0.2935 - val_loss: 3.1791 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2897 - loss: 2.9876 - val_accuracy: 0.2929 - val_loss: 3.1803 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2857 - loss: 2.9886 - val_accuracy: 0.2933 - val_loss: 3.1808 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2835 - loss: 2.9952 - val_accuracy: 0.2944 - val_loss: 3.1827 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2906 - loss: 2.9866 - val_accuracy: 0.2930 - val_loss: 3.1812 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1718/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2867 - loss: 3.0061  \n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2867 - loss: 3.0061 - val_accuracy: 0.2939 - val_loss: 3.1812 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2857 - loss: 2.9967 - val_accuracy: 0.2941 - val_loss: 3.1820 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2878 - loss: 2.9960 - val_accuracy: 0.2946 - val_loss: 3.1825 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2857 - loss: 2.9922 - val_accuracy: 0.2948 - val_loss: 3.1830 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28039us/step - accuracy: 0.2924 - loss: 2.9644 - val_accuracy: 0.2940 - val_loss: 3.1835 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m 345/1732\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 156ms/step - accuracy: 0.2962 - loss: 2.9552\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2905 - loss: 2.9739 - val_accuracy: 0.2943 - val_loss: 3.1823 - learning_rate: 3.1250e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2907 - loss: 2.9874 - val_accuracy: 0.2939 - val_loss: 3.1825 - learning_rate: 1.5625e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2898 - loss: 2.9815 - val_accuracy: 0.2938 - val_loss: 3.1826 - learning_rate: 1.5625e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2843 - loss: 2.9876 - val_accuracy: 0.2936 - val_loss: 3.1826 - learning_rate: 1.5625e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2921 - loss: 2.9840 - val_accuracy: 0.2935 - val_loss: 3.1823 - learning_rate: 1.5625e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1719/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2865 - loss: 2.9779\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2865 - loss: 2.9780 - val_accuracy: 0.2937 - val_loss: 3.1824 - learning_rate: 1.5625e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2868 - loss: 2.9897 - val_accuracy: 0.2935 - val_loss: 3.1823 - learning_rate: 7.8125e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2859 - loss: 2.9941 - val_accuracy: 0.2933 - val_loss: 3.1823 - learning_rate: 7.8125e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2868 - loss: 2.9742 - val_accuracy: 0.2933 - val_loss: 3.1824 - learning_rate: 7.8125e-06\n",
      "Epoch 47: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-51s\u001b[0m -35575us/step\n",
      "Acurácia: 28.80%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.17      0.07      0.10        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.33      0.14      0.19        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.36      0.17      0.23        24\n",
      "      uc0016       0.16      0.08      0.11      1016\n",
      "      uc0017       0.18      0.05      0.08       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.19      0.01      0.02       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.83      1.00      0.91        55\n",
      "      uc0024       0.34      0.33      0.34       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.09      0.07      0.08        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.29      0.27      0.28        26\n",
      "      uc0028       1.00      0.01      0.02       109\n",
      "      uc0029       0.49      0.68      0.57       205\n",
      "      uc0030       0.49      0.60      0.54       252\n",
      "      uc0031       0.36      0.20      0.26       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.19      0.08      0.11        63\n",
      "      uc0034       0.64      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.00      0.00      0.00        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.31      0.41      0.35        99\n",
      "      uc0040       0.40      0.39      0.40       136\n",
      "      uc0041       0.11      0.04      0.06        23\n",
      "      uc0042       0.51      0.47      0.49       156\n",
      "      uc0043       0.22      0.71      0.34      5240\n",
      "      uc0044       0.37      0.30      0.33       741\n",
      "      uc0045       0.32      0.06      0.10       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.00      0.00      0.00       164\n",
      "      uc0049       0.48      0.32      0.38        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.25      0.02      0.04        46\n",
      "      uc0053       0.35      0.18      0.24       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.13      0.06      0.08       176\n",
      "      uc0059       0.23      0.21      0.22       116\n",
      "      uc0060       0.39      0.28      0.32       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.30      0.47      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.37      0.60      0.46      1357\n",
      "      uc0076       0.28      0.04      0.07       200\n",
      "      uc0077       0.44      0.23      0.30       681\n",
      "      uc0078       0.45      0.09      0.15        57\n",
      "      uc0079       0.33      0.06      0.10       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.18      0.06      0.09        32\n",
      "      uc0086       0.36      0.37      0.37       196\n",
      "      uc0087       0.24      0.10      0.14       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.29      0.23      0.26        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.30      0.16      0.21        49\n",
      "      uc0094       0.16      0.02      0.03       785\n",
      "      uc0096       0.28      0.46      0.34      2817\n",
      "      uc0097       0.29      0.12      0.17        92\n",
      "      uc0098       0.65      0.48      0.55       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.60      0.05      0.08        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.40      0.60      0.48       330\n",
      "      uc0108       0.71      0.58      0.64        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.12      0.02      0.03        63\n",
      "      uc0111       0.14      0.02      0.04      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.08      0.00      0.01       284\n",
      "      uc0115       0.20      0.08      0.11        79\n",
      "      uc0116       0.17      0.13      0.15        31\n",
      "      uc0117       0.43      0.62      0.51        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.36      0.11      0.17       772\n",
      "      uc0125       0.30      0.30      0.30       115\n",
      "      uc0126       0.22      0.04      0.06       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.00      0.00      0.00        88\n",
      "      uc0131       0.13      0.00      0.01       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.39      0.13      0.20       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.11      0.08      0.09        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.28      0.23      0.25       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.33      0.39      0.36        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.13      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.50      0.14      0.22        80\n",
      "      uc0150       0.25      0.01      0.02       541\n",
      "      uc0153       0.20      0.22      0.21       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.47      0.34      0.39       149\n",
      "      uc0158       0.35      0.23      0.28        26\n",
      "      uc0159       1.00      0.13      0.24        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.24      0.17      0.20      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.25      0.15      0.19       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.14      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.27      0.18      0.21        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.18      0.08      0.11      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.12      0.06      0.08        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.00      0.00      0.00        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.09      0.06      0.07        17\n",
      "      uc0193       0.30      0.06      0.09        54\n",
      "      uc0195       0.43      0.44      0.44        66\n",
      "      uc0197       0.24      0.11      0.15        38\n",
      "      uc0198       0.50      0.03      0.05       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.49      0.21      0.29       409\n",
      "      uc0211       0.22      0.07      0.11       298\n",
      "      uc0212       0.38      0.26      0.31       274\n",
      "      uc0215       0.26      0.09      0.13       647\n",
      "      uc0216       0.28      0.36      0.32       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.39      0.76      0.51       120\n",
      "      uc0220       0.38      0.51      0.43        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.31      0.36      0.34      1234\n",
      "      uc0223       0.33      0.05      0.09        20\n",
      "      uc0225       0.40      0.33      0.36        24\n",
      "      uc0226       0.46      0.75      0.57       235\n",
      "      uc0228       0.14      0.08      0.10        25\n",
      "      uc0229       0.32      0.37      0.35        94\n",
      "      uc0230       0.24      0.13      0.17        78\n",
      "      uc0232       0.46      0.49      0.47      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.18      0.03      0.06       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.53      0.64      0.58       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.25      0.04      0.06        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.22      0.11      0.14        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.17      0.06      0.09        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.30      0.14      0.19       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.25      0.01      0.02       120\n",
      "      uc1007       0.20      0.18      0.19        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.18      0.18        50\n",
      "      uc1010       0.43      0.53      0.47        70\n",
      "      uc1011       0.43      0.24      0.31        41\n",
      "      uc1012       0.23      0.10      0.14        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.20      0.12      0.15        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.43      0.32      0.36       375\n",
      "      uc1018       0.33      0.10      0.15        10\n",
      "      uc1019       0.00      0.00      0.00         8\n",
      "      uc2001       0.00      0.00      0.00        24\n",
      "      uc2002       0.77      0.35      0.49        48\n",
      "      uc2005       0.48      0.72      0.58        18\n",
      "      uc2006       0.69      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.33      0.37      0.35        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.50      0.44      0.47        16\n",
      "      uc2012       0.52      0.46      0.49        24\n",
      "      uc2014       0.00      0.00      0.00        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.23      0.30      0.26        80\n",
      "      uc2020       0.48      0.18      0.26       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.16      0.12      0.14       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.26      0.34        35\n",
      "      uc2028       0.79      0.81      0.80        37\n",
      "      uc2029       0.31      0.40      0.35       341\n",
      "      uc2030       0.60      0.20      0.30        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.67      0.13      0.22        45\n",
      "      uc2037       0.33      0.03      0.05        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.16      0.05      0.07        61\n",
      "      uc2040       0.14      0.04      0.06        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.42      0.23      0.29        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.35      0.30      0.32        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.38      0.40      0.39        20\n",
      "      uc2049       0.48      0.44      0.46        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.59      0.31      0.41        42\n",
      "      uc2059       0.23      0.61      0.33       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.43      0.32      0.37        37\n",
      "      uc2063       0.69      0.55      0.61        20\n",
      "      uc2064       0.30      0.05      0.08        61\n",
      "      uc2065       0.32      0.47      0.38        66\n",
      "      uc2066       0.24      0.20      0.22       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.50      0.14      0.21        22\n",
      "      uc2072       0.29      0.11      0.15        19\n",
      "      uc2073       0.83      0.40      0.54        25\n",
      "      uc2074       0.00      0.00      0.00         8\n",
      "      uc2075       0.46      0.44      0.45        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.48      0.45      0.46        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.71      0.36      0.48        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.15      0.38      0.22        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.25      0.04      0.07        23\n",
      "      uc2087       1.00      0.18      0.31       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.00      0.00      0.00         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.25      0.01      0.02       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_15 processado com sucesso!\n",
      "\n",
      "🔄 Processando usuário 17/17: usuario_16\n",
      "----------------------------------------\n",
      "=== INICIANDO PREPROCESSAMENTO ===\n",
      "Carregando dados de: ../dados/processados/Dados_TechChallenge_Fase3.csv\n",
      "Dataset carregado: (115591, 7)\n",
      "\n",
      "Distribuição das classes:\n",
      "casoDeUso\n",
      "uc0043    13099\n",
      "uc0232     7408\n",
      "uc0096     7042\n",
      "uc0146     5042\n",
      "uc0075     3394\n",
      "uc0222     3085\n",
      "uc0162     3018\n",
      "uc0111     2963\n",
      "uc0179     2896\n",
      "uc0069     2620\n",
      "Name: count, dtype: int64\n",
      "Processando dados para usuário: *\n",
      "Processando todos os usuários (excluindo 1 usuários): 115465 registros\n",
      "Criando features históricas...\n",
      "Registros após limpeza: 115465\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso_1', 'casoDeUso_2']\n",
      "Processando features contextuais...\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "Normalizando features sequenciais...\n",
      "Dividindo dados em treino e teste...\n",
      "⚠️ Aviso: 10 classes com apenas 1 amostra:\n",
      "casoDeUso\n",
      "uc0185    1\n",
      "uc0152    1\n",
      "uc2099    1\n",
      "uc2047    1\n",
      "uc0207    1\n",
      "Name: count, dtype: int64\n",
      "Removendo classes com poucas amostras para permitir estratificação...\n",
      "Dados após filtro: 115455 amostras, 292 classes\n",
      "Aplicando One-Hot Encoding em: ['casoDeUso']\n",
      "✅ Divisão estratificada realizada com sucesso\n",
      "Treino: 69273 samples\n",
      "Teste: 46182 samples\n",
      "=== CRIANDO MODELO DENSE HÍBRIDO ===\n",
      "Usando Dense layers: [128, 64]\n",
      "Modelo criado e compilado!\n",
      "Parâmetros totais: 104,840\n",
      "\n",
      "\n",
      "=== ARQUITETURA DO MODELO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">77,312</span> │ input_sequence[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_epoch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,272</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_sequence      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m603\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m77,312\u001b[0m │ input_sequence[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_epoch         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_epoch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m292\u001b[0m)       │     \u001b[38;5;34m19,272\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,840</span> (409.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,840\u001b[0m (409.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO TREINAMENTO ===\n",
      "Epoch 1/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1592 - loss: 4.1870 - val_accuracy: 0.2671 - val_loss: 3.3377 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2449 - loss: 3.4336 - val_accuracy: 0.2797 - val_loss: 3.2500 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2598 - loss: 3.3154 - val_accuracy: 0.2831 - val_loss: 3.1993 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2628 - loss: 3.2620 - val_accuracy: 0.2864 - val_loss: 3.1788 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2638 - loss: 3.2412 - val_accuracy: 0.2881 - val_loss: 3.1828 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2651 - loss: 3.2108 - val_accuracy: 0.2905 - val_loss: 3.1665 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2671 - loss: 3.1900 - val_accuracy: 0.2857 - val_loss: 3.1736 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2668 - loss: 3.1776 - val_accuracy: 0.2852 - val_loss: 3.1803 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2671 - loss: 3.1675 - val_accuracy: 0.2841 - val_loss: 3.1780 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2703 - loss: 3.1492 - val_accuracy: 0.2869 - val_loss: 3.1725 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1713/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2710 - loss: 3.1539\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2710 - loss: 3.1539 - val_accuracy: 0.2867 - val_loss: 3.1805 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 33ms/step - accuracy: 0.2714 - loss: 3.1127 - val_accuracy: 0.2898 - val_loss: 3.1674 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-49s\u001b[0m -28293us/step - accuracy: 0.2727 - loss: 3.1037 - val_accuracy: 0.2912 - val_loss: 3.1670 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2782 - loss: 3.0827 - val_accuracy: 0.2897 - val_loss: 3.1677 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2820 - loss: 3.0642 - val_accuracy: 0.2889 - val_loss: 3.1677 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1277/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.2853 - loss: 3.0413\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2833 - loss: 3.0499 - val_accuracy: 0.2907 - val_loss: 3.1681 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2795 - loss: 3.0575 - val_accuracy: 0.2904 - val_loss: 3.1673 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2828 - loss: 3.0268 - val_accuracy: 0.2922 - val_loss: 3.1643 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2807 - loss: 3.0338 - val_accuracy: 0.2929 - val_loss: 3.1676 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2867 - loss: 3.0261 - val_accuracy: 0.2940 - val_loss: 3.1643 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2823 - loss: 3.0388 - val_accuracy: 0.2929 - val_loss: 3.1672 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2849 - loss: 3.0138 - val_accuracy: 0.2929 - val_loss: 3.1717 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m 988/1732\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 56ms/step - accuracy: 0.2803 - loss: 3.0352\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2810 - loss: 3.0340 - val_accuracy: 0.2931 - val_loss: 3.1696 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2868 - loss: 3.0130 - val_accuracy: 0.2944 - val_loss: 3.1688 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2874 - loss: 2.9982 - val_accuracy: 0.2941 - val_loss: 3.1690 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2868 - loss: 3.0106 - val_accuracy: 0.2940 - val_loss: 3.1689 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2869 - loss: 2.9955 - val_accuracy: 0.2941 - val_loss: 3.1697 - learning_rate: 1.2500e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m 247/1732\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:21\u001b[0m 217ms/step - accuracy: 0.2941 - loss: 2.9914\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2885 - loss: 2.9982 - val_accuracy: 0.2941 - val_loss: 3.1708 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2851 - loss: 3.0105 - val_accuracy: 0.2940 - val_loss: 3.1708 - learning_rate: 6.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2858 - loss: 2.9934 - val_accuracy: 0.2946 - val_loss: 3.1706 - learning_rate: 6.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2854 - loss: 3.0036 - val_accuracy: 0.2939 - val_loss: 3.1702 - learning_rate: 6.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2829 - loss: 3.0083 - val_accuracy: 0.2940 - val_loss: 3.1710 - learning_rate: 6.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m 359/1732\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 150ms/step - accuracy: 0.2893 - loss: 2.9817\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2892 - loss: 2.9898 - val_accuracy: 0.2942 - val_loss: 3.1718 - learning_rate: 6.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2887 - loss: 3.0018 - val_accuracy: 0.2934 - val_loss: 3.1719 - learning_rate: 3.1250e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.2912 - loss: 2.9723 - val_accuracy: 0.2939 - val_loss: 3.1716 - learning_rate: 3.1250e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2892 - loss: 2.9896 - val_accuracy: 0.2938 - val_loss: 3.1711 - learning_rate: 3.1250e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2904 - loss: 2.9751 - val_accuracy: 0.2941 - val_loss: 3.1719 - learning_rate: 3.1250e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m 679/1732\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 80ms/step - accuracy: 0.2919 - loss: 2.9998\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2913 - loss: 2.9892 - val_accuracy: 0.2946 - val_loss: 3.1722 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2903 - loss: 2.9811 - val_accuracy: 0.2946 - val_loss: 3.1722 - learning_rate: 1.5625e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1732/1732\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2874 - loss: 2.9984 - val_accuracy: 0.2944 - val_loss: 3.1722 - learning_rate: 1.5625e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Treinamento concluído!\n",
      "=== AVALIANDO MODELO ===\n",
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Acurácia: 28.63%\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      uc0001       0.00      0.00      0.00         1\n",
      "      uc0003       0.00      0.00      0.00        12\n",
      "      uc0004       0.08      0.02      0.04        44\n",
      "      uc0006       0.00      0.00      0.00         2\n",
      "      uc0012       0.20      0.14      0.16        51\n",
      "      uc0013       0.00      0.00      0.00        42\n",
      "      uc0014       0.00      0.00      0.00         5\n",
      "      uc0015       0.33      0.17      0.22        24\n",
      "      uc0016       0.16      0.08      0.11      1016\n",
      "      uc0017       0.10      0.01      0.02       247\n",
      "     uc0018b       0.43      0.03      0.05       105\n",
      "      uc0019       0.11      0.00      0.00       812\n",
      "      uc0020       0.00      0.00      0.00         8\n",
      "      uc0021       0.00      0.00      0.00         3\n",
      "      uc0022       0.00      0.00      0.00         1\n",
      "      uc0023       0.77      1.00      0.87        55\n",
      "      uc0024       0.33      0.31      0.32       674\n",
      "      uc0025       1.00      0.33      0.50         9\n",
      "   uc0025_01       0.07      0.04      0.05        68\n",
      "      uc0026       0.00      0.00      0.00        27\n",
      "      uc0027       0.29      0.23      0.26        26\n",
      "      uc0028       0.20      0.01      0.02       109\n",
      "      uc0029       0.48      0.66      0.55       205\n",
      "      uc0030       0.49      0.61      0.54       252\n",
      "      uc0031       0.39      0.18      0.25       574\n",
      "      uc0032       0.30      0.15      0.20        46\n",
      "      uc0033       0.19      0.08      0.11        63\n",
      "      uc0034       0.54      0.06      0.11       115\n",
      "      uc0035       0.00      0.00      0.00        13\n",
      "      uc0036       0.09      0.04      0.05        26\n",
      "      uc0037       0.00      0.00      0.00         5\n",
      "      uc0039       0.33      0.45      0.38        99\n",
      "      uc0040       0.40      0.38      0.39       136\n",
      "      uc0041       0.00      0.00      0.00        23\n",
      "      uc0042       0.49      0.44      0.47       156\n",
      "      uc0043       0.22      0.71      0.33      5240\n",
      "      uc0044       0.40      0.26      0.31       741\n",
      "      uc0045       0.37      0.09      0.15       319\n",
      "      uc0046       0.00      0.00      0.00         1\n",
      "      uc0047       0.11      0.01      0.02       164\n",
      "      uc0049       0.46      0.19      0.27        31\n",
      "      uc0050       0.00      0.00      0.00         8\n",
      "      uc0052       0.10      0.02      0.04        46\n",
      "      uc0053       0.43      0.14      0.21       426\n",
      "      uc0054       0.00      0.00      0.00         6\n",
      "      uc0056       0.00      0.00      0.00        73\n",
      "      uc0057       0.00      0.00      0.00         1\n",
      "      uc0058       0.11      0.04      0.06       176\n",
      "      uc0059       0.24      0.19      0.21       116\n",
      "      uc0060       0.51      0.24      0.33       177\n",
      "      uc0061       0.00      0.00      0.00        26\n",
      "      uc0062       0.00      0.00      0.00        17\n",
      "      uc0064       0.00      0.00      0.00         2\n",
      "      uc0065       0.00      0.00      0.00         8\n",
      "      uc0067       0.00      0.00      0.00        15\n",
      "      uc0068       0.00      0.00      0.00       102\n",
      "      uc0069       0.30      0.49      0.37      1048\n",
      "      uc0070       0.00      0.00      0.00         7\n",
      "      uc0072       0.00      0.00      0.00         2\n",
      "      uc0075       0.38      0.59      0.46      1357\n",
      "      uc0076       0.00      0.00      0.00       200\n",
      "      uc0077       0.43      0.24      0.30       681\n",
      "      uc0078       0.53      0.14      0.22        57\n",
      "      uc0079       0.67      0.03      0.06       130\n",
      "      uc0080       0.00      0.00      0.00        25\n",
      "      uc0081       0.00      0.00      0.00        98\n",
      "      uc0082       0.00      0.00      0.00         1\n",
      "      uc0084       0.00      0.00      0.00         9\n",
      "      uc0085       0.40      0.06      0.11        32\n",
      "      uc0086       0.36      0.35      0.35       196\n",
      "      uc0087       0.21      0.11      0.15       334\n",
      "      uc0089       0.00      0.00      0.00        43\n",
      "      uc0090       0.00      0.00      0.00        64\n",
      "      uc0091       0.29      0.14      0.19        70\n",
      "      uc0092       0.00      0.00      0.00         2\n",
      "      uc0093       0.33      0.16      0.22        49\n",
      "      uc0094       0.17      0.01      0.02       785\n",
      "      uc0096       0.27      0.44      0.34      2817\n",
      "      uc0097       0.25      0.12      0.16        92\n",
      "      uc0098       0.68      0.49      0.57       115\n",
      "      uc0099       0.00      0.00      0.00         8\n",
      "      uc0100       0.27      0.11      0.15        66\n",
      "      uc0101       0.00      0.00      0.00         7\n",
      "      uc0102       0.00      0.00      0.00        21\n",
      "      uc0103       0.00      0.00      0.00        11\n",
      "      uc0105       0.00      0.00      0.00        22\n",
      "      uc0107       0.39      0.58      0.47       330\n",
      "      uc0108       0.69      0.56      0.62        52\n",
      "      uc0109       0.00      0.00      0.00        58\n",
      "      uc0110       0.42      0.17      0.25        63\n",
      "      uc0111       0.11      0.01      0.03      1185\n",
      "      uc0112       0.00      0.00      0.00        57\n",
      "      uc0113       0.00      0.00      0.00        70\n",
      "      uc0114       0.00      0.00      0.00       284\n",
      "      uc0115       0.27      0.09      0.13        79\n",
      "      uc0116       0.18      0.19      0.18        31\n",
      "      uc0117       0.44      0.58      0.50        26\n",
      "      uc0118       0.00      0.00      0.00        71\n",
      "      uc0124       0.37      0.12      0.18       772\n",
      "      uc0125       0.31      0.36      0.33       115\n",
      "      uc0126       0.22      0.05      0.08       727\n",
      "      uc0127       0.00      0.00      0.00         3\n",
      "      uc0128       0.00      0.00      0.00        25\n",
      "      uc0130       0.32      0.14      0.19        88\n",
      "      uc0131       0.20      0.05      0.07       684\n",
      "      uc0132       0.00      0.00      0.00         4\n",
      "      uc0133       0.00      0.00      0.00        13\n",
      "      uc0134       0.45      0.11      0.17       237\n",
      "      uc0135       0.00      0.00      0.00        75\n",
      "      uc0136       0.13      0.08      0.10        26\n",
      "      uc0137       0.00      0.00      0.00        34\n",
      "      uc0138       0.00      0.00      0.00        50\n",
      "      uc0139       0.25      0.26      0.26       431\n",
      "      uc0140       0.00      0.00      0.00        36\n",
      "      uc0141       0.28      0.39      0.33        18\n",
      "      uc0142       0.00      0.00      0.00         3\n",
      "      uc0146       0.20      0.13      0.16      2017\n",
      "      uc0147       0.00      0.00      0.00         7\n",
      "      uc0148       0.00      0.00      0.00         1\n",
      "      uc0149       0.52      0.14      0.22        80\n",
      "      uc0150       0.14      0.01      0.01       541\n",
      "      uc0153       0.21      0.22      0.21       118\n",
      "      uc0155       0.00      0.00      0.00         2\n",
      "      uc0156       0.00      0.00      0.00         8\n",
      "      uc0157       0.44      0.38      0.41       149\n",
      "      uc0158       0.30      0.23      0.26        26\n",
      "      uc0159       0.67      0.13      0.22        15\n",
      "      uc0161       1.00      0.14      0.25         7\n",
      "      uc0162       0.24      0.19      0.21      1207\n",
      "      uc0163       0.00      0.00      0.00         5\n",
      "      uc0164       0.00      0.00      0.00         2\n",
      "      uc0165       0.24      0.15      0.19       136\n",
      "      uc0167       0.00      0.00      0.00        25\n",
      "      uc0169       0.08      0.01      0.01       142\n",
      "      uc0171       0.00      0.00      0.00        35\n",
      "      uc0172       0.27      0.26      0.27        34\n",
      "      uc0173       0.00      0.00      0.00        16\n",
      "      uc0175       0.00      0.00      0.00         6\n",
      "      uc0178       0.00      0.00      0.00        64\n",
      "      uc0179       0.22      0.09      0.13      1158\n",
      "      uc0180       0.00      0.00      0.00        11\n",
      "      uc0181       0.00      0.00      0.00       570\n",
      "      uc0184       0.00      0.00      0.00         1\n",
      "      uc0186       0.00      0.00      0.00        47\n",
      "      uc0187       0.00      0.00      0.00        26\n",
      "      uc0189       0.32      0.17      0.23        46\n",
      "      uc0190       0.00      0.00      0.00        60\n",
      "      uc0191       0.00      0.00      0.00        25\n",
      "      uc0192       0.00      0.00      0.00        17\n",
      "      uc0193       0.23      0.06      0.09        54\n",
      "      uc0195       0.48      0.42      0.45        66\n",
      "      uc0197       0.28      0.13      0.18        38\n",
      "      uc0198       0.00      0.00      0.00       139\n",
      "      uc0199       0.00      0.00      0.00         1\n",
      "      uc0200       0.00      0.00      0.00         2\n",
      "      uc0201       0.00      0.00      0.00        24\n",
      "      uc0202       0.00      0.00      0.00         2\n",
      "      uc0206       0.00      0.00      0.00         1\n",
      "      uc0209       0.41      0.23      0.30       409\n",
      "      uc0211       0.27      0.06      0.10       298\n",
      "      uc0212       0.38      0.27      0.31       274\n",
      "      uc0215       0.26      0.09      0.13       647\n",
      "      uc0216       0.22      0.33      0.26       183\n",
      "      uc0217       0.00      0.00      0.00         6\n",
      "      uc0219       0.41      0.71      0.52       120\n",
      "      uc0220       0.40      0.52      0.45        89\n",
      "      uc0221       0.41      0.18      0.25        90\n",
      "      uc0222       0.33      0.37      0.35      1234\n",
      "      uc0223       0.50      0.05      0.09        20\n",
      "      uc0225       0.47      0.38      0.42        24\n",
      "      uc0226       0.46      0.75      0.57       235\n",
      "      uc0228       0.00      0.00      0.00        25\n",
      "      uc0229       0.37      0.35      0.36        94\n",
      "      uc0230       0.23      0.13      0.17        78\n",
      "      uc0232       0.46      0.49      0.48      2944\n",
      "      uc0233       0.00      0.00      0.00        28\n",
      "      uc0234       0.00      0.00      0.00        82\n",
      "      uc0235       0.16      0.02      0.03       460\n",
      "      uc0237       0.00      0.00      0.00        16\n",
      "      uc0238       0.52      0.61      0.57       194\n",
      "      uc0240       0.00      0.00      0.00        42\n",
      "      uc0241       0.00      0.00      0.00        19\n",
      "      uc0242       0.00      0.00      0.00        28\n",
      "      uc0243       0.00      0.00      0.00        12\n",
      "      uc0244       0.00      0.00      0.00         3\n",
      "      uc0250       0.00      0.00      0.00         1\n",
      "      uc0253       0.00      0.00      0.00        57\n",
      "      uc0254       0.00      0.00      0.00         2\n",
      "      uc0255       0.00      0.00      0.00        11\n",
      "      uc0256       0.00      0.00      0.00        11\n",
      "      uc0264       0.00      0.00      0.00         4\n",
      "      uc0265       0.00      0.00      0.00        17\n",
      "      uc0266       0.00      0.00      0.00         5\n",
      "      uc1003       0.29      0.16      0.20       126\n",
      "      uc1004       0.00      0.00      0.00        64\n",
      "      uc1005       0.00      0.00      0.00         1\n",
      "      uc1006       0.22      0.02      0.03       120\n",
      "      uc1007       0.17      0.18      0.17        17\n",
      "      uc1008       0.00      0.00      0.00         8\n",
      "      uc1009       0.18      0.20      0.19        50\n",
      "      uc1010       0.39      0.51      0.44        70\n",
      "      uc1011       0.45      0.24      0.32        41\n",
      "      uc1012       0.20      0.03      0.06        31\n",
      "      uc1013       0.00      0.00      0.00        16\n",
      "      uc1014       0.00      0.00      0.00        39\n",
      "      uc1015       0.21      0.12      0.15        26\n",
      "      uc1016       0.00      0.00      0.00        48\n",
      "      uc1017       0.44      0.32      0.37       375\n",
      "      uc1018       0.00      0.00      0.00        10\n",
      "      uc1019       0.09      0.12      0.11         8\n",
      "      uc2001       0.31      0.17      0.22        24\n",
      "      uc2002       0.55      0.35      0.43        48\n",
      "      uc2005       0.72      0.72      0.72        18\n",
      "      uc2006       0.65      0.20      0.31        54\n",
      "      uc2007       0.00      0.00      0.00        17\n",
      "      uc2008       0.42      0.35      0.38        51\n",
      "      uc2009       0.00      0.00      0.00         2\n",
      "      uc2010       0.00      0.00      0.00         1\n",
      "      uc2011       0.54      0.44      0.48        16\n",
      "      uc2012       0.60      0.50      0.55        24\n",
      "      uc2014       0.54      0.17      0.26        40\n",
      "      uc2015       0.00      0.00      0.00         6\n",
      "      uc2017       0.00      0.00      0.00        14\n",
      "      uc2018       0.00      0.00      0.00         1\n",
      "      uc2019       0.19      0.29      0.23        80\n",
      "      uc2020       0.50      0.18      0.27       114\n",
      "      uc2021       0.00      0.00      0.00         1\n",
      "      uc2023       0.18      0.07      0.10       358\n",
      "      uc2025       0.00      0.00      0.00        45\n",
      "      uc2026       0.00      0.00      0.00        16\n",
      "      uc2027       0.50      0.29      0.36        35\n",
      "      uc2028       0.79      0.81      0.80        37\n",
      "      uc2029       0.27      0.40      0.32       341\n",
      "      uc2030       0.47      0.37      0.41        46\n",
      "      uc2031       0.00      0.00      0.00        19\n",
      "      uc2032       0.00      0.00      0.00        29\n",
      "      uc2033       0.00      0.00      0.00       125\n",
      "      uc2034       0.00      0.00      0.00        76\n",
      "      uc2036       0.86      0.13      0.23        45\n",
      "      uc2037       0.50      0.03      0.06        34\n",
      "      uc2038       0.00      0.00      0.00         2\n",
      "      uc2039       0.20      0.05      0.08        61\n",
      "      uc2040       0.00      0.00      0.00        24\n",
      "      uc2041       0.00      0.00      0.00        29\n",
      "      uc2043       0.71      0.23      0.34        22\n",
      "      uc2044       0.00      0.00      0.00         3\n",
      "      uc2045       0.26      0.28      0.27        50\n",
      "      uc2046       0.00      0.00      0.00        27\n",
      "      uc2048       0.25      0.45      0.32        20\n",
      "      uc2049       0.46      0.44      0.45        36\n",
      "      uc2051       0.00      0.00      0.00         1\n",
      "      uc2053       0.00      0.00      0.00        16\n",
      "      uc2054       0.00      0.00      0.00         3\n",
      "      uc2055       0.00      0.00      0.00        20\n",
      "      uc2056       0.79      0.26      0.39        42\n",
      "      uc2059       0.22      0.59      0.32       558\n",
      "      uc2060       0.00      0.00      0.00        21\n",
      "      uc2061       0.00      0.00      0.00        34\n",
      "      uc2062       0.29      0.27      0.28        37\n",
      "      uc2063       0.58      0.55      0.56        20\n",
      "      uc2064       0.31      0.08      0.13        61\n",
      "      uc2065       0.35      0.44      0.39        66\n",
      "      uc2066       0.26      0.26      0.26       104\n",
      "      uc2067       0.00      0.00      0.00        31\n",
      "      uc2068       0.00      0.00      0.00        20\n",
      "      uc2071       0.20      0.18      0.19        22\n",
      "      uc2072       0.50      0.11      0.17        19\n",
      "      uc2073       0.53      0.36      0.43        25\n",
      "      uc2074       0.12      0.12      0.12         8\n",
      "      uc2075       1.00      0.08      0.15        25\n",
      "      uc2076       0.00      0.00      0.00        28\n",
      "      uc2077       0.00      0.00      0.00        20\n",
      "      uc2078       0.52      0.51      0.51        51\n",
      "      uc2079       0.00      0.00      0.00         8\n",
      "      uc2081       0.00      0.00      0.00         2\n",
      "      uc2082       0.62      0.36      0.45        14\n",
      "      uc2083       0.00      0.00      0.00        34\n",
      "      uc2084       0.28      0.27      0.27        26\n",
      "      uc2085       0.00      0.00      0.00         3\n",
      "      uc2086       0.00      0.00      0.00        23\n",
      "      uc2087       1.00      0.18      0.31       120\n",
      "      uc2088       0.00      0.00      0.00         2\n",
      "      uc2089       0.60      0.33      0.43         9\n",
      "      uc2090       0.41      0.58      0.48        19\n",
      "      uc2092       0.25      0.04      0.08       112\n",
      "      uc2093       0.00      0.00      0.00        14\n",
      "      uc2095       0.00      0.00      0.00        29\n",
      "      uc2096       0.00      0.00      0.00        30\n",
      "      uc2097       0.00      0.00      0.00         4\n",
      "      uc2098       0.00      0.00      0.00         5\n",
      "      uc2100       0.00      0.00      0.00         3\n",
      "      uc2101       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.29     46182\n",
      "   macro avg       0.19      0.13      0.14     46182\n",
      "weighted avg       0.27      0.29      0.24     46182\n",
      "\n",
      "Debug - Tipo de 'results': <class 'dict'>\n",
      "Debug - Chaves disponíveis: ['accuracy', 'y_pred', 'y_test', 'confusion_matrix', 'class_names', 'y_pred_proba']\n",
      "✅ Usuário usuario_16 processado com sucesso!\n",
      "\n",
      "============================================================\n",
      "COMPARAÇÃO DE RESULTADOS\n",
      "============================================================\n",
      "\n",
      "Usuário: usuario_00\n",
      "Accuracy: 0.2866\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_01\n",
      "Accuracy: 0.2857\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_02\n",
      "Accuracy: 0.2881\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_03\n",
      "Accuracy: 0.2842\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_04\n",
      "Accuracy: 0.2864\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_05\n",
      "Accuracy: 0.2884\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_06\n",
      "Accuracy: 0.2846\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_07\n",
      "Accuracy: 0.2864\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_08\n",
      "Accuracy: 0.2858\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_09\n",
      "Accuracy: 0.2852\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_10\n",
      "Accuracy: 0.2870\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_11\n",
      "Accuracy: 0.2863\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_12\n",
      "Accuracy: 0.2858\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_13\n",
      "Accuracy: 0.2867\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_14\n",
      "Accuracy: 0.2858\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_15\n",
      "Accuracy: 0.2880\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "Usuário: usuario_16\n",
      "Accuracy: 0.2863\n",
      "Precision: N/A\n",
      "Recall: N/A\n",
      "F1-Score: N/A\n",
      "Classes: ['uc0001', 'uc0003', 'uc0004', 'uc0006', 'uc0012', 'uc0013', 'uc0014', 'uc0015', 'uc0016', 'uc0017', 'uc0018b', 'uc0019', 'uc0020', 'uc0021', 'uc0022', 'uc0023', 'uc0024', 'uc0025', 'uc0025_01', 'uc0026', 'uc0027', 'uc0028', 'uc0029', 'uc0030', 'uc0031', 'uc0032', 'uc0033', 'uc0034', 'uc0035', 'uc0036', 'uc0037', 'uc0039', 'uc0040', 'uc0041', 'uc0042', 'uc0043', 'uc0044', 'uc0045', 'uc0046', 'uc0047', 'uc0049', 'uc0050', 'uc0052', 'uc0053', 'uc0054', 'uc0056', 'uc0057', 'uc0058', 'uc0059', 'uc0060', 'uc0061', 'uc0062', 'uc0064', 'uc0065', 'uc0067', 'uc0068', 'uc0069', 'uc0070', 'uc0072', 'uc0075', 'uc0076', 'uc0077', 'uc0078', 'uc0079', 'uc0080', 'uc0081', 'uc0082', 'uc0084', 'uc0085', 'uc0086', 'uc0087', 'uc0089', 'uc0090', 'uc0091', 'uc0092', 'uc0093', 'uc0094', 'uc0096', 'uc0097', 'uc0098', 'uc0099', 'uc0100', 'uc0101', 'uc0102', 'uc0103', 'uc0105', 'uc0107', 'uc0108', 'uc0109', 'uc0110', 'uc0111', 'uc0112', 'uc0113', 'uc0114', 'uc0115', 'uc0116', 'uc0117', 'uc0118', 'uc0124', 'uc0125', 'uc0126', 'uc0127', 'uc0128', 'uc0130', 'uc0131', 'uc0132', 'uc0133', 'uc0134', 'uc0135', 'uc0136', 'uc0137', 'uc0138', 'uc0139', 'uc0140', 'uc0141', 'uc0142', 'uc0146', 'uc0147', 'uc0148', 'uc0149', 'uc0150', 'uc0153', 'uc0155', 'uc0156', 'uc0157', 'uc0158', 'uc0159', 'uc0161', 'uc0162', 'uc0163', 'uc0164', 'uc0165', 'uc0167', 'uc0169', 'uc0171', 'uc0172', 'uc0173', 'uc0175', 'uc0178', 'uc0179', 'uc0180', 'uc0181', 'uc0184', 'uc0186', 'uc0187', 'uc0189', 'uc0190', 'uc0191', 'uc0192', 'uc0193', 'uc0195', 'uc0197', 'uc0198', 'uc0199', 'uc0200', 'uc0201', 'uc0202', 'uc0206', 'uc0209', 'uc0211', 'uc0212', 'uc0215', 'uc0216', 'uc0217', 'uc0219', 'uc0220', 'uc0221', 'uc0222', 'uc0223', 'uc0225', 'uc0226', 'uc0228', 'uc0229', 'uc0230', 'uc0232', 'uc0233', 'uc0234', 'uc0235', 'uc0237', 'uc0238', 'uc0240', 'uc0241', 'uc0242', 'uc0243', 'uc0244', 'uc0250', 'uc0253', 'uc0254', 'uc0255', 'uc0256', 'uc0264', 'uc0265', 'uc0266', 'uc1003', 'uc1004', 'uc1005', 'uc1006', 'uc1007', 'uc1008', 'uc1009', 'uc1010', 'uc1011', 'uc1012', 'uc1013', 'uc1014', 'uc1015', 'uc1016', 'uc1017', 'uc1018', 'uc1019', 'uc2001', 'uc2002', 'uc2005', 'uc2006', 'uc2007', 'uc2008', 'uc2009', 'uc2010', 'uc2011', 'uc2012', 'uc2014', 'uc2015', 'uc2017', 'uc2018', 'uc2019', 'uc2020', 'uc2021', 'uc2023', 'uc2025', 'uc2026', 'uc2027', 'uc2028', 'uc2029', 'uc2030', 'uc2031', 'uc2032', 'uc2033', 'uc2034', 'uc2036', 'uc2037', 'uc2038', 'uc2039', 'uc2040', 'uc2041', 'uc2043', 'uc2044', 'uc2045', 'uc2046', 'uc2048', 'uc2049', 'uc2051', 'uc2053', 'uc2054', 'uc2055', 'uc2056', 'uc2059', 'uc2060', 'uc2061', 'uc2062', 'uc2063', 'uc2064', 'uc2065', 'uc2066', 'uc2067', 'uc2068', 'uc2071', 'uc2072', 'uc2073', 'uc2074', 'uc2075', 'uc2076', 'uc2077', 'uc2078', 'uc2079', 'uc2081', 'uc2082', 'uc2083', 'uc2084', 'uc2085', 'uc2086', 'uc2087', 'uc2088', 'uc2089', 'uc2090', 'uc2092', 'uc2093', 'uc2095', 'uc2096', 'uc2097', 'uc2098', 'uc2100', 'uc2101']\n",
      "\n",
      "============================================================\n",
      "🏆 RANKING DOS MELHORES RESULTADOS\n",
      "============================================================\n",
      "\n",
      "🥇 TOP 3 USUÁRIOS:\n",
      "1º lugar - usuario_05\n",
      "Accuracy: 0.2884\n",
      "Precision: N/A\n",
      "F1-Score: N/A\n",
      "2º lugar - usuario_02\n",
      "Accuracy: 0.2881\n",
      "Precision: N/A\n",
      "F1-Score: N/A\n",
      "3º lugar - usuario_15\n",
      "Accuracy: 0.2880\n",
      "Precision: N/A\n",
      "F1-Score: N/A\n",
      "\n",
      "🎯 MELHOR MODELO: usuario_05 (Accuracy: 0.2884)\n",
      "\n",
      "============================================================\n",
      "RESUMO ESTATÍSTICO\n",
      "============================================================\n",
      "Total de usuários processados: 17\n",
      "Sucessos: 17\n",
      "Erros: 0\n",
      "Taxa de sucesso: 100.0%\n",
      "\n",
      "✅ Pipeline concluído! Resultados salvos em 'resultados_completos'\n",
      "Use 'resultados_completos[\"nome_usuario\"]' para acessar resultados específicos\n"
     ]
    }
   ],
   "source": [
    "from fase3_fiap_4mlet.treino import processar_pipeline\n",
    "\n",
    "print(\"🚀 Iniciando pipeline de Machine Learning por usuário...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_path='../dados/processados/Dados_TechChallenge_Fase3.csv'\n",
    "\n",
    "# Testar Listar usuários\n",
    "lista_usuarios = listar_usuarios(data_path=data_path)\n",
    "print(f\"Usuários encontrados: {lista_usuarios}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dicionário para armazenar todos os resultados\n",
    "resultados_usuarios = {}\n",
    "\n",
    "# Loop através de todos os usuários\n",
    "for i, usuario in enumerate(lista_usuarios, 1):\n",
    "    print(f\"\\n🔄 Processando usuário {i}/{len(lista_usuarios)}: {usuario}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # model, history, results = main(\n",
    "        #     data_path=data_path,\n",
    "        #     usuario=usuario,\n",
    "        #     use_lstm=False,  # Usar Dense layers (mais estável)\n",
    "        #     epochs=50,\n",
    "        #     plotar_resultado=False,\n",
    "        #     salvar_modelo=False\n",
    "        # )\n",
    "        model, history, results = processar_pipeline(\n",
    "            data_path='../dados/processados/Dados_TechChallenge_Fase3.csv', \n",
    "            usuario='*',  \n",
    "            usuarios_exclusao=[\"usuario_02\"],\n",
    "            use_lstm=False,  \n",
    "            epochs=50,\n",
    "            plotar_resultado=False,\n",
    "            salvar_modelo=False,\n",
    "            # modelo_path='modelos_notebooks'\n",
    "        )\n",
    "        \n",
    "        # Debug: verificar estrutura dos resultados\n",
    "        print(f\"Debug - Tipo de 'results': {type(results)}\")\n",
    "        print(f\"Debug - Chaves disponíveis: {list(results.keys()) if isinstance(results, dict) else 'N/A'}\")\n",
    "        \n",
    "        # Armazenar resultados do usuário\n",
    "        resultados_usuarios[usuario] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'results': results,\n",
    "            'status': 'sucesso'\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Usuário {usuario} processado com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar usuário {usuario}: {str(e)}\")\n",
    "        resultados_usuarios[usuario] = {\n",
    "            'model': None,\n",
    "            'history': None,\n",
    "            'results': None,\n",
    "            'status': 'erro',\n",
    "            'erro': str(e)\n",
    "        }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARAÇÃO DE RESULTADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comparar resultados\n",
    "usuarios_sucesso = []\n",
    "usuarios_erro = []\n",
    "\n",
    "for usuario, dados in resultados_usuarios.items():\n",
    "    if dados['status'] == 'sucesso':\n",
    "        usuarios_sucesso.append(usuario)\n",
    "        results = dados['results']\n",
    "        \n",
    "        # Extrair métricas de classificação\n",
    "        accuracy = results.get('accuracy', 'N/A')\n",
    "        confusion_matrix = results.get('confusion_matrix', 'N/A')\n",
    "        class_names = results.get('class_names', 'N/A')\n",
    "        \n",
    "        # Calcular métricas adicionais da matriz de confusão\n",
    "        precision = recall = f1_score = 'N/A'\n",
    "        \n",
    "        if isinstance(confusion_matrix, type(results.get('confusion_matrix'))) and hasattr(confusion_matrix, 'shape'):\n",
    "            try:\n",
    "                # Para classificação binária\n",
    "                tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"\\nUsuário: {usuario}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\" if isinstance(accuracy, (int, float)) else f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision:.4f}\" if isinstance(precision, (int, float)) else f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall:.4f}\" if isinstance(recall, (int, float)) else f\"Recall: {recall}\")\n",
    "        print(f\"F1-Score: {f1_score:.4f}\" if isinstance(f1_score, (int, float)) else f\"F1-Score: {f1_score}\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "    else:\n",
    "        usuarios_erro.append(usuario)\n",
    "        print(f\"\\n❌ Usuário: {usuario} - ERRO: {dados['erro']}\")\n",
    "\n",
    "# Encontrar o melhor usuário baseado em R²\n",
    "if usuarios_sucesso:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🏆 RANKING DOS MELHORES RESULTADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Criar lista de usuários com suas métricas para ranking\n",
    "    usuarios_com_metricas = []\n",
    "    for usuario in usuarios_sucesso:\n",
    "        results = resultados_usuarios[usuario]['results']\n",
    "        \n",
    "        # Usar accuracy como métrica principal para ranking\n",
    "        accuracy_score = results.get('accuracy', 0)\n",
    "        \n",
    "        if isinstance(accuracy_score, (int, float)):\n",
    "            usuarios_com_metricas.append((usuario, accuracy_score, results))\n",
    "    \n",
    "    # Ordenar por Accuracy (maior é melhor)\n",
    "    usuarios_com_metricas.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n🥇 TOP 3 USUÁRIOS:\")\n",
    "    for i, (usuario, accuracy_score, results) in enumerate(usuarios_com_metricas[:3], 1):\n",
    "        \n",
    "        # Calcular métricas adicionais\n",
    "        confusion_matrix = results.get('confusion_matrix')\n",
    "        precision = recall = f1_score = 'N/A'\n",
    "        \n",
    "        if hasattr(confusion_matrix, 'ravel'):\n",
    "            try:\n",
    "                tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"{i}º lugar - {usuario}\")\n",
    "        print(f\"Accuracy: {accuracy_score:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\" if isinstance(precision, (int, float)) else f\"Precision: {precision}\")\n",
    "        print(f\"F1-Score: {f1_score:.4f}\" if isinstance(f1_score, (int, float)) else f\"F1-Score: {f1_score}\")\n",
    "    \n",
    "    melhor_usuario = usuarios_com_metricas[0][0]\n",
    "    print(f\"\\n🎯 MELHOR MODELO: {melhor_usuario} (Accuracy: {usuarios_com_metricas[0][1]:.4f})\")\n",
    "    \n",
    "# Resumo estatístico\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO ESTATÍSTICO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de usuários processados: {len(lista_usuarios)}\")\n",
    "print(f\"Sucessos: {len(usuarios_sucesso)}\")\n",
    "print(f\"Erros: {len(usuarios_erro)}\")\n",
    "print(f\"Taxa de sucesso: {len(usuarios_sucesso)/len(lista_usuarios)*100:.1f}%\")\n",
    "\n",
    "if usuarios_erro:\n",
    "    print(f\"\\nUsuários com erro: {', '.join(usuarios_erro)}\")\n",
    "\n",
    "# Salvar resultados em variável global para acesso posterior\n",
    "globals()['resultados_completos'] = resultados_usuarios\n",
    "\n",
    "print(\"\\n✅ Pipeline concluído! Resultados salvos em 'resultados_completos'\")\n",
    "print(\"Use 'resultados_completos[\\\"nome_usuario\\\"]' para acessar resultados específicos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['resultados_completos'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varre resultados_completos para extrair as acurácias\n",
    "\n",
    "Classifica por acurácia do treino com as amostras de cada usuário, isoladamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que resultados_completos está no globals()\n",
    "# e tem a estrutura mostrada no seu exemplo\n",
    "\n",
    "dados_acuracia = []\n",
    "for usuario, info in globals()['resultados_completos'].items():\n",
    "    acc = info['results'].get('accuracy', None)\n",
    "    dados_acuracia.append({'Usuário': usuario, 'Acurácia': acc})\n",
    "\n",
    "df_acuracia = pd.DataFrame(dados_acuracia)\n",
    "df_acuracia = df_acuracia.sort_values(by='Acurácia', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(df_acuracia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
